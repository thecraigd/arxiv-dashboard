[
  {
    "id": "2503.21780v1",
    "title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation",
    "authors": [
      "Reza Qorbani",
      "Gianluca Villani",
      "Theodoros Panagiotakopoulos",
      "Marc Botet Colomer",
      "Linus HÃ¤renstam-Nielsen",
      "Mattia Segu",
      "Pier Luigi Dovesi",
      "Jussi Karlgren",
      "Daniel Cremers",
      "Federico Tombari",
      "Matteo Poggi"
    ],
    "abstract": "Open-vocabulary semantic segmentation models associate vision and text to\nlabel pixels from an undefined set of classes using textual queries, providing\nversatile performance on novel datasets. However, large shifts between training\nand test domains degrade their performance, requiring fine-tuning for effective\nreal-world applications. We introduce Semantic Library Adaptation (SemLA), a\nnovel framework for training-free, test-time domain adaptation. SemLA leverages\na library of LoRA-based adapters indexed with CLIP embeddings, dynamically\nmerging the most relevant adapters based on proximity to the target domain in\nthe embedding space. This approach constructs an ad-hoc model tailored to each\nspecific input without additional training. Our method scales efficiently,\nenhances explainability by tracking adapter contributions, and inherently\nprotects data privacy, making it ideal for sensitive applications.\nComprehensive experiments on a 20-domain benchmark built over 10 standard\ndatasets demonstrate SemLA's superior adaptability and performance across\ndiverse settings, establishing a new standard in domain adaptation for\nopen-vocabulary semantic segmentation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21779v1",
    "title": "X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction",
    "authors": [
      "Weihao Yu",
      "Yuanhao Cai",
      "Ruyi Zha",
      "Zhiwen Fan",
      "Chenxin Li",
      "Yixuan Yuan"
    ],
    "abstract": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment",
      "misalignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21766v1",
    "title": "Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence",
    "authors": [
      "Haolin Liu",
      "Xiaohang Zhan",
      "Zizheng Yan",
      "Zhongjin Luo",
      "Yuxin Wen",
      "Xiaoguang Han"
    ],
    "abstract": "Establishing character shape correspondence is a critical and fundamental\ntask in computer vision and graphics, with diverse applications including\nre-topology, attribute transfer, and shape interpolation. Current dominant\nfunctional map methods, while effective in controlled scenarios, struggle in\nreal situations with more complex challenges such as non-isometric shape\ndiscrepancies. In response, we revisit registration-for-correspondence methods\nand tap their potential for more stable shape correspondence estimation. To\novercome their common issues including unstable deformations and the necessity\nfor careful pre-alignment or high-quality initial 3D correspondences, we\nintroduce Stable-SCore: A Stable Registration-based Framework for 3D Shape\nCorrespondence. We first re-purpose a foundation model for 2D character\ncorrespondence that ensures reliable and stable 2D mappings. Crucially, we\npropose a novel Semantic Flow Guided Registration approach that leverages 2D\ncorrespondence to guide mesh deformations. Our framework significantly\nsurpasses existing methods in challenging scenarios, and brings possibilities\nfor a wide array of real applications, as demonstrated in our results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21755v1",
    "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness",
    "authors": [
      "Dian Zheng",
      "Ziqi Huang",
      "Hongbo Liu",
      "Kai Zou",
      "Yinan He",
      "Fan Zhang",
      "Yuanhan Zhang",
      "Jingwen He",
      "Wei-Shi Zheng",
      "Yu Qiao",
      "Ziwei Liu"
    ],
    "abstract": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21749v1",
    "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis",
    "authors": [
      "Shitian Zhao",
      "Qilong Wu",
      "Xinyue Li",
      "Bo Zhang",
      "Ming Li",
      "Qi Qin",
      "Dongyang Liu",
      "Kaipeng Zhang",
      "Hongsheng Li",
      "Yu Qiao",
      "Peng Gao",
      "Bin Fu",
      "Zhen Li"
    ],
    "abstract": "We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024$\\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21735v1",
    "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics",
    "authors": [
      "Arsham Gholamzadeh Khoee",
      "Shuai Wang",
      "Yinan Yu",
      "Robert Feldt",
      "Dhasarathy Parthasarathy"
    ],
    "abstract": "Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21727v1",
    "title": "Enhancing Underwater Navigation through Cross-Correlation-Aware Deep INS/DVL Fusion",
    "authors": [
      "Nadav Cohen",
      "Itzik Klein"
    ],
    "abstract": "The accurate navigation of autonomous underwater vehicles critically depends\non the precision of Doppler velocity log (DVL) velocity measurements. Recent\nadvancements in deep learning have demonstrated significant potential in\nimproving DVL outputs by leveraging spatiotemporal dependencies across multiple\nsensor modalities. However, integrating these estimates into model-based\nfilters, such as the extended Kalman filter, introduces statistical\ninconsistencies, most notably, cross-correlations between process and\nmeasurement noise. This paper addresses this challenge by proposing a\ncross-correlation-aware deep INS/DVL fusion framework. Building upon BeamsNet,\na convolutional neural network designed to estimate AUV velocity using DVL and\ninertial data, we integrate its output into a navigation filter that explicitly\naccounts for the cross-correlation induced between the noise sources. This\napproach improves filter consistency and better reflects the underlying sensor\nerror structure. Evaluated on two real-world underwater trajectories, the\nproposed method outperforms both least squares and cross-correlation-neglecting\napproaches in terms of state uncertainty. Notably, improvements exceed 10% in\nvelocity and misalignment angle confidence metrics. Beyond demonstrating\nempirical performance, this framework provides a theoretically principled\nmechanism for embedding deep learning outputs within stochastic filters.",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment",
      "misalignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21721v1",
    "title": "Evaluating Text-to-Image Synthesis with a Conditional FrÃ©chet Distance",
    "authors": [
      "Jaywon Koo",
      "Jefferson Hernandez",
      "Moayed Haji-Ali",
      "Ziyan Yang",
      "Vicente Ordonez"
    ],
    "abstract": "Evaluating text-to-image synthesis is challenging due to misalignment between\nestablished metrics and human preferences. We propose cFreD, a metric based on\nthe notion of Conditional Fr\\'echet Distance that explicitly accounts for both\nvisual fidelity and text-prompt alignment. Existing metrics such as Inception\nScore (IS), Fr\\'echet Inception Distance (FID) and CLIPScore assess either\nimage quality or image-text alignment but not both which limits their\ncorrelation with human preferences. Scoring models explicitly trained to\nreplicate human preferences require constant updates and may not generalize to\nnovel generation techniques or out-of-domain inputs. Through extensive\nexperiments across multiple recently proposed text-to-image models and diverse\nprompt datasets, we demonstrate that cFreD exhibits a higher correlation with\nhuman judgments compared to statistical metrics, including metrics trained with\nhuman preferences. Our findings validate cFreD as a robust, future-proof metric\nfor the systematic evaluation of text-to-image models, standardizing\nbenchmarking in this rapidly evolving field. We release our evaluation toolkit\nand benchmark in the appendix.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment",
      "misalignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21720v1",
    "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
    "authors": [
      "Souradip Chakraborty",
      "Sujay Bhatt",
      "Udari Madhushani Sehwag",
      "Soumya Suvra Ghosal",
      "Jiahao Qiu",
      "Mengdi Wang",
      "Dinesh Manocha",
      "Furong Huang",
      "Alec Koppel",
      "Sumitra Ganesh"
    ],
    "abstract": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21695v1",
    "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation",
    "authors": [
      "Jiahe Qian",
      "Yaoyu Fang",
      "Jinkui Hao",
      "Bo Zhou"
    ],
    "abstract": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment",
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21659v1",
    "title": "InteractionMap: Improving Online Vectorized HDMap Construction with Interaction",
    "authors": [
      "Kuang Wu",
      "Chuan Yang",
      "Zhanbin Li"
    ],
    "abstract": "Vectorized high-definition (HD) maps are essential for an autonomous driving\nsystem. Recently, state-of-the-art map vectorization methods are mainly based\non DETR-like framework to generate HD maps in an end-to-end manner. In this\npaper, we propose InteractionMap, which improves previous map vectorization\nmethods by fully leveraging local-to-global information interaction in both\ntime and space. Firstly, we explore enhancing DETR-like detectors by explicit\nposition relation prior from point-level to instance-level, since map elements\ncontain strong shape priors. Secondly, we propose a key-frame-based\nhierarchical temporal fusion module, which interacts temporal information from\nlocal to global. Lastly, the separate classification branch and regression\nbranch lead to the problem of misalignment in the output distribution. We\ninteract semantic information with geometric information by introducing a novel\ngeometric-aware classification loss in optimization and a geometric-aware\nmatching cost in label assignment. InteractionMap achieves state-of-the-art\nperformance on both nuScenes and Argoverse2 benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment",
      "misalignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21613v1",
    "title": "Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach",
    "authors": [
      "Javier Coronado-BlÃ¡zquez"
    ],
    "abstract": "We study the ability of large language models (LLMs) to generate\ncomprehensive and accurate book summaries solely from their internal knowledge,\nwithout recourse to the original text. Employing a diverse set of books and\nmultiple LLM architectures, we examine whether these models can synthesize\nmeaningful narratives that align with established human interpretations.\nEvaluation is performed with a LLM-as-a-judge paradigm: each AI-generated\nsummary is compared against a high-quality, human-written summary via a\ncross-model assessment, where all participating LLMs evaluate not only their\nown outputs but also those produced by others. This methodology enables the\nidentification of potential biases, such as the proclivity for models to favor\ntheir own summarization style over others. In addition, alignment between the\nhuman-crafted and LLM-generated summaries is quantified using ROUGE and\nBERTScore metrics, assessing the depth of grammatical and semantic\ncorrespondence. The results reveal nuanced variations in content representation\nand stylistic preferences among the models, highlighting both strengths and\nlimitations inherent in relying on internal knowledge for summarization tasks.\nThese findings contribute to a deeper understanding of LLM internal encodings\nof factual information and the dynamics of cross-model evaluation, with\nimplications for the development of more robust natural language generative\nsystems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21608v1",
    "title": "Nonlinear Multiple Response Regression and Learning of Latent Spaces",
    "authors": [
      "Ye Tian",
      "Sanyou Wu",
      "Long Feng"
    ],
    "abstract": "Identifying low-dimensional latent structures within high-dimensional data\nhas long been a central topic in the machine learning community, driven by the\nneed for data compression, storage, transmission, and deeper data\nunderstanding. Traditional methods, such as principal component analysis (PCA)\nand autoencoders (AE), operate in an unsupervised manner, ignoring label\ninformation even when it is available. In this work, we introduce a unified\nmethod capable of learning latent spaces in both unsupervised and supervised\nsettings. We formulate the problem as a nonlinear multiple-response regression\nwithin an index model context. By applying the generalized Stein's lemma, the\nlatent space can be estimated without knowing the nonlinear link functions. Our\nmethod can be viewed as a nonlinear generalization of PCA. Moreover, unlike AE\nand other neural network methods that operate as \"black boxes\", our approach\nnot only offers better interpretability but also reduces computational\ncomplexity while providing strong theoretical guarantees. Comprehensive\nnumerical experiments and real data analyses demonstrate the superior\nperformance of our method.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21598v1",
    "title": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing",
    "authors": [
      "Johan WahrÃ©us",
      "Ahmed Hussain",
      "Panos Papadimitratos"
    ],
    "abstract": "Large Language Models (LLMs) have transformed task automation and content\ngeneration across various domains while incorporating safety filters to prevent\nmisuse. We introduce a novel jailbreaking framework that employs distributed\nprompt processing combined with iterative refinements to bypass these safety\nmeasures, particularly in generating malicious code. Our architecture consists\nof four key modules: prompt segmentation, parallel processing, response\naggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts\nacross 10 cybersecurity categories, the framework achieves a 73.2% Success Rate\n(SR) in generating malicious code. Notably, our comparative analysis reveals\nthat traditional single-LLM judge evaluation overestimates SRs (93.8%) compared\nto our LLM jury system (73.2%), with manual verification confirming that\nsingle-judge assessments often accept incomplete implementations. Moreover, we\ndemonstrate that our distributed architecture improves SRs by 12% over the\nnon-distributed approach in an ablation study, highlighting both the\neffectiveness of distributed prompt processing and the importance of robust\nevaluation methodologies in assessing jailbreak attempts.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21581v1",
    "title": "AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion",
    "authors": [
      "Liuyue Xie",
      "Jiancong Guo",
      "Ozan Cakmakci",
      "Andre Araujo",
      "Laszlo A. Jeni",
      "Zhiheng Jia"
    ],
    "abstract": "Accurate camera calibration is a fundamental task for 3D perception,\nespecially when dealing with real-world, in-the-wild environments where complex\noptical distortions are common. Existing methods often rely on pre-rectified\nimages or calibration patterns, which limits their applicability and\nflexibility. In this work, we introduce a novel framework that addresses these\nchallenges by jointly modeling camera intrinsic and extrinsic parameters using\na generic ray camera model. Unlike previous approaches, AlignDiff shifts focus\nfrom semantic to geometric features, enabling more accurate modeling of local\ndistortions. We propose AlignDiff, a diffusion model conditioned on geometric\npriors, enabling the simultaneous estimation of camera distortions and scene\ngeometry. To enhance distortion prediction, we incorporate edge-aware\nattention, focusing the model on geometric features around image edges, rather\nthan semantic content. Furthermore, to enhance generalizability to real-world\ncaptures, we incorporate a large database of ray-traced lenses containing over\nthree thousand samples. This database characterizes the distortion inherent in\na diverse variety of lens forms. Our experiments demonstrate that the proposed\nmethod significantly reduces the angular error of estimated ray bundles by ~8.2\ndegrees and overall calibration accuracy, outperforming existing approaches on\nchallenging, real-world datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21544v1",
    "title": "SWI: Speaking with Intent in Large Language Models",
    "authors": [
      "Yuwei Yin",
      "EunJeong Hwang",
      "Giuseppe Carenini"
    ],
    "abstract": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21538v1",
    "title": "Formation Shape Control using the Gromov-Wasserstein Metric",
    "authors": [
      "Haruto Nakashima",
      "Siddhartha Ganguly",
      "Kohei Morimoto",
      "Kenji Kashima"
    ],
    "abstract": "This article introduces a formation shape control algorithm, in the optimal\ncontrol framework, for steering an initial population of agents to a desired\nconfiguration via employing the Gromov-Wasserstein distance. The underlying\ndynamical system is assumed to be a constrained linear system and the objective\nfunction is a sum of quadratic control-dependent stage cost and a\nGromov-Wasserstein terminal cost. The inclusion of the Gromov-Wasserstein cost\ntransforms the resulting optimal control problem into a well-known NP-hard\nproblem, making it both numerically demanding and difficult to solve with high\naccuracy. Towards that end, we employ a recent semi-definite relaxation-driven\ntechnique to tackle the Gromov-Wasserstein distance. A numerical example is\nprovided to illustrate our results.",
    "categories": [
      "math.OC",
      "cs.LG",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "control problem"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21528v1",
    "title": "Bayesian Pseudo Posterior Mechanism for Differentially Private Machine Learning",
    "authors": [
      "Robert Chew",
      "Matthew R. Williams",
      "Elan A. Segarra",
      "Alexander J. Preiss",
      "Amanda Konet",
      "Terrance D. Savitsky"
    ],
    "abstract": "Differential privacy (DP) is becoming increasingly important for deployed\nmachine learning applications because it provides strong guarantees for\nprotecting the privacy of individuals whose data is used to train models.\nHowever, DP mechanisms commonly used in machine learning tend to struggle on\nmany real world distributions, including highly imbalanced or small labeled\ntraining sets. In this work, we propose a new scalable DP mechanism for deep\nlearning models, SWAG-PPM, by using a pseudo posterior distribution that\ndownweights by-record likelihood contributions proportionally to their\ndisclosure risks as the randomized mechanism. As a motivating example from\nofficial statistics, we demonstrate SWAG-PPM on a workplace injury text\nclassification task using a highly imbalanced public dataset published by the\nU.S. Occupational Safety and Health Administration (OSHA). We find that\nSWAG-PPM exhibits only modest utility degradation against a non-private\ncomparator while greatly outperforming the industry standard DP-SGD for a\nsimilar privacy budget.",
    "categories": [
      "stat.ML",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21505v1",
    "title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving",
    "authors": [
      "Yue Li",
      "Meng Tian",
      "Zhenyu Lin",
      "Jiangtong Zhu",
      "Dechang Zhu",
      "Haiqiang Liu",
      "Zining Wang",
      "Yueyi Zhang",
      "Zhiwei Xiong",
      "Xinhai Zhao"
    ],
    "abstract": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21504v1",
    "title": "Keyword-Oriented Multimodal Modeling for Euphemism Identification",
    "authors": [
      "Yuxue Hu",
      "Junsong Li",
      "Meixuan Chen",
      "Dongyu Su",
      "Tongguan Wang",
      "Ying Sha"
    ],
    "abstract": "Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21491v1",
    "title": "Data-Driven Contact-Aware Control Method for Real-Time Deformable Tool Manipulation: A Case Study in the Environmental Swabbing",
    "authors": [
      "Siavash Mahmoudi",
      "Amirreza Davar",
      "Dongyi Wang"
    ],
    "abstract": "Deformable Object Manipulation (DOM) remains a critical challenge in robotics\ndue to the complexities of developing suitable model-based control strategies.\nDeformable Tool Manipulation (DTM) further complicates this task by introducing\nadditional uncertainties between the robot and its environment. While humans\neffortlessly manipulate deformable tools using touch and experience, robotic\nsystems struggle to maintain stability and precision. To address these\nchallenges, we present a novel State-Adaptive Koopman LQR (SA-KLQR) control\nframework for real-time deformable tool manipulation, demonstrated through a\ncase study in environmental swab sampling for food safety. This method\nleverages Koopman operator-based control to linearize nonlinear dynamics while\nadapting to state-dependent variations in tool deformation and contact forces.\nA tactile-based feedback system dynamically estimates and regulates the swab\ntool's angle, contact pressure, and surface coverage, ensuring compliance with\nfood safety standards. Additionally, a sensor-embedded contact pad monitors\nforce distribution to mitigate tool pivoting and deformation, improving\nstability during dynamic interactions. Experimental results validate the\nSA-KLQR approach, demonstrating accurate contact angle estimation, robust\ntrajectory tracking, and reliable force regulation. The proposed framework\nenhances precision, adaptability, and real-time control in deformable tool\nmanipulation, bridging the gap between data-driven learning and optimal control\nin robotic interaction tasks.",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21464v1",
    "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection",
    "authors": [
      "Ryan Marinelli",
      "Josef Pichlmeier",
      "Tamas Bisztray"
    ],
    "abstract": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21431v1",
    "title": "Nearest Neighbour Equilibrium Clustering",
    "authors": [
      "David P. Hofmeyr"
    ],
    "abstract": "A novel and intuitive nearest neighbours based clustering algorithm is\nintroduced, in which a cluster is defined in terms of an equilibrium condition\nwhich balances its size and cohesiveness. The formulation of the equilibrium\ncondition allows for a quantification of the strength of alignment of each\npoint to a cluster, with these cluster alignment strengths leading naturally to\na model selection criterion which renders the proposed approach fully\nautomatable. The algorithm is simple to implement and computationally\nefficient, and produces clustering solutions of extremely high quality in\ncomparison with relevant benchmarks from the literature. R code to implement\nthe approach is available from https://github.com/DavidHofmeyr/NNEC.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21426v1",
    "title": "AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram Model",
    "authors": [
      "Sen Zhang",
      "Qingqing Ye",
      "Haibo Hu",
      "Jianliang Xu"
    ],
    "abstract": "The skip-gram model (SGM), which employs a neural network to generate node\nvectors, serves as the basis for numerous popular graph embedding techniques.\nHowever, since the training datasets contain sensitive linkage information, the\nparameters of a released SGM may encode private information and pose\nsignificant privacy risks. Differential privacy (DP) is a rigorous standard for\nprotecting individual privacy in data analysis. Nevertheless, when applying\ndifferential privacy to skip-gram in graphs, it becomes highly challenging due\nto the complex link relationships, which potentially result in high sensitivity\nand necessitate substantial noise injection. To tackle this challenge, we\npresent AdvSGM, a differentially private skip-gram for graphs via adversarial\ntraining. Our core idea is to leverage adversarial training to privatize\nskip-gram while improving its utility. Towards this end, we develop a novel\nadversarial training module by devising two optimizable noise terms that\ncorrespond to the parameters of a skip-gram. By fine-tuning the weights between\nmodules within AdvSGM, we can achieve differentially private gradient updates\nwithout additional noise injection. Extensive experimental results on six\nreal-world graph datasets show that AdvSGM preserves high data utility across\ndifferent downstream tasks.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21411v1",
    "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
    "authors": [
      "Tong Nie",
      "Jian Sun",
      "Wei Ma"
    ],
    "abstract": "Modern transportation systems face pressing challenges due to increasing\ndemand, dynamic environments, and heterogeneous information integration. The\nrapid evolution of Large Language Models (LLMs) offers transformative potential\nto address these challenges. Extensive knowledge and high-level capabilities\nderived from pretraining evolve the default role of LLMs as text generators to\nbecome versatile, knowledge-driven task solvers for intelligent transportation\nsystems. This survey first presents LLM4TR, a novel conceptual framework that\nsystematically categorizes the roles of LLMs in transportation into four\nsynergetic dimensions: information processors, knowledge encoders, component\ngenerators, and decision facilitators. Through a unified taxonomy, we\nsystematically elucidate how LLMs bridge fragmented data pipelines, enhance\npredictive analytics, simulate human-like reasoning, and enable closed-loop\ninteractions across sensing, learning, modeling, and managing tasks in\ntransportation systems. For each role, our review spans diverse applications,\nfrom traffic prediction and autonomous driving to safety analytics and urban\nmobility optimization, highlighting how emergent capabilities of LLMs such as\nin-context learning and step-by-step reasoning can enhance the operation and\nmanagement of transportation systems. We further curate practical guidance,\nincluding available resources and computational guidelines, to support\nreal-world deployment. By identifying challenges in existing LLM-based\nsolutions, this survey charts a roadmap for advancing LLM-driven transportation\nresearch, positioning LLMs as central actors in the next generation of\ncyber-physical-social mobility ecosystems. Online resources can be found in the\nproject page: https://github.com/tongnie/awesome-llm4tr.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21406v1",
    "title": "Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for Skill Learning",
    "authors": [
      "Leon Keller",
      "Daniel Tanneberg",
      "Jan Peters"
    ],
    "abstract": "Imitation learning is a popular method for teaching robots new behaviors.\nHowever, most existing methods focus on teaching short, isolated skills rather\nthan long, multi-step tasks. To bridge this gap, imitation learning algorithms\nmust not only learn individual skills but also an abstract understanding of how\nto sequence these skills to perform extended tasks effectively. This paper\naddresses this challenge by proposing a neuro-symbolic imitation learning\nframework. Using task demonstrations, the system first learns a symbolic\nrepresentation that abstracts the low-level state-action space. The learned\nrepresentation decomposes a task into easier subtasks and allows the system to\nleverage symbolic planning to generate abstract plans. Subsequently, the system\nutilizes this task decomposition to learn a set of neural skills capable of\nrefining abstract plans into actionable robot commands. Experimental results in\nthree simulated robotic environments demonstrate that, compared to baselines,\nour neuro-symbolic approach increases data efficiency, improves generalization\ncapabilities, and facilitates interpretability.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21392v1",
    "title": "HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion Battery RUL Prediction",
    "authors": [
      "Khoa Tran",
      "Bao Huynh",
      "Tri Le",
      "Lam Pham",
      "Vy-Rin Nguyen"
    ],
    "abstract": "Accurate prediction of the remaining useful life (RUL) in Lithium-ion battery\n(LIB) health management systems is crucial for ensuring reliability and safety.\nCurrent methods typically assume that training and testing data share the same\ndistribution, overlooking the benefits of incorporating diverse data sources to\nenhance model performance. To address this limitation, we introduce a\ndata-independent RUL prediction framework along with its domain adaptation (DA)\napproach, which leverages heterogeneous data sources for improved target\npredictions. Our approach integrates comprehensive data preprocessing,\nincluding feature extraction, denoising, and normalization, with a\ndata-independent prediction model that combines Long Short-Term Memory (LSTM),\nMultihead Attention, and a Neural Ordinary Differential Equation (NODE) block,\ntermed HybridoNet. The domain-adapted version, HybridoNet Adapt, is trained\nusing a novel technique inspired by the Domain-Adversarial Neural Network\n(DANN) framework, a regression ensemble method, and Maximum Mean Discrepancy\n(MMD) to learn domain-invariant features from labeled cycling data in the\nsource and target domains. Experimental results demonstrate that our approach\noutperforms state-of-the-art techniques, providing reliable RUL predictions for\nreal-world applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety",
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21356v1",
    "title": "Investigating the Duality of Interpretability and Explainability in Machine Learning",
    "authors": [
      "Moncef Garouani",
      "Josiane Mothe",
      "Ayah Barhrhouj",
      "Julien Aligon"
    ],
    "abstract": "The rapid evolution of machine learning (ML) has led to the widespread\nadoption of complex \"black box\" models, such as deep neural networks and\nensemble methods. These models exhibit exceptional predictive performance,\nmaking them invaluable for critical decision-making across diverse domains\nwithin society. However, their inherently opaque nature raises concerns about\ntransparency and interpretability, making them untrustworthy decision support\nsystems. To alleviate such a barrier to high-stakes adoption, research\ncommunity focus has been on developing methods to explain black box models as a\nmeans to address the challenges they pose. Efforts are focused on explaining\nthese models instead of developing ones that are inherently interpretable.\nDesigning inherently interpretable models from the outset, however, can pave\nthe path towards responsible and beneficial applications in the field of ML. In\nthis position paper, we clarify the chasm between explaining black boxes and\nadopting inherently interpretable models. We emphasize the imperative need for\nmodel interpretability and, following the purpose of attaining better (i.e.,\nmore effective or efficient w.r.t. predictive performance) and trustworthy\npredictors, provide an experimental evaluation of latest hybrid learning\nmethods that integrates symbolic knowledge into neural network predictors. We\ndemonstrate how interpretable hybrid models could potentially supplant black\nbox ones in different domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "interpretability",
      "explainability",
      "transparency"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21350v1",
    "title": "A Data-Driven Method for INS/DVL Alignment",
    "authors": [
      "Guy Damari",
      "Itzik Klein"
    ],
    "abstract": "Autonomous underwater vehicles (AUVs) are sophisticated robotic platforms\ncrucial for a wide range of applications. The accuracy of AUV navigation\nsystems is critical to their success. Inertial sensors and Doppler velocity\nlogs (DVL) fusion is a promising solution for long-range underwater navigation.\nHowever, the effectiveness of this fusion depends heavily on an accurate\nalignment between the inertial sensors and the DVL. While current alignment\nmethods show promise, there remains significant room for improvement in terms\nof accuracy, convergence time, and alignment trajectory efficiency. In this\nresearch we propose an end-to-end deep learning framework for the alignment\nprocess. By leveraging deep-learning capabilities, such as noise reduction and\ncapture of nonlinearities in the data, we show using simulative data, that our\nproposed approach enhances both alignment accuracy and reduces convergence time\nbeyond current model-based methods.",
    "categories": [
      "cs.RO",
      "cs.SE"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21323v1",
    "title": "DuckSegmentation: A segmentation model based on the AnYue Hemp Duck Dataset",
    "authors": [
      "Ling Feng",
      "Tianyu Xie",
      "Wei Ma",
      "Ruijie Fu",
      "Yingxiao Zhang",
      "Jun Li",
      "Bei Zhou"
    ],
    "abstract": "The modernization of smart farming is a way to improve agricultural\nproduction efficiency, and improve the agricultural production environment.\nAlthough many large models have achieved high accuracy in the task of object\nrecognition and segmentation, they cannot really be put into use in the farming\nindustry due to their own poor interpretability and limitations in\ncomputational volume. In this paper, we built AnYue Shelduck Dateset, which\ncontains a total of 1951 Shelduck datasets, and performed target detection and\nsegmentation annotation with the help of professional annotators. Based on\nAnYue ShelduckDateset, this paper describes DuckProcessing, an efficient and\npowerful module for duck identification based on real shelduckfarms. First of\nall, using the YOLOv8 module designed to divide the mahjong between them,\nPrecision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 on\nthe test set. Again using the DuckSegmentation segmentation model,\nDuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentation\nwas used as the teacher model, and through knowledge distillation, Deeplabv3\nr50 was used as the student model, and the final student model achieved 94.49%\nmIoU on the test set. The method provides a new way of thinking in practical\nsisal duck smart farming.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21321v1",
    "title": "Explainable Boosting Machine for Predicting Claim Severity and Frequency in Car Insurance",
    "authors": [
      "MarkÃ©ta KrÃ¹povÃ ",
      "Nabil Rachdi",
      "Quentin Guibert"
    ],
    "abstract": "In a context of constant increase in competition and heightened regulatory\npressure, accuracy, actuarial precision, as well as transparency and\nunderstanding of the tariff, are key issues in non-life insurance.\nTraditionally used generalized linear models (GLM) result in a multiplicative\ntariff that favors interpretability. With the rapid development of machine\nlearning and deep learning techniques, actuaries and the rest of the insurance\nindustry have adopted these techniques widely. However, there is a need to\nassociate them with interpretability techniques. In this paper, our study\nfocuses on introducing an Explainable Boosting Machine (EBM) model that\ncombines intrinsically interpretable characteristics and high prediction\nperformance. This approach is described as a glass-box model and relies on the\nuse of a Generalized Additive Model (GAM) and a cyclic gradient boosting\nalgorithm. It accounts for univariate and pairwise interaction effects between\nfeatures and provides naturally explanations on them. We implement this\napproach on car insurance frequency and severity data and extensively compare\nthe performance of this approach with classical competitors: a GLM, a GAM, a\nCART model and an Extreme Gradient Boosting (XGB) algorithm. Finally, we\nexamine the interpretability of these models to capture the main determinants\nof claim costs.",
    "categories": [
      "stat.AP",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "stat.AP",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "interpretability",
      "transparency"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21315v1",
    "title": "Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack",
    "authors": [
      "Cheng Wang",
      "Yiwei Wang",
      "Yujun Cai",
      "Bryan Hooi"
    ],
    "abstract": "Retrieval-augmented generation (RAG) systems enhance large language models by\nincorporating external knowledge, addressing issues like outdated internal\nknowledge and hallucination. However, their reliance on external knowledge\nbases makes them vulnerable to corpus poisoning attacks, where adversarial\npassages can be injected to manipulate retrieval results. Existing methods for\ncrafting such passages, such as random token replacement or training inversion\nmodels, are often slow and computationally expensive, requiring either access\nto retriever's gradients or large computational resources. To address these\nlimitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an\nefficient black-box method that leverages two key properties of retrievers:\ninsensitivity to token order and bias towards influential tokens. By focusing\non these characteristics, DIGA dynamically adjusts its genetic operations to\ngenerate effective adversarial passages with significantly reduced time and\nmemory usage. Our experimental evaluation shows that DIGA achieves superior\nefficiency and scalability compared to existing methods, while maintaining\ncomparable or better attack success rates across multiple datasets.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21305v1",
    "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
    "authors": [
      "Dorde Popovic",
      "Amin Sadeghi",
      "Ting Yu",
      "Sanjay Chawla",
      "Issa Khalil"
    ],
    "abstract": "Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21303v1",
    "title": "Simulation-informed deep learning for enhanced SWOT observations of fine-scale ocean dynamics",
    "authors": [
      "Eugenio Cutolo",
      "Carlos Granero-Belinchon",
      "Ptashanna Thiraux",
      "Jinbo Wang",
      "Ronan Fablet"
    ],
    "abstract": "Oceanic processes at fine scales are crucial yet difficult to observe\naccurately due to limitations in satellite and in-situ measurements. The\nSurface Water and Ocean Topography (SWOT) mission provides high-resolution Sea\nSurface Height (SSH) data, though noise patterns often obscure fine scale\nstructures. Current methods struggle with noisy data or require extensive\nsupervised training, limiting their effectiveness on real-world observations.\nWe introduce SIMPGEN (Simulation-Informed Metric and Prior for Generative\nEnsemble Networks), an unsupervised adversarial learning framework combining\nreal SWOT observations with simulated reference data. SIMPGEN leverages\nwavelet-informed neural metrics to distinguish noisy from clean fields, guiding\nrealistic SSH reconstructions. Applied to SWOT data, SIMPGEN effectively\nremoves noise, preserving fine-scale features better than existing neural\nmethods. This robust, unsupervised approach not only improves SWOT SSH data\ninterpretation but also demonstrates strong potential for broader oceanographic\napplications, including data assimilation and super-resolution.",
    "categories": [
      "physics.ao-ph",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "physics.ao-ph",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21293v1",
    "title": "Lidar-only Odometry based on Multiple Scan-to-Scan Alignments over a Moving Window",
    "authors": [
      "Aaron Kurda",
      "Simon Steuernagel",
      "Marcus Baum"
    ],
    "abstract": "Lidar-only odometry considers the pose estimation of a mobile robot based on\nthe accumulation of motion increments extracted from consecutive lidar scans.\nMany existing approaches to the problem use a scan-to-map registration, which\nneglects the accumulation of errors within the maintained map due to drift.\nOther methods use a refinement step that jointly optimizes the local map on a\nfeature basis. We propose a solution that avoids this by using multiple\nindependent scan-to-scan Iterative Closest Points (ICP) registrations to\nprevious scans in order to derive constraints for a pose graph. The\noptimization of the pose graph then not only yields an accurate estimate for\nthe latest pose, but also enables the refinement of previous scans in the\noptimization window. By avoiding the need to recompute the scan-to-scan\nalignments, the computational load is minimized. Extensive evaluation on the\npublic KITTI and MulRan datasets as well as on a custom automotive lidar\ndataset is carried out. Results show that the proposed approach achieves\nstate-of-the-art estimation accuracy, while alleviating the mentioned issues.",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21288v1",
    "title": "Haptic bilateral teleoperation system for free-hand dental procedures",
    "authors": [
      "Lorenzo Pagliara",
      "Enrico Ferrentino",
      "Andrea Chiacchio",
      "Giovanni Russo"
    ],
    "abstract": "Free-hand dental procedures are typically repetitive, time-consuming and\nrequire high precision and manual dexterity. Dental robots can play a key role\nin improving procedural accuracy and safety, enhancing patient comfort, and\nreducing operator workload. However, robotic solutions for free-hand procedures\nremain limited or completely lacking, and their acceptance is still low. To\naddress this gap, we develop a haptic bilateral teleoperation system (HBTS) for\nfree-hand dental procedures. The system includes a dedicated mechanical\nend-effector, compatible with standard clinical tools, and equipped with an\nendoscopic camera for improved visibility of the intervention site. By ensuring\nmotion and force correspondence between the operator's actions and the robot's\nmovements, monitored through visual feedback, we enhance the operator's sensory\nawareness and motor accuracy. Furthermore, recognizing the need to ensure\nprocedural safety, we limit interaction forces by scaling the motion references\nprovided to the admittance controller based solely on measured contact forces.\nThis ensures effective force limitation in all contact states without requiring\nprior knowledge of the environment. The proposed HBTS is validated in a dental\nscaling procedure using a dental phantom. The results show that the system\nimproves the naturalness, safety, and accuracy of teleoperation, highlighting\nits potential to enhance free-hand dental procedures.",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21259v1",
    "title": "Reducing CT Metal Artifacts by Learning Latent Space Alignment with Gemstone Spectral Imaging Data",
    "authors": [
      "Wencheng Han",
      "Dongqian Guo",
      "Xiao Chen",
      "Pang Lyu",
      "Yi Jin",
      "Jianbing Shen"
    ],
    "abstract": "Metal artifacts in CT slices have long posed challenges in medical\ndiagnostics. These artifacts degrade image quality, resulting in suboptimal\nvisualization and complicating the accurate interpretation of tissues adjacent\nto metal implants. To address these issues, we introduce the Latent Gemstone\nSpectral Imaging (GSI) Alignment Framework, which effectively reduces metal\nartifacts while avoiding the introduction of noise information. Our work is\nbased on a key finding that even artifact-affected ordinary CT sequences\ncontain sufficient information to discern detailed structures. The challenge\nlies in the inability to clearly represent this information. To address this\nissue, we developed an Alignment Framework that adjusts the representation of\nordinary CT images to match GSI CT sequences. GSI is an advanced imaging\ntechnique using multiple energy levels to mitigate artifacts caused by metal\nimplants. By aligning the representation to GSI data, we can effectively\nsuppress metal artifacts while clearly revealing detailed structure, without\nintroducing extraneous information into CT sequences. To facilitate the\napplication, we propose a new dataset, Artifacts-GSI, captured from real\npatients with metal implants, and establish a new benchmark based on this\ndataset. Experimental results show that our method significantly reduces metal\nartifacts and greatly enhances the readability of CT slices. All our code and\ndata are available at: https://um-lab.github.io/GSI-MAR/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21250v1",
    "title": "Orange Quality Grading with Deep Learning",
    "authors": [
      "Mohamed Lamine Mekhalfi",
      "Paul Chippendale",
      "Francisco Fraile",
      "Marcos Rico"
    ],
    "abstract": "Orange grading is a crucial step in the fruit industry, as it helps to sort\noranges according to different criteria such as size, quality, ripeness, and\nhealth condition, ensuring safety for human consumption and better price\nallocation and client satisfaction. Automated grading enables faster\nprocessing, precision, and reduced human labor. In this paper, we implement a\ndeep learning-based solution for orange grading via machine vision. Unlike\ntypical grading systems that analyze fruits from a single view, we capture\nmultiview images of each single orange in order to enable a richer\nrepresentation. Afterwards, we compose the acquired images into one collage.\nThis enables the analysis of the whole orange skin. We train a convolutional\nneural network (CNN) on the composed images to grade the oranges into three\nclasses, namely good, bad, and undefined. We also evaluate the performance with\ntwo different CNNs (ResNet-18 and SqueezeNet). We show experimentally that\nmulti-view grading is superior to single view grading.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21237v1",
    "title": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval",
    "authors": [
      "Karanbir Singh",
      "William Ngu"
    ],
    "abstract": "Advancements in retrieving accessible information have evolved faster in the\nlast few years compared to the decades since the internet's creation. Search\nengines, like Google, have been the number one way to find relevant data. They\nhave always relied on the user's abilities to find the best information in its\nbillions of links and sources at everybody's fingertips. The advent of large\nlanguage models (LLMs) has completely transformed the field of information\nretrieval. The LLMs excel not only at retrieving relevant knowledge but also at\nsummarizing it effectively, making information more accessible and consumable\nfor users. On top of it, the rise of AI Agents has introduced another aspect to\ninformation retrieval i.e. dynamic information retrieval which enables the\nintegration of real-time data such as weather forecasts, and financial data\nwith the knowledge base to curate context-aware knowledge. However, despite\nthese advancements the agents remain susceptible to issues of bias and\nfairness, challenges deeply rooted within the knowledge base and training of\nLLMs. This study introduces a novel approach to bias-aware knowledge retrieval\nby leveraging agentic framework and the innovative use of bias detectors as\ntools to identify and highlight inherent biases in the retrieved content. By\nempowering users with transparency and awareness, this approach aims to foster\nmore equitable information systems and promote the development of responsible\nAI.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "transparency"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21236v1",
    "title": "Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing",
    "authors": [
      "Shuai Li",
      "Jie Zhang",
      "Yuang Qi",
      "Kejiang Chen",
      "Tianwei Zhang",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "abstract": "Large-scale image retrieval using deep hashing has become increasingly\npopular due to the exponential growth of image data and the remarkable feature\nextraction capabilities of deep neural networks (DNNs). However, deep hashing\nmethods are vulnerable to malicious attacks, including adversarial and backdoor\nattacks. It is worth noting that these attacks typically involve altering the\nquery images, which is not a practical concern in real-world scenarios. In this\npaper, we point out that even clean query images can be dangerous, inducing\nmalicious target retrieval results, like undesired or illegal images. To the\nbest of our knowledge, we are the first to study data \\textbf{p}oisoning\n\\textbf{a}ttacks against \\textbf{d}eep \\textbf{hash}ing\n\\textbf{(\\textit{PADHASH})}. Specifically, we first train a surrogate model to\nsimulate the behavior of the target deep hashing model. Then, a strict gradient\nmatching strategy is proposed to generate the poisoned images. Extensive\nexperiments on different models, datasets, hash methods, and hash code lengths\ndemonstrate the effectiveness and generality of our attack method.",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21226v1",
    "title": "Frequency-Aware Gaussian Splatting Decomposition",
    "authors": [
      "Yishai Lavi",
      "Leo Segre",
      "Shai Avidan"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has revolutionized novel view synthesis with\nits efficient, explicit representation. However, it lacks frequency\ninterpretability, making it difficult to separate low-frequency structures from\nfine details. We introduce a frequency-decomposed 3D-GS framework that groups\n3D Gaussians that correspond to subbands in the Laplacian Pyrmaids of the input\nimages. Our approach enforces coherence within each subband (i.e., group of 3D\nGaussians) through dedicated regularization, ensuring well-separated frequency\ncomponents. We extend color values to both positive and negative ranges,\nallowing higher-frequency layers to add or subtract residual details. To\nstabilize optimization, we employ a progressive training scheme that refines\ndetails in a coarse-to-fine manner. Beyond interpretability, this\nfrequency-aware design unlocks a range of practical benefits. Explicit\nfrequency separation enables advanced 3D editing and stylization, allowing\nprecise manipulation of specific frequency bands. It also supports dynamic\nlevel-of-detail control for progressive rendering, streaming, foveated\nrendering and fast geometry interaction. Through extensive experiments, we\ndemonstrate that our method provides improved control and flexibility for\nemerging applications in scene editing and interactive rendering. Our code will\nbe made publicly available.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21219v1",
    "title": "GenFusion: Closing the Loop between Reconstruction and Generation via Videos",
    "authors": [
      "Sibo Wu",
      "Congrong Xu",
      "Binbin Huang",
      "Andreas Geiger",
      "Anpei Chen"
    ],
    "abstract": "Recently, 3D reconstruction and generation have demonstrated impressive novel\nview synthesis results, achieving high fidelity and efficiency. However, a\nnotable conditioning gap can be observed between these two fields, e.g.,\nscalable 3D scene reconstruction often requires densely captured views, whereas\n3D generation typically relies on a single or no input view, which\nsignificantly limits their applications. We found that the source of this\nphenomenon lies in the misalignment between 3D constraints and generative\npriors. To address this problem, we propose a reconstruction-driven video\ndiffusion model that learns to condition video frames on artifact-prone RGB-D\nrenderings. Moreover, we propose a cyclical fusion pipeline that iteratively\nadds restoration frames from the generative model to the training set, enabling\nprogressive expansion and addressing the viewpoint saturation limitations seen\nin previous reconstruction and generation pipelines. Our evaluation, including\nview synthesis from sparse view and masked input, validates the effectiveness\nof our approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment",
      "misalignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21211v1",
    "title": "Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years",
    "authors": [
      "Rixu Hao",
      "Yuxin Zhao",
      "Shaoqing Zhang",
      "Guihua Wang",
      "Xiong Deng"
    ],
    "abstract": "El Ni\\~no-Southern Oscillation (ENSO) exerts global climate and societal\nimpacts, but real-time prediction with lead times beyond one year remains\nchallenging. Dynamical models suffer from large biases and uncertainties, while\ndeep learning struggles with interpretability and multi-scale dynamics. Here,\nwe introduce PTSTnet, an interpretable model that unifies dynamical processes\nand cross-scale spatiotemporal learning in an innovative neural-network\nframework with physics-encoding learning. PTSTnet produces interpretable\npredictions significantly outperforming state-of-the-art benchmarks with lead\ntimes beyond 24 months, providing physical insights into error propagation in\nocean-atmosphere interactions. PTSTnet learns feature representations with\nphysical consistency from sparse data to tackle inherent multi-scale and\nmulti-physics challenges underlying ocean-atmosphere processes, thereby\ninherently enhancing long-term prediction skill. Our successful realizations\nmark substantial steps forward in interpretable insights into innovative neural\nocean modelling.",
    "categories": [
      "physics.ao-ph",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21190v1",
    "title": "Leveraging LLMs with Iterative Loop Structure for Enhanced Social Intelligence in Video Question Answering",
    "authors": [
      "Erika Mori",
      "Yue Qiu",
      "Hirokatsu Kataoka",
      "Yoshimitsu Aoki"
    ],
    "abstract": "Social intelligence, the ability to interpret emotions, intentions, and\nbehaviors, is essential for effective communication and adaptive responses. As\nrobots and AI systems become more prevalent in caregiving, healthcare, and\neducation, the demand for AI that can interact naturally with humans grows.\nHowever, creating AI that seamlessly integrates multiple modalities, such as\nvision and speech, remains a challenge. Current video-based methods for social\nintelligence rely on general video recognition or emotion recognition\ntechniques, often overlook the unique elements inherent in human interactions.\nTo address this, we propose the Looped Video Debating (LVD) framework, which\nintegrates Large Language Models (LLMs) with visual information, such as facial\nexpressions and body movements, to enhance the transparency and reliability of\nquestion-answering tasks involving human interaction videos. Our results on the\nSocial-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance\nwithout fine-tuning. Furthermore, supplementary human annotations on existing\ndatasets provide insights into the model's accuracy, guiding future\nimprovements in AI-driven social intelligence.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "transparency"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21164v1",
    "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
    "authors": [
      "Samra Irshad",
      "Seungkyu Lee",
      "Nassir Navab",
      "Hong Joo Lee",
      "Seong Tae Kim"
    ],
    "abstract": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety",
      "adversarial",
      "adversarial example"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21158v1",
    "title": "Integrating Travel Behavior Forecasting and Generative Modeling for Predicting Future Urban Mobility and Spatial Transformations",
    "authors": [
      "Eugene Denteh",
      "Andrews Danyo",
      "Joshua Kofi Asamoah",
      "Blessing Agyei Kyem",
      "Twitchell Addai",
      "Armstrong Aboah"
    ],
    "abstract": "Transportation planning plays a critical role in shaping urban development,\neconomic mobility, and infrastructure sustainability. However, traditional\nplanning methods often struggle to accurately predict long-term urban growth\nand transportation demands. This may sometimes result in infrastructure\ndemolition to make room for current transportation planning demands. This study\nintegrates a Temporal Fusion Transformer to predict travel patterns from\ndemographic data with a Generative Adversarial Network to predict future urban\nsettings through satellite imagery. The framework achieved a 0.76 R-square\nscore in travel behavior prediction and generated high-fidelity satellite\nimages with a Structural Similarity Index of 0.81. The results demonstrate that\nintegrating predictive analytics and spatial visualization can significantly\nimprove the decision-making process, fostering more sustainable and efficient\nurban development. This research highlights the importance of data-driven\nmethodologies in modern transportation planning and presents a step toward\noptimizing infrastructure placement, capacity, and long-term viability.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21141v1",
    "title": "Safe Human Robot Navigation in Warehouse Scenario",
    "authors": [
      "Seth Farrell",
      "Chenghao Li",
      "Hongzhan Yu",
      "Ryo Yoshimitsu",
      "Sicun Gao",
      "Henrik I. Christensen"
    ],
    "abstract": "The integration of autonomous mobile robots (AMRs) in industrial\nenvironments, particularly warehouses, has revolutionized logistics and\noperational efficiency. However, ensuring the safety of human workers in\ndynamic, shared spaces remains a critical challenge. This work proposes a novel\nmethodology that leverages control barrier functions (CBFs) to enhance safety\nin warehouse navigation. By integrating learning-based CBFs with the Open\nRobotics Middleware Framework (OpenRMF), the system achieves adaptive and\nsafety-enhanced controls in multi-robot, multi-agent scenarios. Experiments\nconducted using various robot platforms demonstrate the efficacy of the\nproposed approach in avoiding static and dynamic obstacles, including human\npedestrians. Our experiments evaluate different scenarios in which the number\nof robots, robot platforms, speed, and number of obstacles are varied, from\nwhich we achieve promising performance.",
    "categories": [
      "cs.RO",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21106v1",
    "title": "Function Alignment: A New Theory for Mind and Intelligence, Part I: Foundations",
    "authors": [
      "Gus G. Xia"
    ],
    "abstract": "This paper introduces function alignment, a novel theory of mind and\nintelligence that is both intuitively compelling and structurally grounded. It\nexplicitly models how meaning, interpretation, and analogy emerge from\ninteractions among layered representations, forming a coherent framework\ncapable not only of modeling minds but also of serving as a blueprint for\nbuilding them. One of the key theoretical insights derived from function\nalignment is bounded interpretability, which provides a unified explanation for\npreviously fragmented ideas in cognitive science, such as bounded rationality,\nsymbol grounding, and analogy-making. Beyond modeling, the function alignment\nframework bridges disciplines often kept apart, linking computational\narchitecture, psychological theory, and even contemplative traditions such as\nZen. Rather than building on any philosophical systems, it offers a structural\nfoundation upon which multiple ways of understanding the mind may be\nreconstructed.",
    "categories": [
      "cs.CL",
      "68T27, 91E45",
      "I.2.0; I.2.4; F.4.1"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment",
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21082v1",
    "title": "Can Video Diffusion Model Reconstruct 4D Geometry?",
    "authors": [
      "Jinjie Mai",
      "Wenxuan Zhu",
      "Haozhe Liu",
      "Bing Li",
      "Cheng Zheng",
      "JÃ¼rgen Schmidhuber",
      "Bernard Ghanem"
    ],
    "abstract": "Reconstructing dynamic 3D scenes (i.e., 4D geometry) from monocular video is\nan important yet challenging problem. Conventional multiview geometry-based\napproaches often struggle with dynamic motion, whereas recent learning-based\nmethods either require specialized 4D representation or sophisticated\noptimization. In this paper, we present Sora3R, a novel framework that taps\ninto the rich spatiotemporal priors of large-scale video diffusion models to\ndirectly infer 4D pointmaps from casual videos. Sora3R follows a two-stage\npipeline: (1) we adapt a pointmap VAE from a pretrained video VAE, ensuring\ncompatibility between the geometry and video latent spaces; (2) we finetune a\ndiffusion backbone in combined video and pointmap latent space to generate\ncoherent 4D pointmaps for every frame. Sora3R operates in a fully feedforward\nmanner, requiring no external modules (e.g., depth, optical flow, or\nsegmentation) or iterative global alignment. Extensive experiments demonstrate\nthat Sora3R reliably recovers both camera poses and detailed scene geometry,\nachieving performance on par with state-of-the-art methods for dynamic 4D\nreconstruction across diverse scenarios.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21073v1",
    "title": "Shared Global and Local Geometry of Language Model Embeddings",
    "authors": [
      "Andrew Lee",
      "Melanie Weber",
      "Fernanda ViÃ©gas",
      "Martin Wattenberg"
    ],
    "abstract": "Researchers have recently suggested that models share common representations.\nIn this work, we find that the token embeddings of language models exhibit\ncommon geometric structure. First, we find ``global'' similarities: token\nembeddings often share similar relative orientations. Next, we characterize\nlocal geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by\ndefining a simple measure for the intrinsic dimension of each token embedding.\nOur intrinsic dimension measure demonstrates that token embeddings lie on a\nlower dimensional manifold. We qualitatively show that tokens with lower\nintrinsic dimensions often have semantically coherent clusters, while those\nwith higher intrinsic dimensions do not. Both characterizations allow us to\nfind similarities in the local geometry of token embeddings. Perhaps most\nsurprisingly, we find that alignment in token embeddings persists through the\nhidden states of language models, allowing us to develop an application for\ninterpretability. Namely, we empirically demonstrate that steering vectors from\none language model can be transferred to another, despite the two models having\ndifferent dimensions.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment",
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.21061v1",
    "title": "Neural Architecture Search by Learning a Hierarchical Search Space",
    "authors": [
      "Mehraveh Javan Roshtkhari",
      "Matthew Toews",
      "Marco Pedersoli"
    ],
    "abstract": "Monte-Carlo Tree Search (MCTS) is a powerful tool for many non-differentiable\nsearch related problems such as adversarial games. However, the performance of\nsuch approach highly depends on the order of the nodes that are considered at\neach branching of the tree. If the first branches cannot distinguish between\npromising and deceiving configurations for the final task, the efficiency of\nthe search is exponentially reduced. In Neural Architecture Search (NAS), as\nonly the final architecture matters, the visiting order of the branching can be\noptimized to improve learning. In this paper, we study the application of MCTS\nto NAS for image classification. We analyze several sampling methods and\nbranching alternatives for MCTS and propose to learn the branching by\nhierarchical clustering of architectures based on their similarity. The\nsimilarity is measured by the pairwise distance of output vectors of\narchitectures. Extensive experiments on two challenging benchmarks on CIFAR10\nand ImageNet show that MCTS, if provided with a good branching hierarchy, can\nyield promising solutions more efficiently than other approaches for NAS\nproblems.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-27",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20995v1",
    "title": "Multi-head Reward Aggregation Guided by Entropy",
    "authors": [
      "Xiaomin Li",
      "Xupeng Chen",
      "Jingxuan Fan",
      "Eric Hanchen Jiang",
      "Mingye Gao"
    ],
    "abstract": "Aligning large language models (LLMs) with safety guidelines typically\ninvolves reinforcement learning from human feedback (RLHF), relying on\nhuman-generated preference annotations. However, assigning consistent overall\nquality ratings is challenging, prompting recent research to shift towards\ndetailed evaluations based on multiple specific safety criteria. This paper\nuncovers a consistent observation: safety rules characterized by high rating\nentropy are generally less reliable in identifying responses preferred by\nhumans. Leveraging this finding, we introduce ENCORE, a straightforward\nentropy-guided approach that composes multi-head rewards by downweighting rules\nexhibiting high rating entropy. Theoretically, we demonstrate that rules with\nelevated entropy naturally receive minimal weighting in the Bradley-Terry\noptimization framework, justifying our entropy-based penalization. Through\nextensive experiments on RewardBench safety tasks, our method significantly\nsurpasses several competitive baselines, including random weighting, uniform\nweighting, single-head Bradley-Terry models, and LLM-based judging methods. Our\nproposed approach is training-free, broadly applicable to various datasets, and\nmaintains interpretability, offering a practical and effective solution for\nmulti-attribute reward modeling.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "interpretability",
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20988v1",
    "title": "Cross-Modal State-Space Graph Reasoning for Structured Summarization",
    "authors": [
      "Hannah Kim",
      "Sofia Martinez",
      "Jason Lee"
    ],
    "abstract": "The ability to extract compact, meaningful summaries from large-scale and\nmultimodal data is critical for numerous applications, ranging from video\nanalytics to medical reports. Prior methods in cross-modal summarization have\noften suffered from high computational overheads and limited interpretability.\nIn this paper, we propose a \\textit{Cross-Modal State-Space Graph Reasoning}\n(\\textbf{CSS-GR}) framework that incorporates a state-space model with\ngraph-based message passing, inspired by prior work on efficient state-space\nmodels. Unlike existing approaches relying on purely sequential models, our\nmethod constructs a graph that captures inter- and intra-modal relationships,\nallowing more holistic reasoning over both textual and visual streams. We\ndemonstrate that our approach significantly improves summarization quality and\ninterpretability while maintaining computational efficiency, as validated on\nstandard multimodal summarization benchmarks. We also provide a thorough\nablation study to highlight the contributions of each component.",
    "categories": [
      "cs.CL",
      "cs.GR"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20953v1",
    "title": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance",
    "authors": [
      "Julia Ive",
      "Felix Jozsa",
      "Nick Jackson",
      "Paulina Bondaronek",
      "Ciaran Scott Hill",
      "Richard Dobson"
    ],
    "abstract": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 0.98 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20919v1",
    "title": "GatedxLSTM: A Multimodal Affective Computing Approach for Emotion Recognition in Conversations",
    "authors": [
      "Yupei Li",
      "Qiyang Sun",
      "Sunil Munthumoduku Krishna Murthy",
      "Emran Alturki",
      "BjÃ¶rn W. Schuller"
    ],
    "abstract": "Affective Computing (AC) is essential for advancing Artificial General\nIntelligence (AGI), with emotion recognition serving as a key component.\nHowever, human emotions are inherently dynamic, influenced not only by an\nindividual's expressions but also by interactions with others, and\nsingle-modality approaches often fail to capture their full dynamics.\nMultimodal Emotion Recognition (MER) leverages multiple signals but\ntraditionally relies on utterance-level analysis, overlooking the dynamic\nnature of emotions in conversations. Emotion Recognition in Conversation (ERC)\naddresses this limitation, yet existing methods struggle to align multimodal\nfeatures and explain why emotions evolve within dialogues. To bridge this gap,\nwe propose GatedxLSTM, a novel speech-text multimodal ERC model that explicitly\nconsiders voice and transcripts of both the speaker and their conversational\npartner(s) to identify the most influential sentences driving emotional shifts.\nBy integrating Contrastive Language-Audio Pretraining (CLAP) for improved\ncross-modal alignment and employing a gating mechanism to emphasise emotionally\nimpactful utterances, GatedxLSTM enhances both interpretability and\nperformance. Additionally, the Dialogical Emotion Decoder (DED) refines emotion\npredictions by modelling contextual dependencies. Experiments on the IEMOCAP\ndataset demonstrate that GatedxLSTM achieves state-of-the-art (SOTA)\nperformance among open-source methods in four-class emotion classification.\nThese results validate its effectiveness for ERC applications and provide an\ninterpretability analysis from a psychological perspective.",
    "categories": [
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment",
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20916v1",
    "title": "A Study of Perceived Safety for Soft Robotics in Caregiving Tasks",
    "authors": [
      "Cosima du Pasquier",
      "Jennifer Grannen",
      "Chuer Pan",
      "Serin L. Huber",
      "Aliyah Smith",
      "Monroe Kennedy",
      "Shuran Song",
      "Dorsa Sadigh",
      "Allison M. Okamura"
    ],
    "abstract": "In this project, we focus on human-robot interaction in caregiving scenarios\nlike bathing, where physical contact is inevitable and necessary for proper\ntask execution because force must be applied to the skin. Using finite element\nanalysis, we designed a 3D-printed gripper combining positive and negative\npressure for secure yet compliant handling. Preliminary tests showed it exerted\na lower, more uniform pressure profile than a standard rigid gripper. In a user\nstudy, participants' trust in robots significantly increased after they\nexperienced a brief bathing demonstration performed by a robotic arm equipped\nwith the soft gripper. These results suggest that soft robotics can enhance\nperceived safety and acceptance in intimate caregiving scenarios.",
    "categories": [
      "cs.RO",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20884v1",
    "title": "Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework",
    "authors": [
      "Usama Zafar",
      "AndrÃ© Teixeira",
      "Salman Toor"
    ],
    "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20880v1",
    "title": "BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain IHC Computational Pathology",
    "authors": [
      "Amaya Gallagher-Syed",
      "Henry Senior",
      "Omnia Alwazzan",
      "Elena Pontarini",
      "Michele Bombardieri",
      "Costantino Pitzalis",
      "Myles J. Lewis",
      "Michael R. Barnes",
      "Luca Rossi",
      "Gregory Slabaugh"
    ],
    "abstract": "The development of biologically interpretable and explainable models remains\na key challenge in computational pathology, particularly for multistain\nimmunohistochemistry (IHC) analysis. We present BioX-CPath, an explainable\ngraph neural network architecture for whole slide image (WSI) classification\nthat leverages both spatial and semantic features across multiple stains. At\nits core, BioX-CPath introduces a novel Stain-Aware Attention Pooling (SAAP)\nmodule that generates biologically meaningful, stain-aware patient embeddings.\nOur approach achieves state-of-the-art performance on both Rheumatoid Arthritis\nand Sjogren's Disease multistain datasets. Beyond performance metrics,\nBioX-CPath provides interpretable insights through stain attention scores,\nentropy measures, and stain interaction scores, that permit measuring model\nalignment with known pathological mechanisms. This biological grounding,\ncombined with strong classification performance, makes BioX-CPath particularly\nsuitable for clinical applications where interpretability is key. Source code\nand documentation can be found at: https://github.com/AmayaGS/BioX-CPath.",
    "categories": [
      "cs.CV",
      "q-bio.CB",
      "q-bio.QM",
      "q-bio.TO"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment",
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20879v1",
    "title": "Quantum advantage for learning shallow neural networks with natural data distributions",
    "authors": [
      "Laura Lewis",
      "Dar Gilboa",
      "Jarrod R. McClean"
    ],
    "abstract": "The application of quantum computers to machine learning tasks is an exciting\npotential direction to explore in search of quantum advantage. In the absence\nof large quantum computers to empirically evaluate performance, theoretical\nframeworks such as the quantum probably approximately correct (PAC) and quantum\nstatistical query (QSQ) models have been proposed to study quantum algorithms\nfor learning classical functions. Despite numerous works investigating quantum\nadvantage in these models, we nevertheless only understand it at two extremes:\neither exponential quantum advantages for uniform input distributions or no\nadvantage for potentially adversarial distributions. In this work, we study the\ngap between these two regimes by designing an efficient quantum algorithm for\nlearning periodic neurons in the QSQ model over a broad range of non-uniform\ndistributions, which includes Gaussian, generalized Gaussian, and logistic\ndistributions. To our knowledge, our work is also the first result in quantum\nlearning theory for classical functions that explicitly considers real-valued\nfunctions. Recent advances in classical learning theory prove that learning\nperiodic neurons is hard for any classical gradient-based algorithm, giving us\nan exponential quantum advantage over such algorithms, which are the standard\nworkhorses of machine learning. Moreover, in some parameter regimes, the\nproblem remains hard for classical statistical query algorithms and even\ngeneral classical algorithms learning under small amounts of noise.",
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20871v1",
    "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives",
    "authors": [
      "Silin Gao",
      "Sheryl Mathew",
      "Li Mi",
      "Sepideh Mamooler",
      "Mengjie Zhao",
      "Hiromi Wakaki",
      "Yuki Mitsufuji",
      "Syrielle Montariol",
      "Antoine Bosselut"
    ],
    "abstract": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20782v1",
    "title": "Zero-Shot Audio-Visual Editing via Cross-Modal Delta Denoising",
    "authors": [
      "Yan-Bo Lin",
      "Kevin Lin",
      "Zhengyuan Yang",
      "Linjie Li",
      "Jianfeng Wang",
      "Chung-Ching Lin",
      "Xiaofei Wang",
      "Gedas Bertasius",
      "Lijuan Wang"
    ],
    "abstract": "In this paper, we introduce zero-shot audio-video editing, a novel task that\nrequires transforming original audio-visual content to align with a specified\ntextual prompt without additional model training. To evaluate this task, we\ncurate a benchmark dataset, AvED-Bench, designed explicitly for zero-shot\naudio-video editing. AvED-Bench includes 110 videos, each with a 10-second\nduration, spanning 11 categories from VGGSound. It offers diverse prompts and\nscenarios that require precise alignment between auditory and visual elements,\nenabling robust evaluation. We identify limitations in existing zero-shot audio\nand video editing methods, particularly in synchronization and coherence\nbetween modalities, which often result in inconsistent outcomes. To address\nthese challenges, we propose AvED, a zero-shot cross-modal delta denoising\nframework that leverages audio-video interactions to achieve synchronized and\ncoherent edits. AvED demonstrates superior results on both AvED-Bench and the\nrecent OAVE dataset to validate its generalization capabilities. Results are\navailable at https://genjib.github.io/project_page/AVED/index.html",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20737v1",
    "title": "Ontology-based Semantic Similarity Measures for Clustering Medical Concepts in Drug Safety",
    "authors": [
      "Jeffery L Painter",
      "FranÃ§ois Haguinet",
      "Gregory E Powell",
      "Andrew Bate"
    ],
    "abstract": "Semantic similarity measures (SSMs) are widely used in biomedical research\nbut remain underutilized in pharmacovigilance. This study evaluates six\nontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety\ndata. Using the Unified Medical Language System (UMLS), we assess each method's\nability to group PTs around medically meaningful centroids. A high-throughput\nframework was developed with a Java API and Python and R interfaces support\nlarge-scale similarity computations. Results show that while path-based methods\nperform moderately with F1 scores of 0.36 for WUPALMER and 0.28 for LCH,\nintrinsic information content (IC)-based measures, especially INTRINSIC-LIN and\nSOKAL, consistently yield better clustering accuracy (F1 score of 0.403).\nValidated against expert review and standard MedDRA queries (SMQs), our\nfindings highlight the promise of IC-based SSMs in enhancing pharmacovigilance\nworkflows by improving early signal detection and reducing manual review.",
    "categories": [
      "cs.CL",
      "I.2.4; G.3; H.3.3"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20730v1",
    "title": "Benchmarking and optimizing organism wide single-cell RNA alignment methods",
    "authors": [
      "Juan Javier Diaz-Mejia",
      "Elias Williams",
      "Octavian Focsa",
      "Dylan Mendonca",
      "Swechha Singh",
      "Brendan Innes",
      "Sam Cooper"
    ],
    "abstract": "Many methods have been proposed for removing batch effects and aligning\nsingle-cell RNA (scRNA) datasets. However, performance is typically evaluated\nbased on multiple parameters and few datasets, creating challenges in assessing\nwhich method is best for aligning data at scale. Here, we introduce the\nK-Neighbors Intersection (KNI) score, a single score that both penalizes batch\neffects and measures accuracy at cross-dataset cell-type label prediction\nalongside carefully curated small (scMARK) and large (scREF) benchmarks\ncomprising 11 and 46 human scRNA studies respectively, where we have\nstandardized author labels. Using the KNI score, we evaluate and optimize\napproaches for cross-dataset single-cell RNA integration. We introduce Batch\nAdversarial single-cell Variational Inference (BA-scVI), as a new variant of\nscVI that uses adversarial training to penalize batch-effects in the encoder\nand decoder, and show this approach outperforms other methods. In the resulting\naligned space, we find that the granularity of cell-type groupings is\nconserved, supporting the notion that whole-organism cell-type maps can be\ncreated by a single model without loss of information.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment",
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20848v1",
    "title": "The Backfiring Effect of Weak AI Safety Regulation",
    "authors": [
      "Benjamin Laufer",
      "Jon Kleinberg",
      "Hoda Heidari"
    ],
    "abstract": "Recent policy proposals aim to improve the safety of general-purpose AI, but\nthere is little understanding of the efficacy of different regulatory\napproaches to AI safety. We present a strategic model that explores the\ninteractions between the regulator, the general-purpose AI technology creators,\nand domain specialists--those who adapt the AI for specific applications. Our\nanalysis examines how different regulatory measures, targeting different parts\nof the development chain, affect the outcome of the development process. In\nparticular, we assume AI technology is described by two key attributes: safety\nand performance. The regulator first sets a minimum safety standard that\napplies to one or both players, with strict penalties for non-compliance. The\ngeneral-purpose creator then develops the technology, establishing its initial\nsafety and performance levels. Next, domain specialists refine the AI for their\nspecific use cases, and the resulting revenue is distributed between the\nspecialist and generalist through an ex-ante bargaining process. Our analysis\nof this game reveals two key insights: First, weak safety regulation imposed\nonly on the domain specialists can backfire. While it might seem logical to\nregulate use cases (as opposed to the general-purpose technology), our analysis\nshows that weak regulations targeting domain specialists alone can\nunintentionally reduce safety. This effect persists across a wide range of\nsettings. Second, in sharp contrast to the previous finding, we observe that\nstronger, well-placed regulation can in fact benefit all players subjected to\nit. When regulators impose appropriate safety standards on both AI creators and\ndomain specialists, the regulation functions as a commitment mechanism, leading\nto safety and performance gains, surpassing what is achieved under no\nregulation or regulating one player only.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CY",
      "econ.TH"
    ],
    "primary_category": "cs.GT",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety",
      "AI safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20662v1",
    "title": "AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language Model for Lung Nodule Malignancy Prediction",
    "authors": [
      "Sadaf Khademi",
      "Mehran Shabanpour",
      "Reza Taleei",
      "Anastasia Oikonomou",
      "Arash Mohammadi"
    ],
    "abstract": "Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\ncases with similar visual characteristics and closely annotation scores. In\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\nfeatures extracted from Computed Tomography (CT) images, while recent research\nhas primarily focused on deep learning solutions. More recently,\nVision-Language Models (VLMs), particularly Contrastive Language-Image\nPre-Training (CLIP)-based models, have gained attention for their ability to\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\nhave shown promising results, we identified the following potential\nlimitations: (a) dependence on radiologists' annotated attributes, which are\ninherently subjective and error-prone, (b) use of textual information only\nduring training, limiting direct applicability at inference, and (c)\nConvolutional-based vision encoder with randomly initialized weights, which\ndisregards prior knowledge. To address these limitations, we introduce\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\nmulti-modal autoregressive objective. Given that lung tumors are typically\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\noffers significant advantages over its CLIP-based counterparts by capturing\npixel-level differences. Additionally, we introduce conditional context\noptimization, which dynamically generates context-specific prompts based on\ninput Radiomics, improving cross-modal alignment.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20660v2",
    "title": "DR-PETS: Learning-Based Control With Planning in Adversarial Environments",
    "authors": [
      "Hozefa Jesawada",
      "Antonio Acernese",
      "Giovanni Russo",
      "Carmen Del Vecchio"
    ],
    "abstract": "Ensuring robustness against epistemic, possibly adversarial, perturbations is\nessential for reliable real-world decision-making. While the Probabilistic\nEnsembles with Trajectory Sampling (PETS) algorithm inherently handles\nuncertainty via ensemble-based probabilistic models, it lacks guarantees\nagainst structured adversarial or worst-case uncertainty distributions. To\naddress this, we propose DR-PETS, a distributionally robust extension of PETS\nthat certifies robustness against adversarial perturbations. We formalize\nuncertainty via a p-Wasserstein ambiguity set, enabling worst-case-aware\nplanning through a min-max optimization framework. While PETS passively\naccounts for stochasticity, DR-PETS actively optimizes robustness via a\ntractable convex approximation integrated into PETS planning loop. Experiments\non pendulum stabilization and cart-pole balancing show that DR-PETS certifies\nrobustness against adversarial parameter perturbations, achieving consistent\nperformance in worst-case scenarios where PETS deteriorates.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20653v1",
    "title": "UWarp: A Whole Slide Image Registration Pipeline to Characterize Scanner-Induced Local Domain Shift",
    "authors": [
      "Antoine Schieb",
      "Bilal Hadjadji",
      "Daniel Tshokola Mweze",
      "Natalia Fernanda Valderrama",
      "Valentin DerangÃ¨re",
      "Laurent Arnould",
      "Sylvain Ladoire",
      "Alain Lalande",
      "Louis-Oscar Morel",
      "Nathan VinÃ§on"
    ],
    "abstract": "Histopathology slide digitization introduces scanner-induced domain shift\nthat can significantly impact computational pathology models based on deep\nlearning methods. In the state-of-the-art, this shift is often characterized at\na broad scale (slide-level or dataset-level) but not patch-level, which limits\nour comprehension of the impact of localized tissue characteristics on the\naccuracy of the deep learning models. To address this challenge, we present a\ndomain shift analysis framework based on UWarp, a novel registration tool\ndesigned to accurately align histological slides scanned under varying\nconditions. UWarp employs a hierarchical registration approach, combining\nglobal affine transformations with fine-grained local corrections to achieve\nrobust tissue patch alignment. We evaluate UWarp using two private datasets,\nCypathLung and BosomShieldBreast, containing whole slide images scanned by\nmultiple devices. Our experiments demonstrate that UWarp outperforms existing\nopen-source registration methods, achieving a median target registration error\n(TRE) of less than 4 pixels (<1 micrometer at 40x magnification) while\nsignificantly reducing computational time. Additionally, we apply UWarp to\ncharacterize scanner-induced local domain shift in the predictions of\nBreast-NEOprAIdict, a deep learning model for breast cancer pathological\nresponse prediction. We find that prediction variability is strongly correlated\nwith tissue density on a given patch. Our findings highlight the importance of\nlocalized domain shift analysis and suggest that UWarp can serve as a valuable\ntool for improving model robustness and domain adaptation strategies in\ncomputational pathology.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20639v2",
    "title": "PVLens: Enhancing Pharmacovigilance Through Automated Label Extraction",
    "authors": [
      "Jeffery L Painter",
      "Gregory E Powell",
      "Andrew Bate"
    ],
    "abstract": "Reliable drug safety reference databases are essential for pharmacovigilance,\nyet existing resources like SIDER are outdated and static. We introduce PVLens,\nan automated system that extracts labeled safety information from FDA\nStructured Product Labels (SPLs) and maps terms to MedDRA. PVLens integrates\nautomation with expert oversight through a web-based review tool. In validation\nagainst 97 drug labels, PVLens achieved an F1 score of 0.882, with high recall\n(0.983) and moderate precision (0.799). By offering a scalable, more accurate\nand continuously updated alternative to SIDER, PVLens enhances real-time\npharamcovigilance with improved accuracy and contemporaneous insights.",
    "categories": [
      "cs.CL",
      "cs.LG",
      "J.3; H.3.1; D.2.12"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20630v1",
    "title": "$Î²$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation",
    "authors": [
      "Haci Ismail Aslan",
      "Philipp Wiesner",
      "Ping Xiong",
      "Odej Kao"
    ],
    "abstract": "Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20844v1",
    "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
    "authors": [
      "Zongyuan Zhang",
      "Tianyang Duan",
      "Zheng Lin",
      "Dong Huang",
      "Zihan Fang",
      "Zekai Sun",
      "Ling Xiong",
      "Hongbin Liang",
      "Heming Cui",
      "Yong Cui",
      "Yue Gao"
    ],
    "abstract": "Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial",
      "adversarial attack"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20613v1",
    "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
    "authors": [
      "Zongyuan Zhang",
      "Tianyang Duan",
      "Zheng Lin",
      "Dong Huang",
      "Zihan Fang",
      "Zekai Sun",
      "Ling Xiong",
      "Hongbin Liang",
      "Heming Cui",
      "Yong Cui"
    ],
    "abstract": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial",
      "adversarial attack"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20583v1",
    "title": "Feature Statistics with Uncertainty Help Adversarial Robustness",
    "authors": [
      "Ran Wang",
      "Xinlei Zhou",
      "Rihao Li",
      "Meng Hu",
      "Wenhui Wu",
      "Yuheng Jia"
    ],
    "abstract": "Despite the remarkable success of deep neural networks (DNNs), the security\nthreat of adversarial attacks poses a significant challenge to the reliability\nof DNNs. By introducing randomness into different parts of DNNs, stochastic\nmethods can enable the model to learn some uncertainty, thereby improving model\nrobustness efficiently. In this paper, we theoretically discover a universal\nphenomenon that adversarial attacks will shift the distributions of feature\nstatistics. Motivated by this theoretical finding, we propose a robustness\nenhancement module called Feature Statistics with Uncertainty (FSU). It\nresamples channel-wise feature means and standard deviations of examples from\nmultivariate Gaussian distributions, which helps to reconstruct the attacked\nexamples and calibrate the shifted distributions. The calibration recovers some\ndomain characteristics of the data for classification, thereby mitigating the\ninfluence of perturbations and weakening the ability of attacks to deceive\nmodels. The proposed FSU module has universal applicability in training,\nattacking, predicting and fine-tuning, demonstrating impressive robustness\nenhancement ability at trivial additional time cost. For example, against\npowerful optimization-based CW attacks, by incorporating FSU into attacking and\npredicting phases, it endows many collapsed state-of-the-art models with\n50%-80% robust accuracy on CIFAR10, CIFAR100 and SVHN.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial",
      "adversarial attack"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20544v1",
    "title": "Safety integrity framework for automated driving",
    "authors": [
      "Moritz Werling",
      "Rainer Faller",
      "Wolfgang Betz",
      "Daniel Straub"
    ],
    "abstract": "This paper describes the comprehensive safety framework that underpinned the\ndevelopment, release process, and regulatory approval of BMW's first SAE Level\n3 Automated Driving System. The framework combines established qualitative and\nquantitative methods from the fields of Systems Engineering, Engineering Risk\nAnalysis, Bayesian Data Analysis, Design of Experiments, and Statistical\nLearning in a novel manner. The approach systematically minimizes the risks\nassociated with hardware and software faults, performance limitations, and\ninsufficient specifications to an acceptable level that achieves a Positive\nRisk Balance. At the core of the framework is the systematic identification and\nquantification of uncertainties associated with hazard scenarios and the\nredundantly designed system based on designed experiments, field data, and\nexpert knowledge. The residual risk of the system is then estimated through\nStochastic Simulation and evaluated by Sensitivity Analysis. By integrating\nthese advanced analytical techniques into the V-Model, the framework fulfills,\nunifies, and complements existing automotive safety standards. It therefore\nprovides a comprehensive, rigorous, and transparent safety assurance process\nfor the development and deployment of Automated Driving Systems.",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20839v1",
    "title": "TAR: Teacher-Aligned Representations via Contrastive Learning for Quadrupedal Locomotion",
    "authors": [
      "Amr Mousa",
      "Neil Karavis",
      "Michele Caprio",
      "Wei Pan",
      "Richard Allmendinger"
    ],
    "abstract": "Quadrupedal locomotion via Reinforcement Learning (RL) is commonly addressed\nusing the teacher-student paradigm, where a privileged teacher guides a\nproprioceptive student policy. However, key challenges such as representation\nmisalignment between the privileged teacher and the proprioceptive-only\nstudent, covariate shift due to behavioral cloning, and lack of deployable\nadaptation lead to poor generalization in real-world scenarios. We propose\nTeacher-Aligned Representations via Contrastive Learning (TAR), a framework\nthat leverages privileged information with self-supervised contrastive learning\nto bridge this gap. By aligning representations to a privileged teacher in\nsimulation via contrastive objectives, our student policy learns structured\nlatent spaces and exhibits robust generalization to Out-of-Distribution (OOD)\nscenarios, surpassing the fully privileged \"Teacher\". Results showed\naccelerated training by 2x compared to state-of-the-art baselines to achieve\npeak performance. OOD scenarios showed better generalization by 40 percent on\naverage compared to existing methods. Additionally, TAR transitions seamlessly\ninto learning during deployment without requiring privileged states, setting a\nnew benchmark in sample-efficient, adaptive locomotion and enabling continual\nfine-tuning in real-world scenarios. Open-source code and videos are available\nat https://ammousa.github.io/TARLoco/.",
    "categories": [
      "cs.RO",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment",
      "misalignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20496v1",
    "title": "Enhancing Depression Detection via Question-wise Modality Fusion",
    "authors": [
      "Aishik Mandal",
      "Dana Atzil-Slonim",
      "Thamar Solorio",
      "Iryna Gurevych"
    ],
    "abstract": "Depression is a highly prevalent and disabling condition that incurs\nsubstantial personal and societal costs. Current depression diagnosis involves\ndetermining the depression severity of a person through self-reported\nquestionnaires or interviews conducted by clinicians. This often leads to\ndelayed treatment and involves substantial human resources. Thus, several works\ntry to automate the process using multimodal data. However, they usually\noverlook the following: i) The variable contribution of each modality for each\nquestion in the questionnaire and ii) Using ordinal classification for the\ntask. This results in sub-optimal fusion and training methods. In this work, we\npropose a novel Question-wise Modality Fusion (QuestMF) framework trained with\na novel Imbalanced Ordinal Log-Loss (ImbOLL) function to tackle these issues.\nThe performance of our framework is comparable to the current state-of-the-art\nmodels on the E-DAIC dataset and enhances interpretability by predicting scores\nfor each question. This will help clinicians identify an individual's symptoms,\nallowing them to customise their interventions accordingly. We also make the\ncode for the QuestMF framework publicly available.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20493v1",
    "title": "Automated and Risk-Aware Engine Control Calibration Using Constrained Bayesian Optimization",
    "authors": [
      "Maarten Vlaswinkel",
      "Duarte Antunes",
      "Frank Willems"
    ],
    "abstract": "Decarbonization of the transport sector sets increasingly strict demands to\nmaximize thermal efficiency and minimize greenhouse gas emissions of Internal\nCombustion Engines. This has led to complex engines with a surge in the number\nof corresponding tunable parameters in actuator set points and control\nsettings. Automated calibration is therefore essential to keep development time\nand costs at acceptable levels. In this work, an innovative self-learning\ncalibration method is presented based on in-cylinder pressure curve shaping.\nThis method combines Principal Component Decomposition with constrained\nBayesian Optimization. To realize maximal thermal engine efficiency, the\noptimization problem aims at minimizing the difference between the actual\nin-cylinder pressure curve and an Idealized Thermodynamic Cycle. By\ncontinuously updating a Gaussian Process Regression model of the pressure's\nPrincipal Components weights using measurements of the actual operating\nconditions, the mean in-cylinder pressure curve as well as its uncertainty\nbounds are learned. This information drives the optimization of calibration\nparameters, which are automatically adapted while dealing with the risks and\nuncertainties associated with operational safety and combustion stability. This\ndata-driven method does not require prior knowledge of the system. The proposed\nmethod is successfully demonstrated in simulation using a Reactivity Controlled\nCompression Ignition engine model. The difference between the Gross Indicated\nEfficiency of the optimal solution found and the true optimum is 0.017%. For\nthis complex engine, the optimal solution was found after 64.4s, which is\nrelatively fast compared to conventional calibration methods.",
    "categories": [
      "eess.SY",
      "cs.SY",
      "stat.ML"
    ],
    "primary_category": "eess.SY",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20491v1",
    "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization",
    "authors": [
      "Jiale Cheng",
      "Ruiliang Lyu",
      "Xiaotao Gu",
      "Xiao Liu",
      "Jiazheng Xu",
      "Yida Lu",
      "Jiayan Teng",
      "Zhuoyi Yang",
      "Yuxiao Dong",
      "Jie Tang",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "abstract": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment",
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20483v1",
    "title": "Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability",
    "authors": [
      "Yingdong Shi",
      "Changming Li",
      "Yifan Wang",
      "Yongxiang Zhao",
      "Anqi Pang",
      "Sibei Yang",
      "Jingyi Yu",
      "Kan Ren"
    ],
    "abstract": "Diffusion models have demonstrated impressive capabilities in synthesizing\ndiverse content. However, despite their high-quality outputs, these models\noften perpetuate social biases, including those related to gender and race.\nThese biases can potentially contribute to harmful real-world consequences,\nreinforcing stereotypes and exacerbating inequalities in various social\ncontexts. While existing research on diffusion bias mitigation has\npredominantly focused on guiding content generation, it often neglects the\nintrinsic mechanisms within diffusion models that causally drive biased\noutputs. In this paper, we investigate the internal processes of diffusion\nmodels, identifying specific decision-making mechanisms, termed bias features,\nembedded within the model architecture. By directly manipulating these\nfeatures, our method precisely isolates and adjusts the elements responsible\nfor bias generation, permitting granular control over the bias levels in the\ngenerated content. Through experiments on both unconditional and conditional\ndiffusion models across various social bias attributes, we demonstrate our\nmethod's efficacy in managing generation distribution while preserving image\nquality. We also dissect the discovered model mechanism, revealing different\nintrinsic features controlling fine-grained aspects of generation, boosting\nfurther research on mechanistic interpretability of diffusion models.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20472v1",
    "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment",
    "authors": [
      "Yucheng Suo",
      "Fan Ma",
      "Linchao Zhu",
      "Tianyi Wang",
      "Fengyun Rao",
      "Yi Yang"
    ],
    "abstract": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20454v1",
    "title": "Lipschitz Constant Meets Condition Number: Learning Robust and Compact Deep Neural Networks",
    "authors": [
      "Yangqi Feng",
      "Shing-Ho J. Lin",
      "Baoyuan Gao",
      "Xian Wei"
    ],
    "abstract": "Recent research has revealed that high compression of Deep Neural Networks\n(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe\ndrop in accuracy and susceptibility to adversarial attacks. Integration of\nnetwork pruning into an adversarial training framework has been proposed to\npromote adversarial robustness. It has been observed that a highly pruned\nweight matrix tends to be ill-conditioned, i.e., increasing the condition\nnumber of the weight matrix. This phenomenon aggravates the vulnerability of a\nDNN to input noise. Although a highly pruned weight matrix is considered to be\nable to lower the upper bound of the local Lipschitz constant to tolerate large\ndistortion, the ill-conditionedness of such a weight matrix results in a\nnon-robust DNN model. To overcome this challenge, this work develops novel\njoint constraints to adjust the weight distribution of networks, namely, the\nTransformed Sparse Constraint joint with Condition Number Constraint (TSCNC),\nwhich copes with smoothing distribution and differentiable constraint functions\nto reduce condition number and thus avoid the ill-conditionedness of weight\nmatrices. Furthermore, our theoretical analyses unveil the relevance between\nthe condition number and the local Lipschitz constant of the weight matrix,\nnamely, the sharply increasing condition number becomes the dominant factor\nthat restricts the robustness of over-sparsified models. Extensive experiments\nare conducted on several public datasets, and the results show that the\nproposed constraints significantly improve the robustness of a DNN with high\npruning rates.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial",
      "adversarial attack"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20429v1",
    "title": "Latent Beam Diffusion Models for Decoding Image Sequences",
    "authors": [
      "Guilherme Fernandes",
      "Vasco Ramos",
      "Regev Cohen",
      "Idan Szpektor",
      "JoÃ£o MagalhÃ£es"
    ],
    "abstract": "While diffusion models excel at generating high-quality images from text\nprompts, they struggle with visual consistency in image sequences. Existing\nmethods generate each image independently, leading to disjointed narratives - a\nchallenge further exacerbated in non-linear storytelling, where scenes must\nconnect beyond adjacent frames. We introduce a novel beam search strategy for\nlatent space exploration, enabling conditional generation of full image\nsequences with beam search decoding. Unlike prior approaches that use fixed\nlatent priors, our method dynamically searches for an optimal sequence of\nlatent representations, ensuring coherent visual transitions. To address beam\nsearch's quadratic complexity, we integrate a cross-attention mechanism that\nefficiently scores search paths and enables pruning, prioritizing alignment\nwith both textual prompts and visual context. Human evaluations confirm that\nour approach outperforms baseline methods, producing full sequences with\nsuperior coherence, visual continuity, and textual alignment. By bridging\nadvances in search optimization and latent space refinement, this work sets a\nnew standard for structured image sequence generation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20410v1",
    "title": "Learning Data-Driven Uncertainty Set Partitions for Robust and Adaptive Energy Forecasting with Missing Data",
    "authors": [
      "Akylas Stratigakos",
      "Panagiotis Andrianesis"
    ],
    "abstract": "Short-term forecasting models typically assume the availability of input data\n(features) when they are deployed and in use. However, equipment failures,\ndisruptions, cyberattacks, may lead to missing features when such models are\nused operationally, which could negatively affect forecast accuracy, and result\nin suboptimal operational decisions. In this paper, we use adaptive robust\noptimization and adversarial machine learning to develop forecasting models\nthat seamlessly handle missing data operationally. We propose linear- and\nneural network-based forecasting models with parameters that adapt to available\nfeatures, combining linear adaptation with a novel algorithm for learning\ndata-driven uncertainty set partitions. The proposed adaptive models do not\nrely on identifying historical missing data patterns and are suitable for\nreal-time operations under stringent time constraints. Extensive numerical\nexperiments on short-term wind power forecasting considering horizons from 15\nminutes to 4 hours ahead illustrate that our proposed adaptive models are on\npar with imputation when data are missing for very short periods (e.g., when\nonly the latest measurement is missing) whereas they significantly outperform\nimputation when data are missing for longer periods. We further provide\ninsights by showcasing how linear adaptation and data-driven partitions (even\nwith a few subsets) approach the performance of the optimal, yet impractical,\nmethod of retraining for every possible realization of missing data.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20355v1",
    "title": "CNN+Transformer Based Anomaly Traffic Detection in UAV Networks for Emergency Rescue",
    "authors": [
      "Yulu Han",
      "Ziye Jia",
      "Sijie He",
      "Yu Zhang",
      "Qihui Wu"
    ],
    "abstract": "The unmanned aerial vehicle (UAV) network has gained significant attentions\nin recent years due to its various applications. However, the traffic security\nbecomes the key threatening public safety issue in an emergency rescue system\ndue to the increasing vulnerability of UAVs to cyber attacks in environments\nwith high heterogeneities. Hence, in this paper, we propose a novel anomaly\ntraffic detection architecture for UAV networks based on the software-defined\nnetworking (SDN) framework and blockchain technology. Specifically, SDN\nseparates the control and data plane to enhance the network manageability and\nsecurity. Meanwhile, the blockchain provides decentralized identity\nauthentication and data security records. Beisdes, a complete security\narchitecture requires an effective mechanism to detect the time-series based\nabnormal traffic. Thus, an integrated algorithm combining convolutional neural\nnetworks (CNNs) and Transformer (CNN+Transformer) for anomaly traffic detection\nis developed, which is called CTranATD. Finally, the simulation results show\nthat the proposed CTranATD algorithm is effective and outperforms the\nindividual CNN, Transformer, and LSTM algorithms for detecting anomaly traffic.",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20310v1",
    "title": "Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks",
    "authors": [
      "Tao Wu",
      "Tie Luo"
    ],
    "abstract": "Adversarial attacks in black-box settings are highly practical, with\ntransfer-based attacks being the most effective at generating adversarial\nexamples (AEs) that transfer from surrogate models to unseen target models.\nHowever, their performance significantly degrades when transferring across\nheterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers\n(ViTs) -- due to fundamental architectural differences. To address this, we\npropose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method\nthat enhances adversarial transferability across diverse architectures. FPA\nintroduces a novel feature permutation (FP) operation, which rearranges pixel\nvalues in selected feature maps to simulate long-range dependencies,\neffectively making CNNs behave more like ViTs and MLPs. This enhances feature\ndiversity and improves transferability both across heterogeneous architectures\nand within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art\narchitectures show that FPA achieves maximum absolute gains in attack success\nrates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming\nexisting black-box attacks. Additionally, FPA is highly generalizable and can\nseamlessly integrate with other transfer-based attacks to further boost their\nperformance. Our findings establish FPA as a robust, efficient, and\ncomputationally lightweight strategy for enhancing adversarial transferability\nacross heterogeneous architectures.",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial",
      "adversarial attack"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20309v1",
    "title": "Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs",
    "authors": [
      "Zitian Wang",
      "Yue Liao",
      "Kang Rong",
      "Fengyun Rao",
      "Yibo Yang",
      "Si Liu"
    ],
    "abstract": "Preference alignment has emerged as an effective strategy to enhance the\nperformance of Multimodal Large Language Models (MLLMs) following supervised\nfine-tuning. While existing preference alignment methods predominantly target\nhallucination factors, they overlook the factors essential for multi-modal\ncomprehension capabilities, often narrowing their improvements on hallucination\nmitigation. To bridge this gap, we propose Instruction-oriented Preference\nAlignment (IPA), a scalable framework designed to automatically construct\nalignment preferences grounded in instruction fulfillment efficacy. Our method\ninvolves an automated preference construction coupled with a dedicated\nverification process that identifies instruction-oriented factors, avoiding\nsignificant variability in response representations. Additionally, IPA\nincorporates a progressive preference collection pipeline, further recalling\nchallenging samples through model self-evolution and reference-guided\nrefinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness\nacross multiple benchmarks, including hallucination evaluation, visual question\nanswering, and text understanding tasks, highlighting its capability to enhance\ngeneral comprehension.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20308v2",
    "title": "Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics",
    "authors": [
      "Lee Chae-Yeon",
      "Oh Hyun-Bin",
      "Han EunGi",
      "Kim Sung-Bin",
      "Suekyeong Nam",
      "Tae-Hyun Oh"
    ],
    "abstract": "Recent advancements in speech-driven 3D talking head generation have made\nsignificant progress in lip synchronization. However, existing models still\nstruggle to capture the perceptual alignment between varying speech\ncharacteristics and corresponding lip movements. In this work, we claim that\nthree criteria -- Temporal Synchronization, Lip Readability, and Expressiveness\n-- are crucial for achieving perceptually accurate lip movements. Motivated by\nour hypothesis that a desirable representation space exists to meet these three\ncriteria, we introduce a speech-mesh synchronized representation that captures\nintricate correspondences between speech signals and 3D face meshes. We found\nthat our learned representation exhibits desirable characteristics, and we plug\nit into existing models as a perceptual loss to better align lip movements to\nthe given speech. In addition, we utilize this representation as a perceptual\nmetric and introduce two other physically grounded lip synchronization metrics\nto assess how well the generated 3D talking heads align with these three\ncriteria. Experiments show that training 3D talking head generation models with\nour perceptual loss significantly improve all three aspects of perceptually\naccurate lip synchronization. Codes and datasets are available at\nhttps://perceptual-3d-talking-head.github.io/.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20302v1",
    "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications",
    "authors": [
      "Sunayana Sitaram",
      "Adrian de Wynter",
      "Isobel McCrum",
      "Qilong Gu",
      "Si-Qing Chen"
    ],
    "abstract": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard large language\nmodel-based application (meeting transcript summarization), where both the data\ngeneration and the annotation steps followed a human-in-the-loop approach. We\nfind that the proposed guardrails are very effective in reducing misgendering\nrates across all languages in the summaries generated, and without incurring\nloss of quality. Our human-in-the-loop approach demonstrates a method to\nfeasibly scale inclusive and responsible AI-based solutions across multiple\nlanguages and cultures.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "responsible AI"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20301v1",
    "title": "Attribute-formed Class-specific Concept Space: Endowing Language Bottleneck Model with Better Interpretability and Scalability",
    "authors": [
      "Jianyang Zhang",
      "Qianli Luo",
      "Guowu Yang",
      "Wenjing Yang",
      "Weide Liu",
      "Guosheng Lin",
      "Fengmao Lv"
    ],
    "abstract": "Language Bottleneck Models (LBMs) are proposed to achieve interpretable image\nrecognition by classifying images based on textual concept bottlenecks.\nHowever, current LBMs simply list all concepts together as the bottleneck\nlayer, leading to the spurious cue inference problem and cannot generalized to\nunseen classes. To address these limitations, we propose the Attribute-formed\nLanguage Bottleneck Model (ALBM). ALBM organizes concepts in the\nattribute-formed class-specific space, where concepts are descriptions of\nspecific attributes for specific classes. In this way, ALBM can avoid the\nspurious cue inference problem by classifying solely based on the essential\nconcepts of each class. In addition, the cross-class unified attribute set also\nensures that the concept spaces of different classes have strong correlations,\nas a result, the learned concept classifier can be easily generalized to unseen\nclasses. Moreover, to further improve interpretability, we propose Visual\nAttribute Prompt Learning (VAPL) to extract visual features on fine-grained\nattributes. Furthermore, to avoid labor-intensive concept annotation, we\npropose the Description, Summary, and Supplement (DSS) strategy to\nautomatically generate high-quality concept sets with a complete and precise\nattribute. Extensive experiments on 9 widely used few-shot benchmarks\ndemonstrate the interpretability, transferability, and performance of our\napproach. The code and collected concept sets are available at\nhttps://github.com/tiggers23/ALBM.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20285v1",
    "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
    "authors": [
      "Hongye Cao",
      "Fan Feng",
      "Jing Huo",
      "Shangdong Yang",
      "Meng Fang",
      "Tianpei Yang",
      "Yang Gao"
    ],
    "abstract": "Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20281v1",
    "title": "Are We There Yet? Unraveling the State-of-the-Art Graph Network Intrusion Detection Systems",
    "authors": [
      "Chenglong Wang",
      "Pujia Zheng",
      "Jiaping Gui",
      "Cunqing Hua",
      "Wajih Ul Hassan"
    ],
    "abstract": "Network Intrusion Detection Systems (NIDS) are vital for ensuring enterprise\nsecurity. Recently, Graph-based NIDS (GIDS) have attracted considerable\nattention because of their capability to effectively capture the complex\nrelationships within the graph structures of data communications. Despite their\npromise, the reproducibility and replicability of these GIDS remain largely\nunexplored, posing challenges for developing reliable and robust detection\nsystems. This study bridges this gap by designing a systematic approach to\nevaluate state-of-the-art GIDS, which includes critically assessing, extending,\nand clarifying the findings of these systems. We further assess the robustness\nof GIDS under adversarial attacks. Evaluations were conducted on three public\ndatasets as well as a newly collected large-scale enterprise dataset. Our\nfindings reveal significant performance discrepancies, highlighting challenges\nrelated to dataset scale, model inputs, and implementation settings. We\ndemonstrate difficulties in reproducing and replicating results, particularly\nconcerning false positive rates and robustness against adversarial attacks.\nThis work provides valuable insights and recommendations for future research,\nemphasizing the importance of rigorous reproduction and replication studies in\ndeveloping robust and generalizable GIDS solutions.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial",
      "adversarial attack"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20278v1",
    "title": "The cell as a token: high-dimensional geometry in language models and cell embeddings",
    "authors": [
      "William Gilpin"
    ],
    "abstract": "Single-cell sequencing technology maps cells to a high-dimensional space\nencoding their internal activity. This process mirrors parallel developments in\nmachine learning, where large language models ingest unstructured text by\nconverting words into discrete tokens embedded within a high-dimensional vector\nspace. This perspective explores how advances in understanding the structure of\nlanguage embeddings can inform ongoing efforts to analyze and visualize single\ncell datasets. We discuss how the context of tokens influences the geometry of\nembedding space, and the role of low-dimensional manifolds in shaping this\nspace's robustness and interpretability. We highlight new developments in\nlanguage modeling, such as interpretability probes and in-context reasoning,\nthat can inform future efforts to construct and consolidate cell atlases.",
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20252v1",
    "title": "LogicQA: Logical Anomaly Detection with Vision Language Model Generated Questions",
    "authors": [
      "Yejin Kwon",
      "Daeun Moon",
      "Youngje Oh",
      "Hyunsoo Yoon"
    ],
    "abstract": "Anomaly Detection (AD) focuses on detecting samples that differ from the\nstandard pattern, making it a vital tool in process control. Logical anomalies\nmay appear visually normal yet violate predefined constraints on object\npresence, arrangement, or quantity, depending on reasoning and explainability.\nWe introduce LogicQA, a framework that enhances AD by providing industrial\noperators with explanations for logical anomalies. LogicQA compiles\nautomatically generated questions into a checklist and collects responses to\nidentify violations of logical constraints. LogicQA is training-free,\nannotation-free, and operates in a few-shot setting. We achieve\nstate-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO\nAD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the\nexplanations of anomalies. Also, our approach has shown outstanding performance\non semiconductor SEM corporate data, further validating its effectiveness in\nindustrial applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20237v1",
    "title": "A Virtual Fencing Framework for Safe and Efficient Collaborative Robotics",
    "authors": [
      "Vineela Reddy Pippera Badguna",
      "Aliasghar Arab",
      "Durga Avinash Kodavalla"
    ],
    "abstract": "Collaborative robots (cobots) increasingly operate alongside humans,\ndemanding robust real-time safeguarding. Current safety standards (e.g., ISO\n10218, ANSI/RIA 15.06, ISO/TS 15066) require risk assessments but offer limited\nguidance for real-time responses. We propose a virtual fencing approach that\ndetects and predicts human motion, ensuring safe cobot operation. Safety and\nperformance tradeoffs are modeled as an optimization problem and solved via\nsequential quadratic programming. Experimental validation shows that our method\nminimizes operational pauses while maintaining safety, providing a modular\nsolution for human-robot collaboration.",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20230v1",
    "title": "TraNCE: Transformative Non-linear Concept Explainer for CNNs",
    "authors": [
      "Ugochukwu Ejike Akpudo",
      "Yongsheng Gao",
      "Jun Zhou",
      "Andrew Lewis"
    ],
    "abstract": "Convolutional neural networks (CNNs) have succeeded remarkably in various\ncomputer vision tasks. However, they are not intrinsically explainable. While\nthe feature-level understanding of CNNs reveals where the models looked,\nconcept-based explainability methods provide insights into what the models saw.\nHowever, their assumption of linear reconstructability of image activations\nfails to capture the intricate relationships within these activations. Their\nFidelity-only approach to evaluating global explanations also presents a new\nconcern. For the first time, we address these limitations with the novel\nTransformative Nonlinear Concept Explainer (TraNCE) for CNNs. Unlike linear\nreconstruction assumptions made by existing methods, TraNCE captures the\nintricate relationships within the activations. This study presents three\noriginal contributions to the CNN explainability literature: (i) An automatic\nconcept discovery mechanism based on variational autoencoders (VAEs). This\ntransformative concept discovery process enhances the identification of\nmeaningful concepts from image activations. (ii) A visualization module that\nleverages the Bessel function to create a smooth transition between\nprototypical image pixels, revealing not only what the CNN saw but also what\nthe CNN avoided, thereby mitigating the challenges of concept duplication as\ndocumented in previous works. (iii) A new metric, the Faith score, integrates\nboth Coherence and Fidelity for a comprehensive evaluation of explainer\nfaithfulness and consistency.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20228v1",
    "title": "TeleLoRA: Teleporting Model-Specific Alignment Across LLMs",
    "authors": [
      "Xiao Lin",
      "Manoj Acharya",
      "Anirban Roy",
      "Susmit Jha"
    ],
    "abstract": "Mitigating Trojans in Large Language Models (LLMs) is one of many tasks where\nalignment data is LLM specific, as different LLMs have different Trojan\ntriggers and trigger behaviors to be removed. In this paper, we introduce\nTeleLoRA (Teleporting Low-Rank Adaptation), a novel framework that synergizes\nmodel-specific alignment data across multiple LLMs to enable zero-shot Trojan\nmitigation on unseen LLMs without alignment data. TeleLoRA learns a unified\ngenerator of LoRA adapter weights by leveraging local activation information\nacross multiple LLMs. This generator is designed to be permutation symmetric to\ngeneralize across models with different architectures and sizes. We optimize\nthe model design for memory efficiency, making it feasible to learn with\nlarge-scale LLMs with minimal computational resources. Experiments on LLM\nTrojan mitigation benchmarks demonstrate that TeleLoRA effectively reduces\nattack success rates while preserving the benign performance of the models.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20198v1",
    "title": "Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models",
    "authors": [
      "Alex Jinpeng Wang",
      "Linjie Li",
      "Zhengyuan Yang",
      "Lijuan Wang",
      "Min Li"
    ],
    "abstract": "Recent advancements in autoregressive and diffusion models have led to strong\nperformance in image generation with short scene text words. However,\ngenerating coherent, long-form text in images, such as paragraphs in slides or\ndocuments, remains a major challenge for current generative models. We present\nthe first work specifically focused on long text image generation, addressing a\ncritical gap in existing text-to-image systems that typically handle only brief\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\nautoregressive generation models, we identify the image tokenizer as a critical\nbottleneck in text generating quality. To address this, we introduce a novel\ntext-focused, binary tokenizer optimized for capturing detailed scene text\nfeatures. Leveraging our tokenizer, we develop \\ModelName, a multimodal\nautoregressive model that excels in generating high-quality long-text images\nwith unprecedented fidelity. Our model offers robust controllability, enabling\ncustomization of text properties such as font style, size, color, and\nalignment. Extensive experiments demonstrate that \\ModelName~significantly\noutperforms SD3.5 Large~\\cite{sd3} and GPT4o~\\cite{gpt4o} with DALL-E\n3~\\cite{dalle3} in generating long text accurately, consistently, and flexibly.\nBeyond its technical achievements, \\ModelName~opens up exciting opportunities\nfor innovative applications like interleaved document and PowerPoint\ngeneration, establishing a new frontier in long-text image generating.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20194v1",
    "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization",
    "authors": [
      "Zhouhong Gu",
      "Xingzhou Chen",
      "Xiaoran Shi",
      "Tao Wang",
      "Suhang Zheng",
      "Tianyu Li",
      "Hongwei Feng",
      "Yanghua Xiao"
    ],
    "abstract": "Recent advances in large language models have highlighted the critical need\nfor precise control over model outputs through predefined constraints. While\nexisting methods attempt to achieve this through either direct\ninstruction-response synthesis or preferential response optimization, they\noften struggle with constraint understanding and adaptation. This limitation\nbecomes particularly evident when handling fine-grained constraints, leading to\neither hallucination or brittle performance. We introduce Generative\nAdversarial Policy Optimization (GAPO), a novel framework that combines\nGAN-based training dynamics with an encoder-only reward model to progressively\nlearn and adapt to increasingly complex constraints. GAPO leverages adversarial\ntraining to automatically generate training samples of varying difficulty while\nutilizing the encoder-only architecture to better capture prompt-response\nrelationships. Extensive experiments demonstrate GAPO's superior performance\nacross multiple benchmarks, particularly in scenarios requiring fine-grained\nconstraint handling, where it significantly outperforms existing methods like\nPPO, DPO, and KTO. Our results suggest that GAPO's unique approach to\npreferential prompt learning offers a more robust and effective solution for\ncontrolling LLM outputs. Code is avaliable in\nhttps://github.com/MikeGu721/GAPO.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20188v1",
    "title": "Rethinking Vision-Language Model in Face Forensics: Multi-Modal Interpretable Forged Face Detector",
    "authors": [
      "Xiao Guo",
      "Xiufeng Song",
      "Yue Zhang",
      "Xiaohong Liu",
      "Xiaoming Liu"
    ],
    "abstract": "Deepfake detection is a long-established research topic vital for mitigating\nthe spread of malicious misinformation. Unlike prior methods that provide\neither binary classification results or textual explanations separately, we\nintroduce a novel method capable of generating both simultaneously. Our method\nharnesses the multi-modal learning capability of the pre-trained CLIP and the\nunprecedented interpretability of large language models (LLMs) to enhance both\nthe generalization and explainability of deepfake detection. Specifically, we\nintroduce a multi-modal face forgery detector (M2F2-Det) that employs tailored\nface forgery prompt learning, incorporating the pre-trained CLIP to improve\ngeneralization to unseen forgeries. Also, M2F2-Det incorporates an LLM to\nprovide detailed textual explanations of its detection decisions, enhancing\ninterpretability by bridging the gap between natural language and subtle cues\nof facial forgeries. Empirically, we evaluate M2F2-Det on both detection and\nexplanation generation tasks, where it achieves state-of-the-art performance,\ndemonstrating its effectiveness in identifying and explaining diverse\nforgeries.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "interpretability",
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20187v1",
    "title": "Network Inversion for Generating Confidently Classified Counterfeits",
    "authors": [
      "Pirzada Suhail",
      "Amit Sethi"
    ],
    "abstract": "In machine learning, especially with vision classifiers, generating inputs\nthat are confidently classified by the model is essential for understanding its\ndecision boundaries and behavior. However, creating such samples that are\nconfidently classified yet distinct from the training data distribution is a\nchallenge. Traditional methods often modify existing inputs, but they don't\nalways ensure confident classification. In this work, we extend network\ninversion techniques to generate Confidently Classified Counterfeits-synthetic\nsamples that are confidently classified by the model despite being\nsignificantly different from the training data. We achieve this by modifying\nthe generator's conditioning mechanism from soft vector conditioning to one-hot\nvector conditioning and applying Kullback-Leibler divergence (KLD) between the\none-hot vectors and the classifier's output distribution. This encourages the\ngenerator to produce samples that are both plausible and confidently\nclassified. Generating Confidently Classified Counterfeits is crucial for\nensuring the safety and reliability of machine learning systems, particularly\nin safety-critical applications where models must exhibit confidence only on\ndata within the training distribution. By generating such counterfeits, we\nchallenge the assumption that high-confidence predictions are always indicative\nof in-distribution data, providing deeper insights into the model's limitations\nand decision-making process.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20176v1",
    "title": "Offline Reinforcement Learning with Discrete Diffusion Skills",
    "authors": [
      "RuiXi Qiao",
      "Jie Cheng",
      "Xingyuan Dai",
      "Yonglin Tian",
      "Yisheng Lv"
    ],
    "abstract": "Skills have been introduced to offline reinforcement learning (RL) as\ntemporal abstractions to tackle complex, long-horizon tasks, promoting\nconsistent behavior and enabling meaningful exploration. While skills in\noffline RL are predominantly modeled within a continuous latent space, the\npotential of discrete skill spaces remains largely underexplored. In this\npaper, we propose a compact discrete skill space for offline RL tasks supported\nby state-of-the-art transformer-based encoder and diffusion-based decoder.\nCoupled with a high-level policy trained via offline RL techniques, our method\nestablishes a hierarchical RL framework where the trained diffusion decoder\nplays a pivotal role. Empirical evaluations show that the proposed algorithm,\nDiscrete Diffusion Skill (DDS), is a powerful offline RL method. DDS performs\ncompetitively on Locomotion and Kitchen tasks and excels on long-horizon tasks,\nachieving at least a 12 percent improvement on AntMaze-v2 benchmarks compared\nto existing offline RL approaches. Furthermore, DDS offers improved\ninterpretability, training stability, and online exploration compared to\nprevious skill-based methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20134v1",
    "title": "DRPA-MPPI: Dynamic Repulsive Potential Augmented MPPI for Reactive Navigation in Unstructured Environments",
    "authors": [
      "Takahiro Fuke",
      "Masafumi Endo",
      "Kohei Honda",
      "Genya Ishigami"
    ],
    "abstract": "Reactive mobile robot navigation in unstructured environments is challenging\nwhen robots encounter unexpected obstacles that invalidate previously planned\ntrajectories. Model predictive path integral control (MPPI) enables reactive\nplanning, but still suffers from limited prediction horizons that lead to local\nminima traps near obstacles. Current solutions rely on heuristic cost design or\nscenario-specific pre-training, which often limits their adaptability to new\nenvironments. We introduce dynamic repulsive potential augmented MPPI\n(DRPA-MPPI), which dynamically detects potential entrapments on the predicted\ntrajectories. Upon detecting local minima, DRPA-MPPI automatically switches\nbetween standard goal-oriented optimization and a modified cost function that\ngenerates repulsive forces away from local minima. Comprehensive testing in\nsimulated obstacle-rich environments confirms DRPA-MPPI's superior navigation\nperformance and safety compared to conventional methods with less computational\nburden.",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-26",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20113v1",
    "title": "Domain Adaptation Framework for Turning Movement Count Estimation with Limited Data",
    "authors": [
      "Xiaobo Ma",
      "Hyunsoo Noh",
      "Ryan Hatch",
      "James Tokishi",
      "Zepu Wang"
    ],
    "abstract": "Urban transportation networks are vital for the efficient movement of people\nand goods, necessitating effective traffic management and planning. An integral\npart of traffic management is understanding the turning movement counts (TMCs)\nat intersections, Accurate TMCs at intersections are crucial for traffic signal\ncontrol, congestion mitigation, and road safety. In general, TMCs are obtained\nusing physical sensors installed at intersections, but this approach can be\ncost-prohibitive and technically challenging, especially for cities with\nextensive road networks. Recent advancements in machine learning and\ndata-driven approaches have offered promising alternatives for estimating TMCs.\nTraffic patterns can vary significantly across different intersections due to\nfactors such as road geometry, traffic signal settings, and local driver\nbehaviors. This domain discrepancy limits the generalizability and accuracy of\nmachine learning models when applied to new or unseen intersections. In\nresponse to these limitations, this research proposes a novel framework\nleveraging domain adaptation (DA) to estimate TMCs at intersections by using\ntraffic controller event-based data, road infrastructure data, and\npoint-of-interest (POI) data. Evaluated on 30 intersections in Tucson, Arizona,\nthe performance of the proposed DA framework was compared with state-of-the-art\nmodels and achieved the lowest values in terms of Mean Absolute Error and Root\nMean Square Error.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20110v1",
    "title": "Efficient Model Development through Fine-tuning Transfer",
    "authors": [
      "Pin-Jie Lin",
      "Rishab Balasubramanian",
      "Fengyuan Liu",
      "Nikhil Kandpal",
      "Tu Vu"
    ],
    "abstract": "Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20105v1",
    "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations",
    "authors": [
      "Ran Tian",
      "Kratarth Goel"
    ],
    "abstract": "Recent advancements in LLMs have revolutionized motion generation models in\nembodied applications. While LLM-type auto-regressive motion generation models\nbenefit from training scalability, there remains a discrepancy between their\ntoken prediction objectives and human preferences. As a result, models\npre-trained solely with token-prediction objectives often generate behaviors\nthat deviate from what humans would prefer, making post-training preference\nalignment crucial for producing human-preferred motions. Unfortunately,\npost-training alignment requires extensive preference rankings of motions\ngenerated by the pre-trained model, which are costly to annotate, especially in\nmulti-agent settings. Recently, there has been growing interest in leveraging\npre-training demonstrations to scalably generate preference data for\npost-training alignment. However, these methods often adopt an adversarial\nassumption, treating all pre-trained model-generated samples as unpreferred\nexamples. This adversarial approach overlooks the valuable signal provided by\npreference rankings among the model's own generations, ultimately reducing\nalignment effectiveness and potentially leading to misaligned behaviors. In\nthis work, instead of treating all generated samples as equally bad, we\nleverage implicit preferences encoded in pre-training demonstrations to\nconstruct preference rankings among the pre-trained model's generations,\noffering more nuanced preference alignment guidance with zero human cost. We\napply our approach to large-scale traffic simulation and demonstrate its\neffectiveness in improving the realism of pre-trained model's generated\nbehaviors, making a lightweight 1M motion generation model comparable to SOTA\nlarge imitation-based models by relying solely on implicit feedback from\npre-training demonstrations, without additional post-training human preference\nannotations or high computational costs.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment",
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20103v1",
    "title": "Bigger But Not Better: Small Neural Language Models Outperform Large Language Models in Detection of Thought Disorder",
    "authors": [
      "Changye Li",
      "Weizhe Xu",
      "Serguei Pakhomov",
      "Ellen Bradley",
      "Dror Ben-Zeev",
      "Trevor Cohen"
    ],
    "abstract": "Disorganized thinking is a key diagnostic indicator of schizophrenia-spectrum\ndisorders. Recently, clinical estimates of the severity of disorganized\nthinking have been shown to correlate with measures of how difficult speech\ntranscripts would be for large language models (LLMs) to predict. However,\nLLMs' deployment challenges -- including privacy concerns, computational and\nfinancial costs, and lack of transparency of training data -- limit their\nclinical utility. We investigate whether smaller neural language models can\nserve as effective alternatives for detecting positive formal thought disorder,\nusing the same sliding window based perplexity measurements that proved\neffective with larger models. Surprisingly, our results show that smaller\nmodels are more sensitive to linguistic differences associated with formal\nthought disorder than their larger counterparts. Detection capability declines\nbeyond a certain model size and context length, challenging the common\nassumption of ``bigger is better'' for LLM-based applications. Our findings\ngeneralize across audio diaries and clinical interview speech samples from\nindividuals with psychotic symptoms, suggesting a promising direction for\ndeveloping efficient, cost-effective, and privacy-preserving screening tools\nthat can be deployed in both clinical and naturalistic settings.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "transparency"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20099v1",
    "title": "AI Identity, Empowerment, and Mindfulness in Mitigating Unethical AI Use",
    "authors": [
      "Mayssam Tarighi Shaayesteh",
      "Sara Memarian Esfahani",
      "Hossein Mohit"
    ],
    "abstract": "This study examines how AI identity influences psychological empowerment and\nunethical AI behavior among college students, while also exploring the\nmoderating role of IT mindfulness. Findings show that a strong AI identity\nenhances psychological empowerment and academic engagement but can also lead to\nincreased unethical AI practices. Crucially, IT mindfulness acts as an ethical\nsafeguard, promoting sensitivity to ethical concerns and reducing misuse of AI.\nThese insights have implications for educators, policymakers, and AI\ndevelopers, emphasizing For Peer Review the need for a balanced approach that\nencourages digital engagement without compromising student responsibility. The\nstudy also contributes to philosophical discussions of psychological agency,\nsuggesting that empowerment through AI can yield both positive and negative\noutcomes. Mindfulness emerges as essential in guiding ethical AI interactions.\nOverall, the research informs ongoing debates on ethics in education and AI,\noffering strategies to align technological advancement with ethical\naccountability and responsible use.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "ethics",
      "ethical AI"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20084v1",
    "title": "Can Multi-modal (reasoning) LLMs work as deepfake detectors?",
    "authors": [
      "Simiao Ren",
      "Yao Yao",
      "Kidus Zewde",
      "Zisheng Liang",
      "Tsang",
      "Ng",
      "Ning-Yau Cheng",
      "Xiaoou Zhan",
      "Qinzhe Liu",
      "Yifei Chen",
      "Hengwei Xu"
    ],
    "abstract": "Deepfake detection remains a critical challenge in the era of advanced\ngenerative models, particularly as synthetic media becomes more sophisticated.\nIn this study, we explore the potential of state of the art multi-modal\n(reasoning) large language models (LLMs) for deepfake image detection such as\n(OpenAI O1/4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen\n2/2.5 VL, Mistral Pixtral, Claude 3.5/3.7 sonnet) . We benchmark 12 latest\nmulti-modal LLMs against traditional deepfake detection methods across multiple\ndatasets, including recently published real-world deepfake imagery. To enhance\nperformance, we employ prompt tuning and conduct an in-depth analysis of the\nmodels' reasoning pathways to identify key contributing factors in their\ndecision-making process. Our findings indicate that best multi-modal LLMs\nachieve competitive performance with promising generalization ability with zero\nshot, even surpass traditional deepfake detection pipelines in\nout-of-distribution datasets while the rest of the LLM families performs\nextremely disappointing with some worse than random guess. Furthermore, we\nfound newer model version and reasoning capabilities does not contribute to\nperformance in such niche tasks of deepfake detection while model size do help\nin some cases. This study highlights the potential of integrating multi-modal\nreasoning in future deepfake detection frameworks and provides insights into\nmodel interpretability for robustness in real-world scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20062v1",
    "title": "Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays",
    "authors": [
      "Jinsook Lee",
      "AJ Alvero",
      "Thorsten Joachims",
      "RenÃ© Kizilcec"
    ],
    "abstract": "People are increasingly using technologies equipped with large language\nmodels (LLM) to write texts for formal communication, which raises two\nimportant questions at the intersection of technology and society: Who do LLMs\nwrite like (model alignment); and can LLMs be prompted to change who they write\nlike (model steerability). We investigate these questions in the high-stakes\ncontext of undergraduate admissions at a selective university by comparing\nlexical and sentence variation between essays written by 30,000 applicants to\ntwo types of LLM-generated essays: one prompted with only the essay question\nused by the human applicants; and another with additional demographic\ninformation about each applicant. We consistently find that both types of\nLLM-generated essays are linguistically distinct from human-authored essays,\nregardless of the specific model and analytical approach. Further, prompting a\nspecific sociodemographic identity is remarkably ineffective in aligning the\nmodel with the linguistic patterns observed in human writing from this identity\ngroup. This holds along the key dimensions of sex, race, first-generation\nstatus, and geographic location. The demographically prompted and unprompted\nsynthetic texts were also more similar to each other than to the human text,\nmeaning that prompting did not alleviate homogenization. These issues of model\nalignment and steerability in current LLMs raise concerns about the use of LLMs\nin high-stakes contexts.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20047v1",
    "title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis",
    "authors": [
      "Yu Xin",
      "Gorkem Can Ates",
      "Kuang Gong",
      "Wei Shao"
    ],
    "abstract": "Vision-language models (VLMs) have shown promise in 2D medical image\nanalysis, but extending them to 3D remains challenging due to the high\ncomputational demands of volumetric data and the difficulty of aligning 3D\nspatial features with clinical text. We present Med3DVLM, a 3D VLM designed to\naddress these challenges through three key innovations: (1) DCFormer, an\nefficient encoder that uses decomposed 3D convolutions to capture fine-grained\nspatial features at scale; (2) SigLIP, a contrastive learning strategy with\npairwise sigmoid loss that improves image-text alignment without relying on\nlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuses\nlow- and high-level image features with text embeddings for richer multi-modal\nrepresentations. We evaluate our model on the M3D dataset, which includes\nradiology reports and VQA data for 120,084 3D medical images. Results show that\nMed3DVLM achieves superior performance across multiple benchmarks. For\nimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly\noutperforming the current state-of-the-art M3D model (19.10%). For report\ngeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended\nvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in\nclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results\nhighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,\nenabling scalable, multi-task reasoning across clinical applications. Our code\nis publicly available at https://github.com/mirthAI/Med3DVLM.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20020v1",
    "title": "Gemini Robotics: Bringing AI into the Physical World",
    "authors": [
      "Gemini Robotics Team",
      "Saminda Abeyruwan",
      "Joshua Ainslie",
      "Jean-Baptiste Alayrac",
      "Montserrat Gonzalez Arenas",
      "Travis Armstrong",
      "Ashwin Balakrishna",
      "Robert Baruch",
      "Maria Bauza",
      "Michiel Blokzijl",
      "Steven Bohez",
      "Konstantinos Bousmalis",
      "Anthony Brohan",
      "Thomas Buschmann",
      "Arunkumar Byravan",
      "Serkan Cabi",
      "Ken Caluwaerts",
      "Federico Casarini",
      "Oscar Chang",
      "Jose Enrique Chen",
      "Xi Chen",
      "Hao-Tien Lewis Chiang",
      "Krzysztof Choromanski",
      "David D'Ambrosio",
      "Sudeep Dasari",
      "Todor Davchev",
      "Coline Devin",
      "Norman Di Palo",
      "Tianli Ding",
      "Adil Dostmohamed",
      "Danny Driess",
      "Yilun Du",
      "Debidatta Dwibedi",
      "Michael Elabd",
      "Claudio Fantacci",
      "Cody Fong",
      "Erik Frey",
      "Chuyuan Fu",
      "Marissa Giustina",
      "Keerthana Gopalakrishnan",
      "Laura Graesser",
      "Leonard Hasenclever",
      "Nicolas Heess",
      "Brandon Hernaez",
      "Alexander Herzog",
      "R. Alex Hofer",
      "Jan Humplik",
      "Atil Iscen",
      "Mithun George Jacob",
      "Deepali Jain",
      "Ryan Julian",
      "Dmitry Kalashnikov",
      "M. Emre Karagozler",
      "Stefani Karp",
      "Chase Kew",
      "Jerad Kirkland",
      "Sean Kirmani",
      "Yuheng Kuang",
      "Thomas Lampe",
      "Antoine Laurens",
      "Isabel Leal",
      "Alex X. Lee",
      "Tsang-Wei Edward Lee",
      "Jacky Liang",
      "Yixin Lin",
      "Sharath Maddineni",
      "Anirudha Majumdar",
      "Assaf Hurwitz Michaely",
      "Robert Moreno",
      "Michael Neunert",
      "Francesco Nori",
      "Carolina Parada",
      "Emilio Parisotto",
      "Peter Pastor",
      "Acorn Pooley",
      "Kanishka Rao",
      "Krista Reymann",
      "Dorsa Sadigh",
      "Stefano Saliceti",
      "Pannag Sanketi",
      "Pierre Sermanet",
      "Dhruv Shah",
      "Mohit Sharma",
      "Kathryn Shea",
      "Charles Shu",
      "Vikas Sindhwani",
      "Sumeet Singh",
      "Radu Soricut",
      "Jost Tobias Springenberg",
      "Rachel Sterneck",
      "Razvan Surdulescu",
      "Jie Tan",
      "Jonathan Tompson",
      "Vincent Vanhoucke",
      "Jake Varley",
      "Grace Vesom",
      "Giulia Vezzani",
      "Oriol Vinyals",
      "Ayzaan Wahid",
      "Stefan Welker",
      "Paul Wohlhart",
      "Fei Xia",
      "Ted Xiao",
      "Annie Xie",
      "Jinyu Xie",
      "Peng Xu",
      "Sichun Xu",
      "Ying Xu",
      "Zhuo Xu",
      "Yuxiang Yang",
      "Rui Yao",
      "Sergey Yaroshenko",
      "Wenhao Yu",
      "Wentao Yuan",
      "Jingwei Zhang",
      "Tingnan Zhang",
      "Allan Zhou",
      "Yuxiang Zhou"
    ],
    "abstract": "Recent advancements in large multimodal models have led to the emergence of\nremarkable generalist capabilities in digital domains, yet their translation to\nphysical agents such as robots remains a significant challenge. This report\nintroduces a new family of AI models purposefully designed for robotics and\nbuilt upon the foundation of Gemini 2.0. We present Gemini Robotics, an\nadvanced Vision-Language-Action (VLA) generalist model capable of directly\ncontrolling robots. Gemini Robotics executes smooth and reactive movements to\ntackle a wide range of complex manipulation tasks while also being robust to\nvariations in object types and positions, handling unseen environments as well\nas following diverse, open vocabulary instructions. We show that with\nadditional fine-tuning, Gemini Robotics can be specialized to new capabilities\nincluding solving long-horizon, highly dexterous tasks, learning new\nshort-horizon tasks from as few as 100 demonstrations and adapting to\ncompletely novel robot embodiments. This is made possible because Gemini\nRobotics builds on top of the Gemini Robotics-ER model, the second model we\nintroduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends\nGemini's multimodal reasoning capabilities into the physical world, with\nenhanced spatial and temporal understanding. This enables capabilities relevant\nto robotics including object detection, pointing, trajectory and grasp\nprediction, as well as multi-view correspondence and 3D bounding box\npredictions. We show how this novel combination can support a variety of\nrobotics applications. We also discuss and address important safety\nconsiderations related to this new class of robotics foundation models. The\nGemini Robotics family marks a substantial step towards developing\ngeneral-purpose robots that realizes AI's potential in the physical world.",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19916v1",
    "title": "EventFly: Event Camera Perception from Ground to the Sky",
    "authors": [
      "Lingdong Kong",
      "Dongyue Lu",
      "Xiang Xu",
      "Lai Xing Ng",
      "Wei Tsang Ooi",
      "Benoit R. Cottereau"
    ],
    "abstract": "Cross-platform adaptation in event-based dense perception is crucial for\ndeploying event cameras across diverse settings, such as vehicles, drones, and\nquadrupeds, each with unique motion dynamics, viewpoints, and class\ndistributions. In this work, we introduce EventFly, a framework for robust\ncross-platform adaptation in event camera perception. Our approach comprises\nthree key components: i) Event Activation Prior (EAP), which identifies\nhigh-activation regions in the target domain to minimize prediction entropy,\nfostering confident, domain-adaptive predictions; ii) EventBlend, a data-mixing\nstrategy that integrates source and target event voxel grids based on\nEAP-driven similarity and density maps, enhancing feature alignment; and iii)\nEventMatch, a dual-discriminator technique that aligns features from source,\ntarget, and blended domains for better domain-invariant learning. To\nholistically assess cross-platform adaptation abilities, we introduce EXPo, a\nlarge-scale benchmark with diverse samples across vehicle, drone, and quadruped\nplatforms. Extensive experiments validate our effectiveness, demonstrating\nsubstantial gains over popular adaptation methods. We hope this work can pave\nthe way for more adaptive, high-performing event perception across diverse and\ncomplex environments.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19912v1",
    "title": "SuperFlow++: Enhanced Spatiotemporal Consistency for Cross-Modal Data Pretraining",
    "authors": [
      "Xiang Xu",
      "Lingdong Kong",
      "Hui Shuai",
      "Wenwei Zhang",
      "Liang Pan",
      "Kai Chen",
      "Ziwei Liu",
      "Qingshan Liu"
    ],
    "abstract": "LiDAR representation learning has emerged as a promising approach to reducing\nreliance on costly and labor-intensive human annotations. While existing\nmethods primarily focus on spatial alignment between LiDAR and camera sensors,\nthey often overlook the temporal dynamics critical for capturing motion and\nscene continuity in driving scenarios. To address this limitation, we propose\nSuperFlow++, a novel framework that integrates spatiotemporal cues in both\npretraining and downstream tasks using consecutive LiDAR-camera pairs.\nSuperFlow++ introduces four key components: (1) a view consistency alignment\nmodule to unify semantic information across camera views, (2) a dense-to-sparse\nconsistency regularization mechanism to enhance feature robustness across\nvarying point cloud densities, (3) a flow-based contrastive learning approach\nthat models temporal relationships for improved scene understanding, and (4) a\ntemporal voting strategy that propagates semantic information across LiDAR\nscans to improve prediction consistency. Extensive evaluations on 11\nheterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms\nstate-of-the-art methods across diverse tasks and driving conditions.\nFurthermore, by scaling both 2D and 3D backbones during pretraining, we uncover\nemergent properties that provide deeper insights into developing scalable 3D\nfoundation models. With strong generalizability and computational efficiency,\nSuperFlow++ establishes a new benchmark for data-efficient LiDAR-based\nperception in autonomous driving. The code is publicly available at\nhttps://github.com/Xiangxu-0103/SuperFlow",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19906v2",
    "title": "AvatarArtist: Open-Domain 4D Avatarization",
    "authors": [
      "Hongyu Liu",
      "Xuan Wang",
      "Ziyu Wan",
      "Yue Ma",
      "Jingye Chen",
      "Yanbo Fan",
      "Yujun Shen",
      "Yibing Song",
      "Qifeng Chen"
    ],
    "abstract": "This work focuses on open-domain 4D avatarization, with the purpose of\ncreating a 4D avatar from a portrait image in an arbitrary style. We select\nparametric triplanes as the intermediate 4D representation and propose a\npractical training paradigm that takes advantage of both generative adversarial\nnetworks (GANs) and diffusion models. Our design stems from the observation\nthat 4D GANs excel at bridging images and triplanes without supervision yet\nusually face challenges in handling diverse data distributions. A robust 2D\ndiffusion prior emerges as the solution, assisting the GAN in transferring its\nexpertise across various domains. The synergy between these experts permits the\nconstruction of a multi-domain image-triplane dataset, which drives the\ndevelopment of a general 4D avatar creator. Extensive experiments suggest that\nour model, AvatarArtist, is capable of producing high-quality 4D avatars with\nstrong robustness to various source image domains. The code, the data, and the\nmodels will be made publicly available to facilitate future studies.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19904v1",
    "title": "Tracktention: Leveraging Point Tracking to Attend Videos Faster and Better",
    "authors": [
      "Zihang Lai",
      "Andrea Vedaldi"
    ],
    "abstract": "Temporal consistency is critical in video prediction to ensure that outputs\nare coherent and free of artifacts. Traditional methods, such as temporal\nattention and 3D convolution, may struggle with significant object motion and\nmay not capture long-range temporal dependencies in dynamic scenes. To address\nthis gap, we propose the Tracktention Layer, a novel architectural component\nthat explicitly integrates motion information using point tracks, i.e.,\nsequences of corresponding points across frames. By incorporating these motion\ncues, the Tracktention Layer enhances temporal alignment and effectively\nhandles complex object motions, maintaining consistent feature representations\nover time. Our approach is computationally efficient and can be seamlessly\nintegrated into existing models, such as Vision Transformers, with minimal\nmodification. It can be used to upgrade image-only models to state-of-the-art\nvideo ones, sometimes outperforming models natively designed for video\nprediction. We demonstrate this on video depth prediction and video\ncolorization, where models augmented with the Tracktention Layer exhibit\nsignificantly improved temporal consistency compared to baselines.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19889v1",
    "title": "A Multi-Agent Framework Integrating Large Language Models and Generative AI for Accelerated Metamaterial Design",
    "authors": [
      "Jie Tian",
      "Martin Taylor Sobczak",
      "Dhanush Patil",
      "Jixin Hou",
      "Lin Pang",
      "Arunachalam Ramanathan",
      "Libin Yang",
      "Xianyan Chen",
      "Yuval Golan",
      "Hongyue Sun",
      "Kenan Song",
      "Xianqiao Wang"
    ],
    "abstract": "Metamaterials, renowned for their exceptional mechanical, electromagnetic,\nand thermal properties, hold transformative potential across diverse\napplications, yet their design remains constrained by labor-intensive\ntrial-and-error methods and limited data interoperability. Here, we introduce\nCrossMatAgent -- a novel multi-agent framework that synergistically integrates\nlarge language models with state-of-the-art generative AI to revolutionize\nmetamaterial design. By orchestrating a hierarchical team of agents -- each\nspecializing in tasks such as pattern analysis, architectural synthesis, prompt\nengineering, and supervisory feedback -- our system leverages the multimodal\nreasoning of GPT-4o alongside the generative precision of DALL-E 3 and a\nfine-tuned Stable Diffusion XL model. This integrated approach automates data\naugmentation, enhances design fidelity, and produces simulation- and 3D\nprinting-ready metamaterial patterns. Comprehensive evaluations, including\nCLIP-based alignment, SHAP interpretability analyses, and mechanical\nsimulations under varied load conditions, demonstrate the framework's ability\nto generate diverse, reproducible, and application-ready designs. CrossMatAgent\nthus establishes a scalable, AI-driven paradigm that bridges the gap between\nconceptual innovation and practical realization, paving the way for accelerated\nmetamaterial development.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.RO"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment",
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19887v1",
    "title": "A proposal for an incident regime that tracks and counters threats to national security posed by AI systems",
    "authors": [
      "Alejandro Ortega"
    ],
    "abstract": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19881v1",
    "title": "Mask$^2$DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long Video Generation",
    "authors": [
      "Tianhao Qi",
      "Jianlong Yuan",
      "Wanquan Feng",
      "Shancheng Fang",
      "Jiawei Liu",
      "SiYu Zhou",
      "Qian He",
      "Hongtao Xie",
      "Yongdong Zhang"
    ],
    "abstract": "Sora has unveiled the immense potential of the Diffusion Transformer (DiT)\narchitecture in single-scene video generation. However, the more challenging\ntask of multi-scene video generation, which offers broader applications,\nremains relatively underexplored. To bridge this gap, we propose Mask$^2$DiT, a\nnovel approach that establishes fine-grained, one-to-one alignment between\nvideo segments and their corresponding text annotations. Specifically, we\nintroduce a symmetric binary mask at each attention layer within the DiT\narchitecture, ensuring that each text annotation applies exclusively to its\nrespective video segment while preserving temporal coherence across visual\ntokens. This attention mechanism enables precise segment-level\ntextual-to-visual alignment, allowing the DiT architecture to effectively\nhandle video generation tasks with a fixed number of scenes. To further equip\nthe DiT architecture with the ability to generate additional scenes based on\nexisting ones, we incorporate a segment-level conditional mask, which\nconditions each newly generated segment on the preceding video segments,\nthereby enabling auto-regressive scene extension. Both qualitative and\nquantitative experiments confirm that Mask$^2$DiT excels in maintaining visual\nconsistency across segments while ensuring semantic alignment between each\nsegment and its corresponding text description. Our project page is\nhttps://tianhao-qi.github.io/Mask2DiTProject.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19860v1",
    "title": "Unpaired Translation of Chest X-ray Images for Lung Opacity Diagnosis via Adaptive Activation Masks and Cross-Domain Alignment",
    "authors": [
      "Junzhi Ning",
      "Dominic Marshall",
      "Yijian Gao",
      "Xiaodan Xing Yang Nan",
      "Yingying Fang",
      "Sheng Zhang",
      "Matthieu Komorowski",
      "Guang Yang"
    ],
    "abstract": "Chest X-ray radiographs (CXRs) play a pivotal role in diagnosing and\nmonitoring cardiopulmonary diseases. However, lung opacities in CXRs frequently\nobscure anatomical structures, impeding clear identification of lung borders\nand complicating the localization of pathology. This challenge significantly\nhampers segmentation accuracy and precise lesion identification, which are\ncrucial for diagnosis. To tackle these issues, our study proposes an unpaired\nCXR translation framework that converts CXRs with lung opacities into\ncounterparts without lung opacities while preserving semantic features. Central\nto our approach is the use of adaptive activation masks to selectively modify\nopacity regions in lung CXRs. Cross-domain alignment ensures translated CXRs\nwithout opacity issues align with feature maps and prediction labels from a\npre-trained CXR lesion classifier, facilitating the interpretability of the\ntranslation process. We validate our method using RSNA, MIMIC-CXR-JPG and JSRT\ndatasets, demonstrating superior translation quality through lower Frechet\nInception Distance (FID) and Kernel Inception Distance (KID) scores compared to\nexisting methods (FID: 67.18 vs. 210.4, KID: 0.01604 vs. 0.225). Evaluation on\nRSNA opacity, MIMIC acute respiratory distress syndrome (ARDS) patient CXRs and\nJSRT CXRs show our method enhances segmentation accuracy of lung borders and\nimproves lesion classification, further underscoring its potential in clinical\nsettings (RSNA: mIoU: 76.58% vs. 62.58%, Sensitivity: 85.58% vs. 77.03%; MIMIC\nARDS: mIoU: 86.20% vs. 72.07%, Sensitivity: 92.68% vs. 86.85%; JSRT: mIoU:\n91.08% vs. 85.6%, Sensitivity: 97.62% vs. 95.04%). Our approach advances CXR\nimaging analysis, especially in investigating segmentation impacts through\nimage translation techniques.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment",
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19848v1",
    "title": "Guarding against artificial intelligence--hallucinated citations: the case for full-text reference deposit",
    "authors": [
      "Alex Glynn"
    ],
    "abstract": "The tendency of generative artificial intelligence (AI) systems to\n\"hallucinate\" false information is well-known; AI-generated citations to\nnon-existent sources have made their way into the reference lists of\npeer-reviewed publications. Here, I propose a solution to this problem, taking\ninspiration from the Transparency and Openness Promotion (TOP) data sharing\nguidelines, the clash of generative AI with the American judiciary, and the\nprecedent set by submissions of prior art to the United States Patent and\nTrademark Office. Journals should require authors to submit the full text of\neach cited source along with their manuscripts, thereby preventing authors from\nciting any material whose full text they cannot produce. This solution requires\nlimited additional work on the part of authors or editors while effectively\nimmunizing journals against hallucinated references.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "I.2.0; K.4.1"
    ],
    "primary_category": "cs.DL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "transparency"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19817v1",
    "title": "Bitstream Collisions in Neural Image Compression via Adversarial Perturbations",
    "authors": [
      "Jordan Madden",
      "Lhamo Dorje",
      "Xiaohua Li"
    ],
    "abstract": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "adversarial",
      "adversarial attack"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19798v1",
    "title": "Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with Keypoints-Guided Diffusion Models",
    "authors": [
      "Ruixi You",
      "Hecheng Jia",
      "Feng Xu"
    ],
    "abstract": "Synthetic Aperture Radar (SAR) imagery provides all-weather, all-day, and\nhigh-resolution imaging capabilities but its unique imaging mechanism makes\ninterpretation heavily reliant on expert knowledge, limiting interpretability,\nespecially in complex target tasks. Translating SAR images into optical images\nis a promising solution to enhance interpretation and support downstream tasks.\nMost existing research focuses on scene-level translation, with limited work on\nobject-level translation due to the scarcity of paired data and the challenge\nof accurately preserving contour and texture details. To address these issues,\nthis study proposes a keypoint-guided diffusion model (KeypointDiff) for\nSAR-to-optical image translation of unpaired aircraft targets. This framework\nintroduces supervision on target class and azimuth angle via keypoints, along\nwith a training strategy for unpaired data. Based on the classifier-free\nguidance diffusion architecture, a class-angle guidance module (CAGM) is\ndesigned to integrate class and angle information into the diffusion generation\nprocess. Furthermore, adversarial loss and consistency loss are employed to\nimprove image fidelity and detail quality, tailored for aircraft targets.\nDuring sampling, aided by a pre-trained keypoint detector, the model eliminates\nthe requirement for manually labeled class and azimuth information, enabling\nautomated SAR-to-optical translation. Experimental results demonstrate that the\nproposed method outperforms existing approaches across multiple metrics,\nproviding an efficient and effective solution for object-level SAR-to-optical\ntranslation and downstream tasks. Moreover, the method exhibits strong\nzero-shot generalization to untrained aircraft types with the assistance of the\nkeypoint detector.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability",
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19793v2",
    "title": "In the Blink of an Eye: Instant Game Map Editing using a Generative-AI Smart Brush",
    "authors": [
      "Vitaly Gnatyuk",
      "Valeriia Koriukina",
      "Ilya Levoshevich",
      "Pavel Nurminskiy",
      "Guenter Wallner"
    ],
    "abstract": "With video games steadily increasing in complexity, automated generation of\ngame content has found widespread interest. However, the task of 3D gaming map\nart creation remains underexplored to date due to its unique complexity and\ndomain-specific challenges. While recent works have addressed related topics\nsuch as retro-style level generation and procedural terrain creation, these\nworks primarily focus on simpler data distributions. To the best of our\nknowledge, we are the first to demonstrate the application of modern AI\ntechniques for high-resolution texture manipulation in complex, highly detailed\nAAA 3D game environments. We introduce a novel Smart Brush for map editing,\ndesigned to assist artists in seamlessly modifying selected areas of a game map\nwith minimal effort. By leveraging generative adversarial networks and\ndiffusion models we propose two variants of the brush that enable efficient and\ncontext-aware generation. Our hybrid workflow aims to enhance both artistic\nflexibility and production efficiency, enabling the refinement of environments\nwithout manually reworking every detail, thus helping to bridge the gap between\nautomation and creative control in game development. A comparative evaluation\nof our two methods with adapted versions of several state-of-the art models\nshows that our GAN-based brush produces the sharpest and most detailed outputs\nwhile preserving image context while the evaluated state-of-the-art models tend\ntowards blurrier results and exhibit difficulties in maintaining contextual\nconsistency.",
    "categories": [
      "cs.CV",
      "I.4"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19791v1",
    "title": "SITA: Structurally Imperceptible and Transferable Adversarial Attacks for Stylized Image Generation",
    "authors": [
      "Jingdan Kang",
      "Haoxin Yang",
      "Yan Cai",
      "Huaidong Zhang",
      "Xuemiao Xu",
      "Yong Du",
      "Shengfeng He"
    ],
    "abstract": "Image generation technology has brought significant advancements across\nvarious fields but has also raised concerns about data misuse and potential\nrights infringements, particularly with respect to creating visual artworks.\nCurrent methods aimed at safeguarding artworks often employ adversarial\nattacks. However, these methods face challenges such as poor transferability,\nhigh computational costs, and the introduction of noticeable noise, which\ncompromises the aesthetic quality of the original artwork. To address these\nlimitations, we propose a Structurally Imperceptible and Transferable\nAdversarial (SITA) attacks. SITA leverages a CLIP-based destylization loss,\nwhich decouples and disrupts the robust style representation of the image. This\ndisruption hinders style extraction during stylized image generation, thereby\nimpairing the overall stylization process. Importantly, SITA eliminates the\nneed for a surrogate diffusion model, leading to significantly reduced\ncomputational overhead. The method's robust style feature disruption ensures\nhigh transferability across diverse models. Moreover, SITA introduces\nperturbations by embedding noise within the imperceptible structural details of\nthe image. This approach effectively protects against style extraction without\ncompromising the visual quality of the artwork. Extensive experiments\ndemonstrate that SITA offers superior protection for artworks against\nunauthorized use in stylized generation. It significantly outperforms existing\nmethods in terms of transferability, computational efficiency, and noise\nimperceptibility. Code is available at https://github.com/A-raniy-day/SITA.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "adversarial",
      "adversarial attack"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19777v1",
    "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation",
    "authors": [
      "Vladan StojniÄ",
      "Yannis Kalantidis",
      "JiÅÃ­ Matas",
      "Giorgos Tolias"
    ],
    "abstract": "We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps://github.com/vladan-stojnic/LPOSS",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19948v1",
    "title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards",
    "authors": [
      "Alexander Gambashidze",
      "Konstantin Sobolev",
      "Andrey Kuznetsov",
      "Ivan Oseledets"
    ],
    "abstract": "Can Visual Language Models (VLMs) effectively capture human visual\npreferences? This work addresses this question by training VLMs to think about\npreferences at test time, employing reinforcement learning methods inspired by\nDeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human\nPreference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the\nImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2\n(trained on approximately 25% of its data). These results match traditional\nencoder-based models while providing transparent reasoning and enhanced\ngeneralization. This approach allows to use not only rich VLM world knowledge,\nbut also its potential to think, yielding interpretable outcomes that help\ndecision-making processes. By demonstrating that human visual preferences\nreasonable by current VLMs, we introduce efficient soft-reward strategies for\nimage ranking, outperforming simplistic selection or scoring methods. This\nreasoning capability enables VLMs to rank arbitrary images-regardless of aspect\nratio or complexity-thereby potentially amplifying the effectiveness of visual\nPreference Optimization. By reducing the need for extensive markup while\nimproving reward generalization and explainability, our findings can be a\nstrong mile-stone that will enhance text-to-vision models even further.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19763v1",
    "title": "Interpretable Deep Regression Models with Interval-Censored Failure Time Data",
    "authors": [
      "Changhui Yuan",
      "Shishun Zhao",
      "Shuwei Li",
      "Xinyuan Song",
      "Zhao Chen"
    ],
    "abstract": "Deep neural networks (DNNs) have become powerful tools for modeling complex\ndata structures through sequentially integrating simple functions in each\nhidden layer. In survival analysis, recent advances of DNNs primarily focus on\nenhancing model capabilities, especially in exploring nonlinear covariate\neffects under right censoring. However, deep learning methods for\ninterval-censored data, where the unobservable failure time is only known to\nlie in an interval, remain underexplored and limited to specific data type or\nmodel. This work proposes a general regression framework for interval-censored\ndata with a broad class of partially linear transformation models, where key\ncovariate effects are modeled parametrically while nonlinear effects of\nnuisance multi-modal covariates are approximated via DNNs, balancing\ninterpretability and flexibility. We employ sieve maximum likelihood estimation\nby leveraging monotone splines to approximate the cumulative baseline hazard\nfunction. To ensure reliable and tractable estimation, we develop an EM\nalgorithm incorporating stochastic gradient descent. We establish the\nasymptotic properties of parameter estimators and show that the DNN estimator\nachieves minimax-optimal convergence. Extensive simulations demonstrate\nsuperior estimation and prediction accuracy over state-of-the-art methods.\nApplying our method to the Alzheimer's Disease Neuroimaging Initiative dataset\nyields novel insights and improved predictive performance compared to\ntraditional approaches.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19757v1",
    "title": "Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy",
    "authors": [
      "Zhi Hou",
      "Tianyi Zhang",
      "Yuwen Xiong",
      "Haonan Duan",
      "Hengjun Pu",
      "Ronglei Tong",
      "Chengyang Zhao",
      "Xizhou Zhu",
      "Yu Qiao",
      "Jifeng Dai",
      "Yuntao Chen"
    ],
    "abstract": "While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io.",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19739v2",
    "title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via Frequency-Decoupled Alignment and Degradation-Robust Fusion",
    "authors": [
      "Pihai Sun",
      "Junjun Jiang",
      "Yuanqi Yao",
      "Youyu Chen",
      "Wenbo Zhao",
      "Kui Jiang",
      "Xianming Liu"
    ],
    "abstract": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19733v1",
    "title": "How to RETIRE Tabular Data in Favor of Discrete Digital Signal Representation",
    "authors": [
      "PaweÅ Zyblewski",
      "Szymon Wojciechowski"
    ],
    "abstract": "The successes achieved by deep neural networks in computer vision tasks have\nled in recent years to the emergence of a new research area dubbed\nMulti-Dimensional Encoding (MDE). Methods belonging to this family aim to\ntransform tabular data into a homogeneous form of discrete digital signals\n(images) to apply convolutional networks to initially unsuitable problems.\nDespite the successive emerging works, the pool of multi-dimensional encoding\nmethods is still low, and the scope of research on existing modality encoding\ntechniques is quite limited. To contribute to this area of research, we propose\nthe Radar-based Encoding from Tabular to Image REpresentation (RETIRE), which\nallows tabular data to be represented as radar graphs, capturing the feature\ncharacteristics of each problem instance. RETIRE was compared with a pool of\nstate-of-the-art MDE algorithms as well as with XGBoost in terms of\nclassification accuracy and computational complexity. In addition, an analysis\nwas carried out regarding transferability and explainability to provide more\ninsight into both RETIRE and existing MDE techniques. The results obtained,\nsupported by statistical analysis, confirm the superiority of RETIRE over other\nestablished MDE methods.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20758v1",
    "title": "MindfulLIME: A Stable Solution for Explanations of Machine Learning Models with Enhanced Localization Precision -- A Medical Image Case Study",
    "authors": [
      "Shakiba Rahimiaghdam",
      "Hande Alemdar"
    ],
    "abstract": "Ensuring transparency in machine learning decisions is critically important,\nespecially in sensitive sectors such as healthcare, finance, and justice.\nDespite this, some popular explainable algorithms, such as Local Interpretable\nModel-agnostic Explanations (LIME), often produce unstable explanations due to\nthe random generation of perturbed samples. Random perturbation introduces\nsmall changes or noise to modified instances of the original data, leading to\ninconsistent explanations. Even slight variations in the generated samples\nsignificantly affect the explanations provided by such models, undermining\ntrust and hindering the adoption of interpretable models. To address this\nchallenge, we propose MindfulLIME, a novel algorithm that intelligently\ngenerates purposive samples using a graph-based pruning algorithm and\nuncertainty sampling. MindfulLIME substantially improves the consistency of\nvisual explanations compared to random sampling approaches. Our experimental\nevaluation, conducted on a widely recognized chest X-ray dataset, confirms\nMindfulLIME's stability with a 100% success rate in delivering reliable\nexplanations under identical conditions. Additionally, MindfulLIME improves the\nlocalization precision of visual explanations by reducing the distance between\nthe generated explanations and the actual local annotations compared to LIME.\nWe also performed comprehensive experiments considering various segmentation\nalgorithms and sample numbers, focusing on stability, quality, and efficiency.\nThe results demonstrate the outstanding performance of MindfulLIME across\ndifferent segmentation settings, generating fewer high-quality samples within a\nreasonable processing time. By addressing the stability limitations of LIME in\nimage data, MindfulLIME enhances the trustworthiness and interpretability of\nmachine learning models in specific medical imaging applications, a critical\ndomain.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability",
      "transparency"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19712v1",
    "title": "Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions",
    "authors": [
      "Sanghyuk Kim",
      "Minsik Seo",
      "Namwoo Kang"
    ],
    "abstract": "This study proposes a neural framework that predicts 3D vehicle collision\ndynamics by independently modeling global rigid-body motion and local\nstructural deformation. Unlike approaches directly predicting absolute\ndisplacement, this method explicitly separates the vehicle's overall\ntranslation and rotation from its structural deformation. Two specialized\nnetworks form the core of the framework: a quaternion-based Rigid Net for rigid\nmotion and a coordinate-based Deformation Net for local deformation. By\nindependently handling fundamentally distinct physical phenomena, the proposed\narchitecture achieves accurate predictions without requiring separate\nsupervision for each component. The model, trained on only 10% of available\nsimulation data, significantly outperforms baseline models, including single\nmulti-layer perceptron (MLP) and deep operator networks (DeepONet), with\nprediction errors reduced by up to 83%. Extensive validation demonstrates\nstrong generalization to collision conditions outside the training range,\naccurately predicting responses even under severe impacts involving extreme\nvelocities and large impact angles. Furthermore, the framework successfully\nreconstructs high-resolution deformation details from low-resolution inputs\nwithout increased computational effort. Consequently, the proposed approach\nprovides an effective, computationally efficient method for rapid and reliable\nassessment of vehicle safety across complex collision scenarios, substantially\nreducing the required simulation data and time while preserving prediction\nfidelity.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19711v1",
    "title": "Writing as a testbed for open ended agents",
    "authors": [
      "Sian Gooding",
      "Lucia Lopez-Rivilla",
      "Edward Grefenstette"
    ],
    "abstract": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19706v1",
    "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations",
    "authors": [
      "Jungin Park",
      "Jiyoung Lee",
      "Kwanghoon Sohn"
    ],
    "abstract": "View-invariant representation learning from egocentric (first-person, ego)\nand exocentric (third-person, exo) videos is a promising approach toward\ngeneralizing video understanding systems across multiple viewpoints. However,\nthis area has been underexplored due to the substantial differences in\nperspective, motion patterns, and context between ego and exo views. In this\npaper, we propose a novel masked ego-exo modeling that promotes both causal\ntemporal dynamics and cross-view alignment, called Bootstrap Your Own Views\n(BYOV), for fine-grained view-invariant video representation learning from\nunpaired ego-exo videos. We highlight the importance of capturing the\ncompositional nature of human actions as a basis for robust cross-view\nunderstanding. Specifically, self-view masking and cross-view masking\npredictions are designed to learn view-invariant and powerful representations\nconcurrently. Experimental results demonstrate that our BYOV significantly\nsurpasses existing approaches with notable gains across all metrics in four\ndownstream ego-exo video tasks. The code is available at\nhttps://github.com/park-jungin/byov.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19699v1",
    "title": "Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control",
    "authors": [
      "Muhammad Al-Zafar Khan",
      "Jamal Al-Karaki"
    ],
    "abstract": "In this study, we formulate the drone delivery problem as a control problem\nand solve it using Model Predictive Control. Two experiments are performed: The\nfirst is on a less challenging grid world environment with lower\ndimensionality, and the second is with a higher dimensionality and added\ncomplexity. The MPC method was benchmarked against three popular Multi-Agent\nReinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action\nLearners (JAL), and Value-Decomposition Networks (VDN). It was shown that the\nMPC method solved the problem quicker and required fewer optimal numbers of\ndrones to achieve a minimized cost and navigate the optimal path.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "control problem"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19690v2",
    "title": "Risk-Aware Reinforcement Learning for Autonomous Driving: Improving Safety When Driving through Intersection",
    "authors": [
      "Bo Leng",
      "Ran Yu",
      "Wei Han",
      "Lu Xiong",
      "Zhuoren Li",
      "Hailong Huang"
    ],
    "abstract": "Applying reinforcement learning to autonomous driving has garnered widespread\nattention. However, classical reinforcement learning methods optimize policies\nby maximizing expected rewards but lack sufficient safety considerations, often\nputting agents in hazardous situations. This paper proposes a risk-aware\nreinforcement learning approach for autonomous driving to improve the safety\nperformance when crossing the intersection. Safe critics are constructed to\nevaluate driving risk and work in conjunction with the reward critic to update\nthe actor. Based on this, a Lagrangian relaxation method and cyclic gradient\niteration are combined to project actions into a feasible safe region.\nFurthermore, a Multi-hop and Multi-layer perception (MLP) mixed Attention\nMechanism (MMAM) is incorporated into the actor-critic network, enabling the\npolicy to adapt to dynamic traffic and overcome permutation sensitivity\nchallenges. This allows the policy to focus more effectively on surrounding\npotential risks while enhancing the identification of passing opportunities.\nSimulation tests are conducted on different tasks at unsignalized\nintersections. The results show that the proposed approach effectively reduces\ncollision rates and improves crossing efficiency in comparison to baseline\nalgorithms. Additionally, our ablation experiments demonstrate the benefits of\nincorporating risk-awareness and MMAM into RL.",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19668v1",
    "title": "A multitask transformer to sign language translation using motion gesture primitives",
    "authors": [
      "Fredy Alejandro Mendoza LÃ³pez",
      "Jefferson Rodriguez",
      "Fabio MartÃ­nez"
    ],
    "abstract": "The absence of effective communication the deaf population represents the\nmain social gap in this community. Furthermore, the sign language, main deaf\ncommunication tool, is unlettered, i.e., there is no formal written\nrepresentation. In consequence, main challenge today is the automatic\ntranslation among spatiotemporal sign representation and natural text language.\nRecent approaches are based on encoder-decoder architectures, where the most\nrelevant strategies integrate attention modules to enhance non-linear\ncorrespondences, besides, many of these approximations require complex training\nand architectural schemes to achieve reasonable predictions, because of the\nabsence of intermediate text projections. However, they are still limited by\nthe redundant background information of the video sequences. This work\nintroduces a multitask transformer architecture that includes a gloss learning\nrepresentation to achieve a more suitable translation. The proposed approach\nalso includes a dense motion representation that enhances gestures and includes\nkinematic information, a key component in sign language. From this\nrepresentation it is possible to avoid background information and exploit the\ngeometry of the signs, in addition, it includes spatiotemporal representations\nthat facilitate the alignment between gestures and glosses as an intermediate\ntextual representation. The proposed approach outperforms the state-of-the-art\nevaluated on the CoL-SLTD dataset, achieving a BLEU-4 of 72,64% in split 1, and\na BLEU-4 of 14,64% in split 2. Additionally, the strategy was validated on the\nRWTH-PHOENIX-Weather 2014 T dataset, achieving a competitive BLEU-4 of 11,58%.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19661v1",
    "title": "CoSimGen: Controllable Diffusion Model for Simultaneous Image and Mask Generation",
    "authors": [
      "Rupak Bose",
      "Chinedu Innocent Nwoye",
      "Aditya Bhat",
      "Nicolas Padoy"
    ],
    "abstract": "The acquisition of annotated datasets with paired images and segmentation\nmasks is a critical challenge in domains such as medical imaging, remote\nsensing, and computer vision. Manual annotation demands significant resources,\nfaces ethical constraints, and depends heavily on domain expertise. Existing\ngenerative models often target single-modality outputs, either images or\nsegmentation masks, failing to address the need for high-quality, simultaneous\nimage-mask generation. Additionally, these models frequently lack adaptable\nconditioning mechanisms, restricting control over the generated outputs and\nlimiting their applicability for dataset augmentation and rare scenario\nsimulation. We propose CoSimGen, a diffusion-based framework for controllable\nsimultaneous image and mask generation. Conditioning is intuitively achieved\nthrough (1) text prompts grounded in class semantics, (2) spatial embedding of\ncontext prompts to provide spatial coherence, and (3) spectral embedding of\ntimestep information to model noise levels during diffusion. To enhance\ncontrollability and training efficiency, the framework incorporates contrastive\ntriplet loss between text and class embeddings, alongside diffusion and\nadversarial losses. Initial low-resolution outputs 128 x 128 are super-resolved\nto 512 x 512, producing high-fidelity images and masks with strict adherence to\nconditions. We evaluate CoSimGen on metrics such as FID, KID, LPIPS, Class FID,\nPositive predicted value for image fidelity and semantic alignment of generated\nsamples over 4 diverse datasets. CoSimGen achieves state-of-the-art performance\nacross all datasets, achieving the lowest KID of 0.11 and LPIPS of 0.53 across\ndatasets.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment",
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19654v2",
    "title": "RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models",
    "authors": [
      "Mehdi Moshtaghi",
      "Siavash H. Khajavi",
      "Joni Pajarinen"
    ],
    "abstract": "We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-27",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19607v1",
    "title": "Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review",
    "authors": [
      "Edward Gu",
      "Ho Chit Siu",
      "Melanie Platt",
      "Isabelle Hurley",
      "Jaime PeÃ±a",
      "Rohan Paleja"
    ],
    "abstract": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19598v1",
    "title": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian Moral Dilemmas",
    "authors": [
      "Giovanni Franco Gabriel Marraffini",
      "AndrÃ©s Cotton",
      "Noe Fabian Hsueh",
      "Axel Fridman",
      "Juan Wisznia",
      "Luciano Del Corro"
    ],
    "abstract": "The question of how to make decisions that maximise the well-being of all\npersons is very relevant to design language models that are beneficial to\nhumanity and free from harm. We introduce the Greatest Good Benchmark to\nevaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis\nacross 15 diverse LLMs reveals consistently encoded moral preferences that\ndiverge from established moral theories and lay population moral standards.\nMost LLMs have a marked preference for impartial beneficence and rejection of\ninstrumental harm. These findings showcase the 'artificial moral compass' of\nLLMs, offering insights into their moral alignment.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19591v1",
    "title": "Boosting the Transferability of Audio Adversarial Examples with Acoustic Representation Optimization",
    "authors": [
      "Weifei Jin",
      "Junjie Su",
      "Hejia Wang",
      "Yulin Ye",
      "Jie Hao"
    ],
    "abstract": "With the widespread application of automatic speech recognition (ASR)\nsystems, their vulnerability to adversarial attacks has been extensively\nstudied. However, most existing adversarial examples are generated on specific\nindividual models, resulting in a lack of transferability. In real-world\nscenarios, attackers often cannot access detailed information about the target\nmodel, making query-based attacks unfeasible. To address this challenge, we\npropose a technique called Acoustic Representation Optimization that aligns\nadversarial perturbations with low-level acoustic characteristics derived from\nspeech representation models. Rather than relying on model-specific,\nhigher-layer abstractions, our approach leverages fundamental acoustic\nrepresentations that remain consistent across diverse ASR architectures. By\nenforcing an acoustic representation loss to guide perturbations toward these\nrobust, lower-level representations, we enhance the cross-model transferability\nof adversarial examples without degrading audio quality. Our method is\nplug-and-play and can be integrated with any existing attack methods. We\nevaluate our approach on three modern ASR models, and the experimental results\ndemonstrate that our method significantly improves the transferability of\nadversarial examples generated by previous methods while preserving the audio\nquality.",
    "categories": [
      "cs.SD",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "adversarial",
      "adversarial attack",
      "adversarial example"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19586v1",
    "title": "Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment",
    "authors": [
      "Hanlin Wu",
      "Xufeng Duan",
      "Zhenguang Cai"
    ],
    "abstract": "Voice-based AI development faces unique challenges in processing both\nlinguistic and paralinguistic information. This study compares how large\naudio-language models (LALMs) and humans integrate speaker characteristics\nduring speech comprehension, asking whether LALMs process\nspeaker-contextualized language in ways that parallel human cognitive\nmechanisms. We compared two LALMs' (Qwen2-Audio and Ultravox 0.5) processing\npatterns with human EEG responses. Using surprisal and entropy metrics from the\nmodels, we analyzed their sensitivity to speaker-content incongruency across\nsocial stereotype violations (e.g., a man claiming to regularly get manicures)\nand biological knowledge violations (e.g., a man claiming to be pregnant).\nResults revealed that Qwen2-Audio exhibited increased surprisal for\nspeaker-incongruent content and its surprisal values significantly predicted\nhuman N400 responses, while Ultravox 0.5 showed limited sensitivity to speaker\ncharacteristics. Importantly, neither model replicated the human-like\nprocessing distinction between social violations (eliciting N400 effects) and\nbiological violations (eliciting P600 effects). These findings reveal both the\npotential and limitations of current LALMs in processing speaker-contextualized\nlanguage, and suggest differences in social-linguistic processing mechanisms\nbetween humans and LALMs.",
    "categories": [
      "cs.CL",
      "q-bio.NC"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19564v1",
    "title": "FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments",
    "authors": [
      "Sree Bhargavi Balija"
    ],
    "abstract": "As artificial intelligence systems increasingly operate in Real-world\nenvironments, the integration of multi-modal data sources such as vision,\nlanguage, and audio presents both unprecedented opportunities and critical\nchallenges for achieving trustworthy intelligence. In this paper, we propose a\nnovel framework that unifies federated learning with explainable multi-modal\nreasoning to ensure trustworthiness in decentralized, dynamic settings. Our\napproach, called FedMM-X (Federated Multi-Modal Explainable Intelligence),\nleverages cross-modal consistency checks, client-level interpretability\nmechanisms, and dynamic trust calibration to address challenges posed by data\nheterogeneity, modality imbalance, and out-of-distribution generalization.\nThrough rigorous evaluation across federated multi-modal benchmarks involving\nvision-language tasks, we demonstrate improved performance in both accuracy and\ninterpretability while reducing vulnerabilities to adversarial and spurious\ncorrelations. Further, we introduce a novel trust score aggregation method to\nquantify global model reliability under dynamic client participation. Our\nfindings pave the way toward developing robust, interpretable, and socially\nresponsible AI systems in Real-world environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability",
      "adversarial",
      "responsible AI"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19540v1",
    "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
    "authors": [
      "Dahyun Jung",
      "Seungyoon Lee",
      "Hyeonseok Moon",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety",
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19508v1",
    "title": "Improved Alignment of Modalities in Large Vision Language Models",
    "authors": [
      "Kartik Jangra",
      "Aman Kumar Singh",
      "Yashwani Mann",
      "Geetanjali Rathee"
    ],
    "abstract": "Recent advancements in vision-language models have achieved remarkable\nresults in making language models understand vision inputs. However, a unified\napproach to align these models across diverse tasks such as image captioning\nand visual question answering remains a challenge. Existing methods either\nrequire very big language models or very big datasets which is not efficient in\nutilizing existing models. This paper addresses this gap and devises a training\nstrategy of auto-regressive vision-language models, to unify vision-language\ntasks like image-captioning and visual question answering. We propose four\ntraining stages for aligning the vision model with the language model, in other\nwords, the language model is given an ability to process visual inputs. We also\ndevise different attention masks for training transformer-based language models\nthat improve the quality of visual features. Further, we introduce some\nfindings, 1) the attention mask should not be applied on visual inputs, 2) the\nLanguage model converges faster on AI- generated data, 3) More work should be\ndone in the alignment stage during the pre-training of the model, 4) the model\ncan easily adapt to any downstream tasks like visual question answering on\nhealthcare datasets like PathVQA. After training the model for one epoch for\nall the stages, it outperforms large models like VILA-13 billion models on\ncommon benchmarks like CIDEr scores on COCO and Flickr30k datasets and achieves\nvery close scores to GIT-2 on the same dataset despite being a much smaller\nmodel trained on a much smaller dataset. All of the training is done using best\npractices available like multi- GPU parallel training, lower-precision training\nwith 16-bit float numbers, faster attention (SDPA), and gradient accumulation,\nand completed the training within 12 hours.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19503v1",
    "title": "Adaptive Weighted Parameter Fusion with CLIP for Class-Incremental Learning",
    "authors": [
      "Juncen Guo",
      "Xiaoguang Zhu",
      "Liangyu Teng",
      "Hao Yang",
      "Jing Liu",
      "Yang Liu",
      "Liang Song"
    ],
    "abstract": "Class-incremental Learning (CIL) enables the model to incrementally absorb\nknowledge from new classes and build a generic classifier across all previously\nencountered classes. When the model optimizes with new classes, the knowledge\nof previous classes is inevitably erased, leading to catastrophic forgetting.\nAddressing this challenge requires making a trade-off between retaining old\nknowledge and accommodating new information. However, this balancing process\noften requires sacrificing some information, which can lead to a partial loss\nin the model's ability to discriminate between classes. To tackle this issue,\nwe design the adaptive weighted parameter fusion with Contrastive\nLanguage-Image Pre-training (CLIP), which not only takes into account the\nvariability of the data distribution of different tasks, but also retains all\nthe effective information of the parameter matrix to the greatest extent. In\naddition, we introduce a balance factor that can balance the data distribution\nalignment and distinguishability of adjacent tasks. Experimental results on\nseveral traditional benchmarks validate the superiority of the proposed method.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19502v1",
    "title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model",
    "authors": [
      "Qi Chen",
      "Yinghao Cui",
      "Guobin Hong",
      "Karumuri Ashok",
      "Yuchun Pu",
      "Xiaogu Zheng",
      "Xuanze Zhang",
      "Wei Zhong",
      "Peng Zhan",
      "Zhonglei Wang"
    ],
    "abstract": "El Ni\\~no-Southern Oscillation (ENSO) is a prominent mode of interannual\nclimate variability with far-reaching global impacts. Its evolution is governed\nby intricate air-sea interactions, posing significant challenges for long-term\nprediction. In this study, we introduce CTEFNet, a multivariate deep learning\nmodel that synergizes convolutional neural networks and transformers to enhance\nENSO forecasting. By integrating multiple oceanic and atmospheric predictors,\nCTEFNet extends the effective forecast lead time to 20 months while mitigating\nthe impact of the spring predictability barrier, outperforming both dynamical\nmodels and state-of-the-art deep learning approaches. Furthermore, CTEFNet\noffers physically meaningful and statistically significant insights through\ngradient-based sensitivity analysis, revealing the key precursor signals that\ngovern ENSO dynamics, which align with well-established theories and reveal new\ninsights about inter-basin interactions among the Pacific, Atlantic, and Indian\nOceans. The CTEFNet's superior predictive skill and interpretable sensitivity\nassessments underscore its potential for advancing climate prediction. Our\nfindings highlight the importance of multivariate coupling in ENSO evolution\nand demonstrate the promise of deep learning in capturing complex climate\ndynamics with enhanced interpretability.",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "primary_category": "physics.geo-ph",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19501v1",
    "title": "Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs",
    "authors": [
      "Vinayak Mali",
      "Saurabh Jaiswal"
    ],
    "abstract": "Falls among elderly residents in assisted living homes pose significant\nhealth risks, often leading to injuries and a decreased quality of life.\nCurrent fall detection solutions typically rely on sensor-based systems that\nrequire dedicated hardware, or on video-based models that demand high\ncomputational resources and GPUs for real-time processing. In contrast, this\npaper presents a robust fall detection system that does not require any\nadditional sensors or high-powered hardware. The system uses pose estimation\ntechniques, combined with threshold-based analysis and a voting mechanism, to\neffectively distinguish between fall and non-fall activities. For pose\ndetection, we leverage MediaPipe, a lightweight and efficient framework that\nenables real-time processing on standard CPUs with minimal computational\noverhead. By analyzing motion, body position, and key pose points, the system\nprocesses pose features with a 20-frame buffer, minimizing false positives and\nmaintaining high accuracy even in real-world settings. This unobtrusive,\nresource-efficient approach provides a practical solution for enhancing\nresident safety in old age homes, without the need for expensive sensors or\nhigh-end computational resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19496v1",
    "title": "SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables Design Exploration",
    "authors": [
      "Mohammad Daffa Robani",
      "Paul Saves",
      "Pramudita Satria Palar",
      "Lavi Rizki Zuhal",
      "oseph Morlier"
    ],
    "abstract": "Surrogate models are of high interest for many engineering applications,\nserving as cheap-to-evaluate time-efficient approximations of black-box\nfunctions to help engineers and practitioners make decisions and understand\ncomplex systems. As such, the need for explainability methods is rising and\nmany studies have been performed to facilitate knowledge discovery from\nsurrogate models. To respond to these enquiries, this paper introduces SMT-EX,\nan enhancement of the open-source Python Surrogate Modeling Toolbox (SMT) that\nintegrates explainability techniques into a state-of-the-art surrogate\nmodelling framework. More precisely, SMT-EX includes three key explainability\nmethods: Shapley Additive Explanations, Partial Dependence Plot, and Individual\nConditional Expectations. A peculiar explainability dependency of SMT has been\ndeveloped for such purpose that can be easily activated once the surrogate\nmodel is built, offering a user-friendly and efficient tool for swift insight\nextraction. The effectiveness of SMT-EX is showcased through two test cases.\nThe first case is a 10-variable wing weight problem with purely continuous\nvariables and the second one is a 3-variable mixed-categorical cantilever beam\nbending problem. Relying on SMT-EX analyses for these problems, we demonstrate\nits versatility in addressing a diverse range of problem characteristics.\nSMT-Explainability is freely available on Github:\nhttps://github.com/SMTorg/smt-explainability .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19476v1",
    "title": "Extracting Interpretable Logic Rules from Graph Neural Networks",
    "authors": [
      "Chuqin Geng",
      "Zhaoyue Wang",
      "Ziyu Zhao",
      "Haolin Ye",
      "Xujie Si"
    ],
    "abstract": "Graph neural networks (GNNs) operate over both input feature spaces and\ncombinatorial graph structures, making it challenging to understand the\nrationale behind their predictions. As GNNs gain widespread popularity and\ndemonstrate success across various domains, such as drug discovery, studying\ntheir interpretability has become a critical task. To address this, many\nexplainability methods have been proposed, with recent efforts shifting from\ninstance-specific explanations to global concept-based explainability. However,\nthese approaches face several limitations, such as relying on predefined\nconcepts and explaining only a limited set of patterns. To address this, we\npropose a novel framework, LOGICXGNN, for extracting interpretable logic rules\nfrom GNNs. LOGICXGNN is model-agnostic, efficient, and data-driven, eliminating\nthe need for predefined concepts. More importantly, it can serve as a\nrule-based classifier and even outperform the original neural models. Its\ninterpretability facilitates knowledge discovery, as demonstrated by its\nability to extract detailed and accurate chemistry knowledge that is often\noverlooked by existing methods. Another key advantage of LOGICXGNN is its\nability to generate new graph instances in a controlled and transparent manner,\noffering significant potential for applications such as drug design. We\nempirically demonstrate these merits through experiments on real-world datasets\nsuch as MUTAG and BBBP.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability",
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19466v1",
    "title": "A Probabilistic Neuro-symbolic Layer for Algebraic Constraint Satisfaction",
    "authors": [
      "Leander Kurscheidt",
      "Paolo Morettin",
      "Roberto Sebastiani",
      "Andrea Passerini",
      "Antonio Vergari"
    ],
    "abstract": "In safety-critical applications, guaranteeing the satisfaction of constraints\nover continuous environments is crucial, e.g., an autonomous agent should never\ncrash into obstacles or go off-road. Neural models struggle in the presence of\nthese constraints, especially when they involve intricate algebraic\nrelationships. To address this, we introduce a differentiable probabilistic\nlayer that guarantees the satisfaction of non-convex algebraic constraints over\ncontinuous variables. This probabilistic algebraic layer (PAL) can be\nseamlessly plugged into any neural architecture and trained via maximum\nlikelihood without requiring approximations. PAL defines a distribution over\nconjunctions and disjunctions of linear inequalities, parameterized by\npolynomials. This formulation enables efficient and exact renormalization via\nsymbolic integration, which can be amortized across different data points and\neasily parallelized on a GPU. We showcase PAL and our integration scheme on a\nnumber of benchmarks for algebraic constraint integration and on real-world\ntrajectory data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19462v1",
    "title": "AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset",
    "authors": [
      "Haiyu Zhang",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Xihui Liu",
      "Yunhong Wang",
      "Yu Qiao"
    ],
    "abstract": "Diffusion models have achieved remarkable progress in the field of video\ngeneration. However, their iterative denoising nature requires a large number\nof inference steps to generate a video, which is slow and computationally\nexpensive. In this paper, we begin with a detailed analysis of the challenges\npresent in existing diffusion distillation methods and propose a novel\nefficient method, namely AccVideo, to reduce the inference steps for\naccelerating video diffusion models with synthetic dataset. We leverage the\npretrained video diffusion model to generate multiple valid denoising\ntrajectories as our synthetic dataset, which eliminates the use of useless data\npoints during distillation. Based on the synthetic dataset, we design a\ntrajectory-based few-step guidance that utilizes key data points from the\ndenoising trajectories to learn the noise-to-video mapping, enabling video\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\nthe data distribution at each diffusion timestep, we introduce an adversarial\ntraining strategy to align the output distribution of the student model with\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\nexperiments demonstrate that our model achieves 8.5x improvements in generation\nspeed compared to the teacher model while maintaining comparable performance.\nCompared to previous accelerating methods, our approach is capable of\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\n720x1280, 24fps.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19418v1",
    "title": "Multi-Agent Deep Reinforcement Learning for Safe Autonomous Driving with RICS-Assisted MEC",
    "authors": [
      "Xueyao Zhang",
      "Bo Yang",
      "Xuelin Cao",
      "Zhiwen Yu",
      "George C. Alexandropoulos",
      "Yan Zhang",
      "Merouane Debbah",
      "Chau Yuen"
    ],
    "abstract": "Environment sensing and fusion via onboard sensors are envisioned to be\nwidely applied in future autonomous driving networks. This paper considers a\nvehicular system with multiple self-driving vehicles that is assisted by\nmulti-access edge computing (MEC), where image data collected by the sensors is\noffloaded from cellular vehicles to the MEC server using\nvehicle-to-infrastructure (V2I) links. Sensory data can also be shared among\nsurrounding vehicles via vehicle-to-vehicle (V2V) communication links. To\nimprove spectrum utilization, the V2V links may reuse the same frequency\nspectrum with V2I links, which may cause severe interference. To tackle this\nissue, we leverage reconfigurable intelligent computational surfaces (RICSs) to\njointly enable V2I reflective links and mitigate interference appearing at the\nV2V links. Considering the limitations of traditional algorithms in addressing\nthis problem, such as the assumption for quasi-static channel state\ninformation, which restricts their ability to adapt to dynamic environmental\nchanges and leads to poor performance under frequently varying channel\nconditions, in this paper, we formulate the problem at hand as a Markov game.\nOur novel formulation is applied to time-varying channels subject to multi-user\ninterference and introduces a collaborative learning mechanism among users. The\nconsidered optimization problem is solved via a driving safety-enabled\nmulti-agent deep reinforcement learning (DS-MADRL) approach that capitalizes on\nthe RICS presence. Our extensive numerical investigations showcase that the\nproposed reinforcement learning approach achieves faster convergence and\nsignificant enhancements in both data rate and driving safety, as compared to\nvarious state-of-the-art benchmarks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19405v1",
    "title": "Multi-modal 3D Pose and Shape Estimation with Computed Tomography",
    "authors": [
      "Mingxiao Tu",
      "Hoijoon Jung",
      "Alireza Moghadam",
      "Jineel Raythatha",
      "Lachlan Allan",
      "Jeremy Hsu",
      "Andre Kyme",
      "Jinman Kim"
    ],
    "abstract": "In perioperative care, precise in-bed 3D patient pose and shape estimation\n(PSE) can be vital in optimizing patient positioning in preoperative planning,\nenabling accurate overlay of medical images for augmented reality-based\nsurgical navigation, and mitigating risks of prolonged immobility during\nrecovery. Conventional PSE methods relying on modalities such as RGB-D,\ninfrared, or pressure maps often struggle with occlusions caused by bedding and\ncomplex patient positioning, leading to inaccurate estimation that can affect\nclinical outcomes. To address these challenges, we present the first\nmulti-modal in-bed patient 3D PSE network that fuses detailed geometric\nfeatures extracted from routinely acquired computed tomography (CT) scans with\ndepth maps (mPSE-CT). mPSE-CT incorporates a shape estimation module that\nutilizes probabilistic correspondence alignment, a pose estimation module with\na refined neural network, and a final parameters mixing module. This\nmulti-modal network robustly reconstructs occluded body regions and enhances\nthe accuracy of the estimated 3D human mesh model. We validated mPSE-CT using\nproprietary whole-body rigid phantom and volunteer datasets in clinical\nscenarios. mPSE-CT outperformed the best-performing prior method by 23% and\n49.16% in pose and shape estimation respectively, demonstrating its potential\nfor improving clinical outcomes in challenging perioperative environments.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19404v2",
    "title": "LangBridge: Interpreting Image as a Combination of Language Embeddings",
    "authors": [
      "Jiaqi Liao",
      "Yuwei Niu",
      "Fanqing Meng",
      "Hao Li",
      "Changyao Tian",
      "Yinuo Du",
      "Yuwen Xiong",
      "Dianqi Li",
      "Xizhou Zhu",
      "Li Yuan",
      "Jifeng Dai",
      "Yu Cheng"
    ],
    "abstract": "Recent years have witnessed remarkable advances in Large Vision-Language\nModels (LVLMs), which have achieved human-level performance across various\ncomplex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMs\ntypically employ a shallow MLP for visual-language alignment through a\ntwo-stage training process: pretraining for cross-modal alignment followed by\ninstruction tuning. While this approach has proven effective, the underlying\nmechanisms of how MLPs bridge the modality gap remain poorly understood.\nAlthough some research has explored how LLMs process transformed visual tokens,\nfew studies have investigated the fundamental alignment mechanism. Furthermore,\nthe MLP adapter requires retraining whenever switching LLM backbones. To\naddress these limitations, we first investigate the working principles of MLP\nadapters and discover that they learn to project visual embeddings into\nsubspaces spanned by corresponding text embeddings progressively. Based on this\ninsight, we propose LangBridge, a novel adapter that explicitly maps visual\ntokens to linear combinations of LLM vocabulary embeddings. This innovative\ndesign enables pretraining-free adapter transfer across different LLMs while\nmaintaining performance. Our experimental results demonstrate that a LangBridge\nadapter pre-trained on Qwen2-0.5B can be directly applied to larger models such\nas LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall,\nLangBridge enables interpretable vision-language alignment by grounding visual\nrepresentations in LLM vocab embedding, while its plug-and-play design ensures\nefficient reuse across multiple LLMs with nearly no performance degradation.\nSee our project page at https://jiaqiliao77.github.io/LangBridge.github.io/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19397v1",
    "title": "Quality-focused Active Adversarial Policy for Safe Grasping in Human-Robot Interaction",
    "authors": [
      "Chenghao Li",
      "Razvan Beuran",
      "Nak Young Chong"
    ],
    "abstract": "Vision-guided robot grasping methods based on Deep Neural Networks (DNNs)\nhave achieved remarkable success in handling unknown objects, attributable to\ntheir powerful generalizability. However, these methods with this\ngeneralizability tend to recognize the human hand and its adjacent objects as\ngraspable targets, compromising safety during Human-Robot Interaction (HRI). In\nthis work, we propose the Quality-focused Active Adversarial Policy (QFAAP) to\nsolve this problem. Specifically, the first part is the Adversarial Quality\nPatch (AQP), wherein we design the adversarial quality patch loss and leverage\nthe grasp dataset to optimize a patch with high quality scores. Next, we\nconstruct the Projected Quality Gradient Descent (PQGD) and integrate it with\nthe AQP, which contains only the hand region within each real-time frame,\nendowing the AQP with fast adaptability to the human hand shape. Through AQP\nand PQGD, the hand can be actively adversarial with the surrounding objects,\nlowering their quality scores. Therefore, further setting the quality score of\nthe hand to zero will reduce the grasping priority of both the hand and its\nadjacent objects, enabling the robot to grasp other objects away from the hand\nwithout emergency stops. We conduct extensive experiments on the benchmark\ndatasets and a cobot, showing the effectiveness of QFAAP. Our code and demo\nvideos are available here: https://github.com/clee-jaist/QFAAP.",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety",
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19391v1",
    "title": "TraF-Align: Trajectory-aware Feature Alignment for Asynchronous Multi-agent Perception",
    "authors": [
      "Zhiying Song",
      "Lei Yang",
      "Fuxi Wen",
      "Jun Li"
    ],
    "abstract": "Cooperative perception presents significant potential for enhancing the\nsensing capabilities of individual vehicles, however, inter-agent latency\nremains a critical challenge. Latencies cause misalignments in both spatial and\nsemantic features, complicating the fusion of real-time observations from the\nego vehicle with delayed data from others. To address these issues, we propose\nTraF-Align, a novel framework that learns the flow path of features by\npredicting the feature-level trajectory of objects from past observations up to\nthe ego vehicle's current time. By generating temporally ordered sampling\npoints along these paths, TraF-Align directs attention from the current-time\nquery to relevant historical features along each trajectory, supporting the\nreconstruction of current-time features and promoting semantic interaction\nacross multiple frames. This approach corrects spatial misalignment and ensures\nsemantic consistency across agents, effectively compensating for motion and\nachieving coherent feature fusion. Experiments on two real-world datasets,\nV2V4Real and DAIR-V2X-Seq, show that TraF-Align sets a new benchmark for\nasynchronous cooperative perception.",
    "categories": [
      "cs.CV",
      "cs.MA"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment",
      "misalignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19377v1",
    "title": "Interpretable Generative Models through Post-hoc Concept Bottlenecks",
    "authors": [
      "Akshay Kulkarni",
      "Ge Yan",
      "Chung-En Sun",
      "Tuomas Oikarinen",
      "Tsui-Wei Weng"
    ],
    "abstract": "Concept bottleneck models (CBM) aim to produce inherently interpretable\nmodels that rely on human-understandable concepts for their predictions.\nHowever, existing approaches to design interpretable generative models based on\nCBMs are not yet efficient and scalable, as they require expensive generative\nmodel training from scratch as well as real images with labor-intensive concept\nsupervision. To address these challenges, we present two novel and low-cost\nmethods to build interpretable generative models through post-hoc techniques\nand we name our approaches: concept-bottleneck autoencoder (CB-AE) and concept\ncontroller (CC). Our proposed approaches enable efficient and scalable training\nwithout the need of real data and require only minimal to no concept\nsupervision. Additionally, our methods generalize across modern generative\nmodel families including generative adversarial networks and diffusion models.\nWe demonstrate the superior interpretability and steerability of our methods on\nnumerous standard datasets like CelebA, CelebA-HQ, and CUB with large\nimprovements (average ~25%) over the prior work, while being 4-15x faster to\ntrain. Finally, a large-scale user study is performed to validate the\ninterpretability and steerability of our methods.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability",
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19361v1",
    "title": "ImageSet2Text: Describing Sets of Images through Text",
    "authors": [
      "Piera Riccio",
      "Francesco Galati",
      "Kajetan Schweighofer",
      "Noa Garcia",
      "Nuria Oliver"
    ],
    "abstract": "We introduce ImageSet2Text, a novel approach that leverages vision-language\nfoundation models to automatically create natural language descriptions of\nimage sets. Inspired by concept bottleneck models (CBMs) and based on\nvisual-question answering (VQA) chains, ImageSet2Text iteratively extracts key\nconcepts from image subsets, encodes them into a structured graph, and refines\ninsights using an external knowledge graph and CLIP-based validation. This\niterative process enhances interpretability and enables accurate and detailed\nset-level summarization. Through extensive experiments, we evaluate\nImageSet2Text's descriptions on accuracy, completeness, readability and overall\nquality, benchmarking it against existing vision-language models and\nintroducing new datasets for large-scale group image captioning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19349v1",
    "title": "Optimal Parameter Adaptation for Safety-Critical Control via Safe Barrier Bayesian Optimization",
    "authors": [
      "Shengbo Wang",
      "Ke Li",
      "Zheng Yan",
      "Zhenyuan Guo",
      "Song Zhu",
      "Guanghui Wen",
      "Shiping Wen"
    ],
    "abstract": "Safety is of paramount importance in control systems to avoid costly risks\nand catastrophic damages. The control barrier function (CBF) method, a\npromising solution for safety-critical control, poses a new challenge of\nenhancing control performance due to its direct modification of original\ncontrol design and the introduction of uncalibrated parameters. In this work,\nwe shed light on the crucial role of configurable parameters in the CBF method\nfor performance enhancement with a systematical categorization. Based on that,\nwe propose a novel framework combining the CBF method with Bayesian\noptimization (BO) to optimize the safe control performance. Considering\nfeasibility/safety-critical constraints, we develop a safe version of BO using\nthe barrier-based interior method to efficiently search for promising feasible\nconfigurable parameters. Furthermore, we provide theoretical criteria of our\nframework regarding safety and optimality. An essential advantage of our\nframework lies in that it can work in model-agnostic environments, leaving\nsufficient flexibility in designing objective and constraint functions.\nFinally, simulation experiments on swing-up control and high-fidelity adaptive\ncruise control are conducted to demonstrate the effectiveness of our framework.",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19347v1",
    "title": "Stop Walking in Circles! Bailing Out Early in Projected Gradient Descent",
    "authors": [
      "Philip Doldo",
      "Derek Everett",
      "Amol Khanna",
      "Andre T Nguyen",
      "Edward Raff"
    ],
    "abstract": "Projected Gradient Descent (PGD) under the $L_\\infty$ ball has become one of\nthe defacto methods used in adversarial robustness evaluation for computer\nvision (CV) due to its reliability and efficacy, making a strong and\neasy-to-implement iterative baseline. However, PGD is computationally demanding\nto apply, especially when using thousands of iterations is the current\nbest-practice recommendation to generate an adversarial example for a single\nimage. In this work, we introduce a simple novel method for early termination\nof PGD based on cycle detection by exploiting the geometry of how PGD is\nimplemented in practice and show that it can produce large speedup factors\nwhile providing the \\emph{exact} same estimate of model robustness as standard\nPGD. This method substantially speeds up PGD without sacrificing any attack\nstrength, enabling evaluations of robustness that were previously\ncomputationally intractable.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "adversarial",
      "adversarial example"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19328v1",
    "title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents",
    "authors": [
      "Vidya Srinivas",
      "Xuhai Xu",
      "Xin Liu",
      "Kumar Ayush",
      "Isaac Galatzer-Levy",
      "Shwetak Patel",
      "Daniel McDuff",
      "Tim Althoff"
    ],
    "abstract": "While NLP research has made strides in conversational tasks, many approaches\nfocus on single-turn responses with well-defined objectives or evaluation\ncriteria. In contrast, coaching presents unique challenges with initially\nundefined goals that evolve through multi-turn interactions, subjective\nevaluation criteria, mixed-initiative dialogue. In this work, we describe and\nimplement five multi-turn coaching agents that exhibit distinct conversational\nstyles, and evaluate them through a user study, collecting first-person\nfeedback on 155 conversations. We find that users highly value core\nfunctionality, and that stylistic components in absence of core components are\nviewed negatively. By comparing user feedback with third-person evaluations\nfrom health experts and an LM, we reveal significant misalignment across\nevaluation approaches. Our findings provide insights into design and evaluation\nof conversational coaching agents and contribute toward improving\nhuman-centered NLP applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment",
      "misalignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19311v1",
    "title": "LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text",
    "authors": [
      "Weizhi Chen",
      "Jingbo Chen",
      "Yupeng Deng",
      "Jiansheng Chen",
      "Yuman Feng",
      "Zhihao Xi",
      "Diyou Liu",
      "Kai Li",
      "Yu Meng"
    ],
    "abstract": "This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https://github.com/MitsuiChen14/LRSCLIP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19309v1",
    "title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees",
    "authors": [
      "Gollam Rabby",
      "Diyana Muhammed",
      "Prasenjit Mitra",
      "SÃ¶ren Auer"
    ],
    "abstract": "Scientific hypothesis generation is a fundamentally challenging task in\nresearch, requiring the synthesis of novel and empirically grounded insights.\nTraditional approaches rely on human intuition and domain expertise, while\npurely large language model (LLM) based methods often struggle to produce\nhypotheses that are both innovative and reliable. To address these limitations,\nwe propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel\nframework that integrates Monte Carlo Tree Search with Nash Equilibrium\nstrategies to iteratively refine and validate hypotheses. MC-NEST dynamically\nbalances exploration and exploitation through adaptive sampling strategies,\nwhich prioritize high-potential hypotheses while maintaining diversity in the\nsearch space. We demonstrate the effectiveness of MC-NEST through comprehensive\nexperiments across multiple domains, including biomedicine, social science, and\ncomputer science. MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a\n1-3 scale) for novelty, clarity, significance, and verifiability metrics on the\nsocial science, computer science, and biomedicine datasets, respectively,\noutperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51,\nand 2.52 on the same datasets. These results underscore MC-NEST's ability to\ngenerate high-quality, empirically grounded hypotheses across diverse domains.\nFurthermore, MC-NEST facilitates structured human-AI collaboration, ensuring\nthat LLMs augment human creativity rather than replace it. By addressing key\nchallenges such as iterative refinement and the exploration-exploitation\nbalance, MC-NEST sets a new benchmark in automated hypothesis generation.\nAdditionally, MC-NEST's ethical design enables responsible AI use, emphasizing\ntransparency and human supervision in hypothesis generation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "transparency",
      "responsible AI"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19306v1",
    "title": "Centroid Decision Forest",
    "authors": [
      "Amjad Ali",
      "Zardad Khan",
      "Saeed Aldahmani"
    ],
    "abstract": "This paper introduces the centroid decision forest (CDF), a novel ensemble\nlearning framework that redefines the splitting strategy and tree building in\nthe ordinary decision trees for high-dimensional classification. The splitting\napproach in CDF differs from the traditional decision trees in theat the class\nseparability score (CSS) determines the selection of the most discriminative\nfeatures at each node to construct centroids of the partitions (daughter\nnodes). The splitting criterion uses the Euclidean distance measurements from\neach class centroid to achieve a splitting mechanism that is more flexible and\nrobust. Centroids are constructed by computing the mean feature values of the\nselected features for each class, ensuring a class-representative division of\nthe feature space. This centroid-driven approach enables CDF to capture complex\nclass structures while maintaining interpretability and scalability. To\nevaluate CDF, 23 high-dimensional datasets are used to assess its performance\nagainst different state-of-the-art classifiers through classification accuracy\nand Cohen's kappa statistic. The experimental results show that CDF outperforms\nthe conventional methods establishing its effectiveness and flexibility for\nhigh-dimensional classification problems.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "14J60"
    ],
    "primary_category": "stat.ML",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19295v1",
    "title": "Exploring Semantic Feature Discrimination for Perceptual Image Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment",
    "authors": [
      "Guanglu Dong",
      "Xiangyu Liao",
      "Mingyang Li",
      "Guihuan Guo",
      "Chao Ren"
    ],
    "abstract": "Generative Adversarial Networks (GANs) have been widely applied to image\nsuper-resolution (SR) to enhance the perceptual quality. However, most existing\nGAN-based SR methods typically perform coarse-grained discrimination directly\non images and ignore the semantic information of images, making it challenging\nfor the super resolution networks (SRN) to learn fine-grained and\nsemantic-related texture details. To alleviate this issue, we propose a\nsemantic feature discrimination method, SFD, for perceptual SR. Specifically,\nwe first design a feature discriminator (Feat-D), to discriminate the\npixel-wise middle semantic features from CLIP, aligning the feature\ndistributions of SR images with that of high-quality images. Additionally, we\npropose a text-guided discrimination method (TG-D) by introducing learnable\nprompt pairs (LPP) in an adversarial manner to perform discrimination on the\nmore abstract output feature of CLIP, further enhancing the discriminative\nability of our method. With both Feat-D and TG-D, our SFD can effectively\ndistinguish between the semantic feature distributions of low-quality and\nhigh-quality images, encouraging SRN to generate more realistic and\nsemantic-relevant textures. Furthermore, based on the trained Feat-D and LPP,\nwe propose a novel opinion-unaware no-reference image quality assessment (OU\nNR-IQA) method, SFD-IQA, greatly improving OU NR-IQA performance without any\nadditional targeted training. Extensive experiments on classical SISR,\nreal-world SISR, and OU NR-IQA tasks demonstrate the effectiveness of our\nproposed methods.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19285v2",
    "title": "No Black Box Anymore: Demystifying Clinical Predictive Modeling with Temporal-Feature Cross Attention Mechanism",
    "authors": [
      "Yubo Li",
      "Xinyu Yao",
      "Rema Padman"
    ],
    "abstract": "Despite the outstanding performance of deep learning models in clinical\nprediction tasks, explainability remains a significant challenge. Inspired by\ntransformer architectures, we introduce the Temporal-Feature Cross Attention\nMechanism (TFCAM), a novel deep learning framework designed to capture dynamic\ninteractions among clinical features across time, enhancing both predictive\naccuracy and interpretability. In an experiment with 1,422 patients with\nChronic Kidney Disease, predicting progression to End-Stage Renal Disease,\nTFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an\nF1-score of 0.69. Beyond performance gains, TFCAM provides multi-level\nexplainability by identifying critical temporal periods, ranking feature\nimportance, and quantifying how features influence each other across time\nbefore affecting predictions. Our approach addresses the \"black box\"\nlimitations of deep learning in healthcare, offering clinicians transparent\ninsights into disease progression mechanisms while maintaining state-of-the-art\npredictive performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "interpretability",
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19267v1",
    "title": "NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios",
    "authors": [
      "Songyi Gao",
      "Zuolin Tu",
      "Rong-Jun Qin",
      "Yi-Hao Sun",
      "Xiong-Hui Chen",
      "Yang Yu"
    ],
    "abstract": "Offline reinforcement learning (RL) aims to learn from historical data\nwithout requiring (costly) access to the environment. To facilitate offline RL\nresearch, we previously introduced NeoRL, which highlighted that datasets from\nreal-world tasks are often conservative and limited. With years of experience\napplying offline RL to various domains, we have identified additional\nreal-world challenges. These include extremely conservative data distributions\nproduced by deployed control systems, delayed action effects caused by\nhigh-latency transitions, external factors arising from the uncontrollable\nvariance of transitions, and global safety constraints that are difficult to\nevaluate during the decision-making process. These challenges are\nunderrepresented in previous benchmarks but frequently occur in real-world\ntasks. To address this, we constructed the extended Near Real-World Offline RL\nBenchmark (NeoRL-2), which consists of 7 datasets from 7 simulated tasks along\nwith their corresponding evaluation simulators. Benchmarking results from\nstate-of-the-art offline RL approaches demonstrate that current methods often\nstruggle to outperform the data-collection behavior policy, highlighting the\nneed for more effective methods. We hope NeoRL-2 will accelerate the\ndevelopment of reinforcement learning algorithms for real-world applications.\nThe benchmark project page is available at https://github.com/polixir/NeoRL2.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19262v1",
    "title": "Learning Hazing to Dehazing: Towards Realistic Haze Generation for Real-World Image Dehazing",
    "authors": [
      "Ruiyi Wang",
      "Yushuo Zheng",
      "Zicheng Zhang",
      "Chunyi Li",
      "Shuaicheng Liu",
      "Guangtao Zhai",
      "Xiaohong Liu"
    ],
    "abstract": "Existing real-world image dehazing methods primarily attempt to fine-tune\npre-trained models or adapt their inference procedures, thus heavily relying on\nthe pre-trained models and associated training data. Moreover, restoring\nheavily distorted information under dense haze requires generative diffusion\nmodels, whose potential in dehazing remains underutilized partly due to their\nlengthy sampling processes. To address these limitations, we introduce a novel\nhazing-dehazing pipeline consisting of a Realistic Hazy Image Generation\nframework (HazeGen) and a Diffusion-based Dehazing framework (DiffDehaze).\nSpecifically, HazeGen harnesses robust generative diffusion priors of\nreal-world hazy images embedded in a pre-trained text-to-image diffusion model.\nBy employing specialized hybrid training and blended sampling strategies,\nHazeGen produces realistic and diverse hazy images as high-quality training\ndata for DiffDehaze. To alleviate the inefficiency and fidelity concerns\nassociated with diffusion-based methods, DiffDehaze adopts an Accelerated\nFidelity-Preserving Sampling process (AccSamp). The core of AccSamp is the\nTiled Statistical Alignment Operation (AlignOp), which can provide a clean and\nfaithful dehazing estimate within a small fraction of sampling steps to reduce\ncomplexity and enable effective fidelity guidance. Extensive experiments\ndemonstrate the superior dehazing performance and visual quality of our\napproach over existing methods. The code is available at\nhttps://github.com/ruiyi-w/Learning-Hazing-to-Dehazing.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-25",
    "last_updated": "2025-03-25",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19201v1",
    "title": "A Shared Low-Rank Adaptation Approach to Personalized RLHF",
    "authors": [
      "Renpu Liu",
      "Peng Wang",
      "Donghao Li",
      "Cong Shen",
      "Jing Yang"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntechnique for aligning artificial intelligence systems with human values,\nachieving remarkable success in fine-tuning large language models. However,\nexisting RLHF frameworks often assume that human preferences are relatively\nhomogeneous and can be captured by a single, unified reward model. This\nassumption overlooks the inherent diversity and heterogeneity across\nindividuals, limiting the adaptability of RLHF to personalized scenarios and\nrisking misalignments that can diminish user satisfaction and trust in AI\nsystems. In this paper, we address these challenges by introducing Low-Rank\nAdaptation (LoRA) into the personalized RLHF framework. We apply LoRA in the\nthe aggregated parameter space of all personalized reward functions, thereby\nenabling efficient learning of personalized reward models from potentially\nlimited local datasets. Our approach exploits potential shared structures among\nthe local ground-truth reward models while allowing for individual adaptation,\nwithout relying on restrictive assumptions about shared representations as in\nprior works. We further establish sample complexity guarantees for our method.\nTheoretical analysis demonstrates the effectiveness of the proposed approach in\ncapturing both shared and individual-specific structures within heterogeneous\nhuman preferences, addressing the dual challenge of personalization\nrequirements and practical data constraints. Experimental results on real-world\ndatasets corroborate the efficiency of our algorithm in the personalized RLHF\nsetting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment",
      "misalignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19173v1",
    "title": "Graph neural networks extrapolate out-of-distribution for shortest paths",
    "authors": [
      "Robert R. Nerem",
      "Samantha Chen",
      "Sanjoy Dasgupta",
      "Yusu Wang"
    ],
    "abstract": "Neural networks (NNs), despite their success and wide adoption, still\nstruggle to extrapolate out-of-distribution (OOD), i.e., to inputs that are not\nwell-represented by their training dataset. Addressing the OOD generalization\ngap is crucial when models are deployed in environments significantly different\nfrom the training set, such as applying Graph Neural Networks (GNNs) trained on\nsmall graphs to large, real-world graphs. One promising approach for achieving\nrobust OOD generalization is the framework of neural algorithmic alignment,\nwhich incorporates ideas from classical algorithms by designing neural\narchitectures that resemble specific algorithmic paradigms (e.g. dynamic\nprogramming). The hope is that trained models of this form would have superior\nOOD capabilities, in much the same way that classical algorithms work for all\ninstances. We rigorously analyze the role of algorithmic alignment in achieving\nOOD generalization, focusing on graph neural networks (GNNs) applied to the\ncanonical shortest path problem. We prove that GNNs, trained to minimize a\nsparsity-regularized loss over a small set of shortest path instances, exactly\nimplement the Bellman-Ford (BF) algorithm for shortest paths. In fact, if a GNN\nminimizes this loss within an error of $\\epsilon$, it implements the BF\nalgorithm with an error of $O(\\epsilon)$. Consequently, despite limited\ntraining data, these GNNs are guaranteed to extrapolate to arbitrary\nshortest-path problems, including instances of any size. Our empirical results\nsupport our theory by showing that NNs trained by gradient descent are able to\nminimize this loss and extrapolate in practice.",
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19135v1",
    "title": "Cooperative Control of Multi-Quadrotors for Transporting Cable-Suspended Payloads: Obstacle-Aware Planning and Event-Based Nonlinear Model Predictive Control",
    "authors": [
      "Tohid Kargar Tasooji",
      "Sakineh Khodadadi",
      "Guangjun Liu",
      "Richard Wang"
    ],
    "abstract": "This paper introduces a novel methodology for the cooperative control of\nmultiple quadrotors transporting cablesuspended payloads, emphasizing\nobstacle-aware planning and event-based Nonlinear Model Predictive Control\n(NMPC). Our approach integrates trajectory planning with real-time control\nthrough a combination of the A* algorithm for global path planning and NMPC for\nlocal control, enhancing trajectory adaptability and obstacle avoidance. We\npropose an advanced event-triggered control system that updates based on events\nidentified through dynamically generated environmental maps. These maps are\nconstructed using a dual-camera setup, which includes multi-camera systems for\nstatic obstacle detection and event cameras for high-resolution, low-latency\ndetection of dynamic obstacles. This design is crucial for addressing\nfast-moving and transient obstacles that conventional cameras may overlook,\nparticularly in environments with rapid motion and variable lighting\nconditions. When new obstacles are detected, the A* algorithm recalculates\nwaypoints based on the updated map, ensuring safe and efficient navigation.\nThis real-time obstacle detection and map updating integration allows the\nsystem to adaptively respond to environmental changes, markedly improving\nsafety and navigation efficiency. The system employs SLAM and object detection\ntechniques utilizing data from multi-cameras, event cameras, and IMUs for\naccurate localization and comprehensive environmental mapping. The NMPC\nframework adeptly manages the complex dynamics of multiple quadrotors and\nsuspended payloads, incorporating safety constraints to maintain dynamic\nfeasibility and stability. Extensive simulations validate the proposed\napproach, demonstrating significant enhancements in energy efficiency,\ncomputational resource management, and responsiveness.",
    "categories": [
      "cs.RO",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.20807v1",
    "title": "Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models",
    "authors": [
      "Pin-Yu Chen",
      "Han Shen",
      "Payel Das",
      "Tianyi Chen"
    ],
    "abstract": "Fine-tuning Large Language Models (LLMs) on some task-specific datasets has\nbeen a primary use of LLMs. However, it has been empirically observed that this\napproach to enhancing capability inevitably compromises safety, a phenomenon\nalso known as the safety-capability trade-off in LLM fine-tuning. This paper\npresents a theoretical framework for understanding the interplay between safety\nand capability in two primary safety-aware LLM fine-tuning strategies,\nproviding new insights into the effects of data similarity, context overlap,\nand alignment loss landscape. Our theoretical results characterize the\nfundamental limits of the safety-capability trade-off in LLM fine-tuning, which\nare also validated by numerical experiments.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment",
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19134v1",
    "title": "MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks",
    "authors": [
      "Wenhao You",
      "Bryan Hooi",
      "Yiwei Wang",
      "Youke Wang",
      "Zong Ke",
      "Ming-Hsuan Yang",
      "Zi Huang",
      "Yujun Cai"
    ],
    "abstract": "While safety mechanisms have significantly progressed in filtering harmful\ntext inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit\ntheir cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal\njailbreak framework that exploits narrative-driven context and role immersion\nto circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By\nsystematically decomposing the toxic query into environment, role, and action\ntriplets, MIRAGE constructs a multi-turn visual storytelling sequence of images\nand text using Stable Diffusion, guiding the target model through an engaging\ndetective narrative. This process progressively lowers the model's defences and\nsubtly guides its reasoning through structured contextual cues, ultimately\neliciting harmful responses. In extensive experiments on the selected datasets\nwith six mainstream MLLMs, MIRAGE achieves state-of-the-art performance,\nimproving attack success rates by up to 17.5% over the best baselines.\nMoreover, we demonstrate that role immersion and structured semantic\nreconstruction can activate inherent model biases, facilitating the model's\nspontaneous violation of ethical safeguards. These results highlight critical\nweaknesses in current multimodal safety mechanisms and underscore the urgent\nneed for more robust defences against cross-modal threats.",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19126v1",
    "title": "Tractable downfall of basis pursuit in structured sparse optimization",
    "authors": [
      "Maya V. Marmary",
      "Christian Grussler"
    ],
    "abstract": "The problem of finding the sparsest solution to a linear underdetermined\nsystem of equations, as it often appears in data analysis, optimal control and\nsystem identification problems, is considered. This non-convex problem is\ncommonly solved by convexification via $\\ell_1$-norm minimization, also known\nas basis pursuit. In this work, a class of structured matrices, representing\nthe system of equations, is introduced for which the basis pursuit approach\ntractably fails to recover the sparsest solution. In particular, we are able to\nidentify matrix columns that correspond to unrecoverable non-zero entries of\nthe sparsest solution, as well as to conclude the uniqueness of the sparsest\nsolution in polynomial time. These deterministic guarantees contrast popular\nprobabilistic ones, and as such, provide valuable insights into the a priori\ndesign of sparse optimization problems. As our matrix structure appears\nnaturally in optimal control problems, we exemplify our findings by showing\nthat it is possible to verify a priori that basis pursuit may fail in finding\nfuel optimal regulators for a class of discrete-time linear time-invariant\nsystems.",
    "categories": [
      "math.OC",
      "stat.ML",
      "90C06, 90C25, 90C26, 90C59, 49M25"
    ],
    "primary_category": "math.OC",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "control problem"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19123v1",
    "title": "Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided Language Modeling",
    "authors": [
      "Haebin Shin",
      "Lei Ji",
      "Xiao Liu",
      "Yeyun Gong"
    ],
    "abstract": "Using large teacher models to guide the training of smaller student models\nhas become the prevailing paradigm for efficient and effective learning.\nHowever, vocabulary mismatches between teacher and student language models pose\nsignificant challenges in language modeling, resulting in divergent token\nsequences and output distributions. To overcome these limitations, we propose\nVocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel\napproach that bridges the gap caused by vocabulary mismatch through two key\nmethods: (1) Token-level Lexical Alignment, which aligns token sequences across\nmismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss\nof teacher model to guide effective student training. We demonstrate its\neffectiveness in language modeling with 1B student model using various 7B\nteacher models with different vocabularies. Notably, with\nQwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary\nwith TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to\nnaive continual pretraining. Furthermore, we demonstrate that VocAgnoLM\nconsistently benefits from stronger teacher models, providing a robust solution\nto vocabulary mismatches in language modeling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19099v1",
    "title": "Masks and Mimicry: Strategic Obfuscation and Impersonation Attacks on Authorship Verification",
    "authors": [
      "Kenneth Alperin",
      "Rohan Leekha",
      "Adaku Uchendu",
      "Trang Nguyen",
      "Srilakshmi Medarametla",
      "Carlos Levya Capote",
      "Seth Aycock",
      "Charlie Dagli"
    ],
    "abstract": "The increasing use of Artificial Intelligence (AI) technologies, such as\nLarge Language Models (LLMs) has led to nontrivial improvements in various\ntasks, including accurate authorship identification of documents. However,\nwhile LLMs improve such defense techniques, they also simultaneously provide a\nvehicle for malicious actors to launch new attack vectors. To combat this\nsecurity risk, we evaluate the adversarial robustness of authorship models\n(specifically an authorship verification model) to potent LLM-based attacks.\nThese attacks include untargeted methods - \\textit{authorship obfuscation} and\ntargeted methods - \\textit{authorship impersonation}. For both attacks, the\nobjective is to mask or mimic the writing style of an author while preserving\nthe original texts' semantics, respectively. Thus, we perturb an accurate\nauthorship verification model, and achieve maximum attack success rates of 92\\%\nand 78\\% for both obfuscation and impersonation attacks, respectively.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19096v1",
    "title": "Uncertainty-Aware Decomposed Hybrid Networks",
    "authors": [
      "Sina Ditzel",
      "Achref Jaziri",
      "Iuliia Pliushch",
      "Visvanathan Ramesh"
    ],
    "abstract": "The robustness of image recognition algorithms remains a critical challenge,\nas current models often depend on large quantities of labeled data. In this\npaper, we propose a hybrid approach that combines the adaptability of neural\nnetworks with the interpretability, transparency, and robustness of\ndomain-specific quasi-invariant operators. Our method decomposes the\nrecognition into multiple task-specific operators that focus on different\ncharacteristics, supported by a novel confidence measurement tailored to these\noperators. This measurement enables the network to prioritize reliable features\nand accounts for noise. We argue that our design enhances transparency and\nrobustness, leading to improved performance, particularly in low-data regimes.\nExperimental results in traffic sign detection highlight the effectiveness of\nthe proposed method, especially in semi-supervised and unsupervised scenarios,\nunderscoring its potential for data-constrained applications.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "interpretability",
      "transparency"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19074v1",
    "title": "HingeRLC-GAN: Combating Mode Collapse with Hinge Loss and RLC Regularization",
    "authors": [
      "Osman Goni",
      "Himadri Saha Arka",
      "Mithun Halder",
      "Mir Moynuddin Ahmed Shibly",
      "Swakkhar Shatabda"
    ],
    "abstract": "Recent advances in Generative Adversarial Networks (GANs) have demonstrated\ntheir capability for producing high-quality images. However, a significant\nchallenge remains mode collapse, which occurs when the generator produces a\nlimited number of data patterns that do not reflect the diversity of the\ntraining dataset. This study addresses this issue by proposing a number of\narchitectural changes aimed at increasing the diversity and stability of GAN\nmodels. We start by improving the loss function with Wasserstein loss and\nGradient Penalty to better capture the full range of data variations. We also\ninvestigate various network architectures and conclude that ResNet\nsignificantly contributes to increased diversity. Building on these findings,\nwe introduce HingeRLC-GAN, a novel approach that combines RLC Regularization\nand the Hinge loss function. With a FID Score of 18 and a KID Score of 0.001,\nour approach outperforms existing methods by effectively balancing training\nstability and increased diversity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "adversarial"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19041v1",
    "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
    "authors": [
      "Kangwei Liu",
      "Mengru Wang",
      "Yujie Luo",
      "Lin Yuan",
      "Mengshu Sun",
      "Ningyu Zhang",
      "Lei Liang",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Huajun Chen"
    ],
    "abstract": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment",
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18941v1",
    "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval",
    "authors": [
      "Hongru Cai",
      "Yongqi Li",
      "Ruifeng Yuan",
      "Wenjie Wang",
      "Zhen Zhang",
      "Wenjie Li",
      "Tat-Seng Chua"
    ],
    "abstract": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems.",
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19011v1",
    "title": "RomanTex: Decoupling 3D-aware Rotary Positional Embedded Multi-Attention Network for Texture Synthesis",
    "authors": [
      "Yifei Feng",
      "Mingxin Yang",
      "Shuhui Yang",
      "Sheng Zhang",
      "Jiaao Yu",
      "Zibo Zhao",
      "Yuhong Liu",
      "Jie Jiang",
      "Chunchao Guo"
    ],
    "abstract": "Painting textures for existing geometries is a critical yet labor-intensive\nprocess in 3D asset generation. Recent advancements in text-to-image (T2I)\nmodels have led to significant progress in texture generation. Most existing\nresearch approaches this task by first generating images in 2D spaces using\nimage diffusion models, followed by a texture baking process to achieve UV\ntexture. However, these methods often struggle to produce high-quality textures\ndue to inconsistencies among the generated multi-view images, resulting in\nseams and ghosting artifacts. In contrast, 3D-based texture synthesis methods\naim to address these inconsistencies, but they often neglect 2D diffusion model\npriors, making them challenging to apply to real-world objects To overcome\nthese limitations, we propose RomanTex, a multiview-based texture generation\nframework that integrates a multi-attention network with an underlying 3D\nrepresentation, facilitated by our novel 3D-aware Rotary Positional Embedding.\nAdditionally, we incorporate a decoupling characteristic in the multi-attention\nblock to enhance the model's robustness in image-to-texture task, enabling\nsemantically-correct back-view synthesis. Furthermore, we introduce a\ngeometry-related Classifier-Free Guidance (CFG) mechanism to further improve\nthe alignment with both geometries and images. Quantitative and qualitative\nevaluations, along with comprehensive user studies, demonstrate that our method\nachieves state-of-the-art results in texture quality and consistency.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18931v1",
    "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
    "authors": [
      "Yitong Chen",
      "Lingchen Meng",
      "Wujian Peng",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18880v1",
    "title": "Seeing Speech and Sound: Distinguishing and Locating Audios in Visual Scenes",
    "authors": [
      "Hyeonggon Ryu",
      "Seongyu Kim",
      "Joon Son Chung",
      "Arda Senocak"
    ],
    "abstract": "We present a unified model capable of simultaneously grounding both spoken\nlanguage and non-speech sounds within a visual scene, addressing key\nlimitations in current audio-visual grounding models. Existing approaches are\ntypically limited to handling either speech or non-speech sounds independently,\nor at best, together but sequentially without mixing. This limitation prevents\nthem from capturing the complexity of real-world audio sources that are often\nmixed. Our approach introduces a 'mix-and-separate' framework with audio-visual\nalignment objectives that jointly learn correspondence and disentanglement\nusing mixed audio. Through these objectives, our model learns to produce\ndistinct embeddings for each audio type, enabling effective disentanglement and\ngrounding across mixed audio sources. Additionally, we created a new dataset to\nevaluate simultaneous grounding of mixed audio sources, demonstrating that our\nmodel outperforms prior methods. Our approach also achieves comparable or\nbetter performance in standard segmentation and cross-modal retrieval tasks,\nhighlighting the benefits of our mix-and-separate approach.",
    "categories": [
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18878v1",
    "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders",
    "authors": [
      "Andrey Galichin",
      "Alexey Dontsov",
      "Polina Druzhinina",
      "Anton Razzhigaev",
      "Oleg Y. Rogov",
      "Elena Tutubalina",
      "Ivan Oseledets"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "interpretability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18856v1",
    "title": "MODIS: Multi-Omics Data Integration for Small and Unpaired Datasets",
    "authors": [
      "Daniel Lepe-Soltero",
      "Thierry ArtiÃ¨res",
      "AnaÃ¯s Baudot",
      "Paul Villoutreix"
    ],
    "abstract": "A key challenge today lies in the ability to efficiently handle multi-omics\ndata since such multimodal data may provide a more comprehensive overview of\nthe underlying processes in a system. Yet it comes with challenges: multi-omics\ndata are most often unpaired and only partially labeled, moreover only small\namounts of data are available in some situation such as rare diseases. We\npropose MODIS which stands for Multi-Omics Data Integration for Small and\nunpaired datasets, a semi supervised approach to account for these particular\nsettings. MODIS learns a probabilistic coupling of heterogeneous data\nmodalities and learns a shared latent space where modalities are aligned. We\nrely on artificial data to build controlled experiments to explore how much\nsupervision is needed for an accurate alignment of modalities, and how our\napproach enables dealing with new conditions for which few data are available.\nThe code is available athttps://github.com/VILLOUTREIXLab/MODIS.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18842v2",
    "title": "Three Kinds of AI Ethics",
    "authors": [
      "Emanuele Ratti"
    ],
    "abstract": "There is an overwhelming abundance of works in AI Ethics. This growth is\nchaotic because of how sudden it is, its volume, and its multidisciplinary\nnature. This makes difficult to keep track of debates, and to systematically\ncharacterize goals, research questions, methods, and expertise required by AI\nethicists. In this article, I show that the relation between AI and ethics can\nbe characterized in at least three ways, which correspond to three\nwell-represented kinds of AI ethics: ethics and AI; ethics in AI; ethics of AI.\nI elucidate the features of these three kinds of AI Ethics, characterize their\nresearch questions, and identify the kind of expertise that each kind needs. I\nalso show how certain criticisms to AI ethics are misplaced, as being done from\nthe point of view of one kind of AI ethics, to another kind with different\ngoals. All in all, this work sheds light on the nature of AI ethics, and sets\nthe groundwork for more informed discussions about the scope, methods, and\ntraining of AI ethicists.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-26",
    "safety_keywords_found": [
      "ethics"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18836v1",
    "title": "Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated MRI Reconstruction",
    "authors": [
      "Yuxuan Zhang",
      "Jinkui Hao",
      "Bo Zhou"
    ],
    "abstract": "Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its\ninherently long acquisition times reduce clinical efficiency and patient\ncomfort. Recent advancements in deep learning, particularly diffusion models,\nhave improved accelerated MRI reconstruction. However, existing diffusion\nmodels' training often relies on fully sampled data, models incur high\ncomputational costs, and often lack uncertainty estimation, limiting their\nclinical applicability. To overcome these challenges, we propose a novel\nframework, called Dual-domain Multi-path Self-supervised Diffusion Model\n(DMSM), that integrates a self-supervised dual-domain diffusion model training\nscheme, a lightweight hybrid attention network for the reconstruction diffusion\nmodel, and a multi-path inference strategy, to enhance reconstruction accuracy,\nefficiency, and explainability. Unlike traditional diffusion-based models, DMSM\neliminates the dependency on training from fully sampled data, making it more\npractical for real-world clinical settings. We evaluated DMSM on two human MRI\ndatasets, demonstrating that it achieves favorable performance over several\nsupervised and self-supervised baselines, particularly in preserving fine\nanatomical structures and suppressing artifacts under high acceleration\nfactors. Additionally, our model generates uncertainty maps that correlate\nreasonably well with reconstruction errors, offering valuable clinically\ninterpretable guidance and potentially enhancing diagnostic confidence.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "explainability"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18830v1",
    "title": "DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition",
    "authors": [
      "Zhengxian Wu",
      "Chuanrui Zhang",
      "Hangrui Xu",
      "Peng Jiao",
      "Haoqian Wang"
    ],
    "abstract": "Gait recognition is emerging as a promising and innovative area within the\nfield of computer vision, widely applied to remote person identification.\nAlthough existing gait recognition methods have achieved substantial success in\ncontrolled laboratory datasets, their performance often declines significantly\nwhen transitioning to wild datasets.We argue that the performance gap can be\nprimarily attributed to the spatio-temporal distribution inconsistencies\npresent in wild datasets, where subjects appear at varying angles, positions,\nand distances across the frames. To achieve accurate gait recognition in the\nwild, we propose a skeleton-guided silhouette alignment strategy, which uses\nprior knowledge of the skeletons to perform affine transformations on the\ncorresponding silhouettes.To the best of our knowledge, this is the first study\nto explore the impact of data alignment on gait recognition. We conducted\nextensive experiments across multiple datasets and network architectures, and\nthe results demonstrate the significant advantages of our proposed alignment\nstrategy.Specifically, on the challenging Gait3D dataset, our method achieved\nan average performance improvement of 7.9% across all evaluated networks.\nFurthermore, our method achieves substantial improvements on cross-domain\ndatasets, with accuracy improvements of up to 24.0%.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18817v1",
    "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations",
    "authors": [
      "Jeonghyeon Kim",
      "Sangheum Hwang"
    ],
    "abstract": "Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "alignment"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18814v1",
    "title": "Towards Responsible AI Music: an Investigation of Trustworthy Features for Creative Systems",
    "authors": [
      "Jacopo de Berardinis",
      "Lorenzo Porcaro",
      "Albert MeroÃ±o-PeÃ±uela",
      "Angelo Cangelosi",
      "Tess Buckley"
    ],
    "abstract": "Generative AI is radically changing the creative arts, by fundamentally\ntransforming the way we create and interact with cultural artefacts. While\noffering unprecedented opportunities for artistic expression and\ncommercialisation, this technology also raises ethical, societal, and legal\nconcerns. Key among these are the potential displacement of human creativity,\ncopyright infringement stemming from vast training datasets, and the lack of\ntransparency, explainability, and fairness mechanisms. As generative systems\nbecome pervasive in this domain, responsible design is crucial. Whilst previous\nwork has tackled isolated aspects of generative systems (e.g., transparency,\nevaluation, data), we take a comprehensive approach, grounding these efforts\nwithin the Ethics Guidelines for Trustworthy Artificial Intelligence produced\nby the High-Level Expert Group on AI appointed by the European Commission - a\nframework for designing responsible AI systems across seven macro requirements.\nFocusing on generative music AI, we illustrate how these requirements can be\ncontextualised for the field, addressing trustworthiness across multiple\ndimensions and integrating insights from the existing literature. We further\npropose a roadmap for operationalising these contextualised requirements,\nemphasising interdisciplinary collaboration and stakeholder engagement. Our\nwork provides a foundation for designing and evaluating responsible music\ngeneration systems, calling for collaboration among AI experts, ethicists,\nlegal scholars, and artists. This manuscript is accompanied by a website:\nhttps://amresearchlab.github.io/raim-framework/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "explainability",
      "transparency",
      "ethics",
      "responsible AI"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.18808v1",
    "title": "CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos",
    "authors": [
      "Yang Liu",
      "Hongjin Wang",
      "Zepu Wang",
      "Xiaoguang Zhu",
      "Jing Liu",
      "Peng Sun",
      "Rui Tang",
      "Jianwei Du",
      "Victor C. M. Leung",
      "Liang Song"
    ],
    "abstract": "Video Anomaly Detection (VAD) remains a fundamental yet formidable task in\nthe video understanding community, with promising applications in areas such as\ninformation forensics and public safety protection. Due to the rarity and\ndiversity of anomalies, existing methods only use easily collected regular\nevents to model the inherent normality of normal spatial-temporal patterns in\nan unsupervised manner. Previous studies have shown that existing unsupervised\nVAD models are incapable of label-independent data offsets (e.g., scene\nchanges) in real-world scenarios and may fail to respond to light anomalies due\nto the overgeneralization of deep neural networks. Inspired by causality\nlearning, we argue that there exist causal factors that can adequately\ngeneralize the prototypical patterns of regular events and present significant\ndeviations when anomalous instances occur. In this regard, we propose Causal\nRepresentation Consistency Learning (CRCL) to implicitly mine potential\nscene-robust causal variable in unsupervised video normality learning.\nSpecifically, building on the structural causal models, we propose\nscene-debiasing learning and causality-inspired normality learning to strip\naway entangled scene bias in deep representations and learn causal video\nnormality, respectively. Extensive experiments on benchmarks validate the\nsuperiority of our method over conventional deep representation learning.\nMoreover, ablation studies and extension validation show that the CRCL can cope\nwith label-independent biases in multi-scene settings and maintain stable\nperformance with only limited training data available.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "safety"
    ],
    "is_safety_paper": true
  },
  {
    "id": "2503.19006v1",
    "title": "Computational Thinking with Computer Vision: Developing AI Competency in an Introductory Computer Science Course",
    "authors": [
      "Tahiya Chowdhury"
    ],
    "abstract": "Developing competency in artificial intelligence is becoming increasingly\ncrucial for computer science (CS) students at all levels of the CS curriculum.\nHowever, most previous research focuses on advanced CS courses, as traditional\nintroductory courses provide limited opportunities to develop AI skills and\nknowledge. This paper introduces an introductory CS course where students learn\ncomputational thinking through computer vision, a sub-field of AI, as an\napplication context. The course aims to achieve computational thinking outcomes\nalongside critical thinking outcomes that expose students to AI approaches and\ntheir societal implications. Through experiential activities such as individual\nprojects and reading discussions, our course seeks to balance technical\nlearning and critical thinking goals. Our evaluation, based on pre-and\npost-course surveys, shows an improved sense of belonging, self-efficacy, and\nAI ethics awareness among students. The results suggest that an AI-focused\ncontext can enhance participation and employability, student-selected projects\nsupport self-efficacy, and ethically grounded AI instruction can be effective\nfor interdisciplinary audiences. Students' discussions on reading assignments\ndemonstrated deep engagement with the complex challenges in today's AI\nlandscape. Finally, we share insights on scaling such courses for larger\ncohorts and improving the learning experience for introductory CS students.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "submitted_date": "2025-03-24",
    "last_updated": "2025-03-24",
    "safety_keywords_found": [
      "ethics"
    ],
    "is_safety_paper": true
  }
]