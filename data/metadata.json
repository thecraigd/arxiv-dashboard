{
  "last_updated": "2025-03-30 17:31:06",
  "total_papers": 1000,
  "safety_papers_count": 193,
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.CV",
    "cs.CL",
    "cs.RO",
    "stat.ML"
  ],
  "safety_terms": [
    "alignment",
    "misalignment",
    "value alignment",
    "AI alignment",
    "aligned AI",
    "interpretability",
    "explainability",
    "transparency",
    "existential risk",
    "x-risk",
    "catastrophic risk",
    "safety",
    "AI safety",
    "safe AI",
    "robust AI",
    "control problem",
    "AI control",
    "corrigibility",
    "specification gaming",
    "reward hacking",
    "value learning",
    "outer alignment",
    "inner alignment",
    "adversarial",
    "adversarial attack",
    "adversarial example",
    "ethics",
    "ethical AI",
    "responsible AI",
    "goal misgeneralization",
    "distributional shift",
    "AI governance",
    "AI policy",
    "superintelligence",
    "AGI safety"
  ]
}